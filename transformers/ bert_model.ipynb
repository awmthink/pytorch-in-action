{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert 模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BertModel,\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    BertForMaskedLM,\n",
    ")\n",
    "from transformers.models.bert import BertLayer\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "cls_model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "mask_lm_model = BertForMaskedLM.from_pretrained(model_name, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_lm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/Bert.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型参数量统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_param_cnt = 22.733M\n",
      "encoder_param_cnt = 81.114M\n",
      "pooler_param_cnt = 0.563M\n",
      "total_param_cnt = 104.410M\n"
     ]
    }
   ],
   "source": [
    "embeddings_param_cnt = 0\n",
    "encoder_param_cnt = 0\n",
    "pooler_param_cnt = 0\n",
    "\n",
    "for name, param in bert_model.named_parameters():\n",
    "    if \"embeddings\" in name:\n",
    "        embeddings_param_cnt += param.numel()\n",
    "    if \"encoder\" in name:\n",
    "        encoder_param_cnt += param.numel()\n",
    "    if \"pooler\" in name:\n",
    "        pooler_param_cnt += param.numel()\n",
    "total_param_cnt = embeddings_param_cnt + encoder_param_cnt + pooler_param_cnt\n",
    "\n",
    "print(\n",
    "    f\"embeddings_param_cnt = {embeddings_param_cnt / 1024 / 1024:.3f}M\",\n",
    "    f\"encoder_param_cnt = {encoder_param_cnt / 1024 / 1024:.3f}M\",\n",
    "    f\"pooler_param_cnt = {pooler_param_cnt / 1024 / 1024:.3f}M\",\n",
    "    f\"total_param_cnt = {total_param_cnt / 1024 / 1024:.3f}M\",\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型前向过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input ids (torch.Size([2, 7])): tensor([[  101,  2026,  3899,  2003,  2061, 10140,   102],\n",
      "        [  101,  2002,  7777,  2652,   102,     0,     0]])\n",
      "token_type_ids (torch.Size([2, 7])): tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask (torch.Size([2, 7])): tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "input_text = [\"my dog is so cute\", \"he likes playing\"]\n",
    "\n",
    "tokenizer_output = tokenizer(input_text, padding=True, return_tensors=\"pt\")\n",
    "input_ids = tokenizer_output[\"input_ids\"]\n",
    "token_type_ids = tokenizer_output[\"token_type_ids\"]\n",
    "attention_mask = tokenizer_output[\"attention_mask\"]\n",
    "\n",
    "extended_attention_mask = attention_mask[:, None, None, :]\n",
    "extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)\n",
    "extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(\n",
    "    extended_attention_mask.dtype\n",
    ").min\n",
    "\n",
    "print(f\"input ids ({input_ids.shape}): {input_ids}\")\n",
    "print(f\"token_type_ids ({token_type_ids.shape}): {token_type_ids}\")\n",
    "print(f\"attention_mask ({attention_mask.shape}): {attention_mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
       "           -3.4028e+38, -3.4028e+38]]]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embeddings: torch.Size([2, 7, 768])\n",
      "token_type_embeddints: torch.Size([2, 7, 768])\n",
      "position_embeddings: torch.Size([1, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = bert_model.embeddings.word_embeddings(input_ids)\n",
    "token_type_embeddints = bert_model.embeddings.token_type_embeddings(token_type_ids)\n",
    "position_embeddings = bert_model.embeddings.position_embeddings(\n",
    "    torch.arange(input_ids.size(1))\n",
    ").unsqueeze(0)\n",
    "\n",
    "print(f\"word_embeddings: {word_embeddings.shape}\")\n",
    "print(f\"token_type_embeddints: {token_type_embeddints.shape}\")\n",
    "print(f\"position_embeddings: {position_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_output = word_embeddings + token_type_embeddints + position_embeddings\n",
    "embedding_output = bert_model.embeddings.LayerNorm(embedding_output)\n",
    "embedding_output = bert_model.embeddings.dropout(embedding_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff: 0\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer(input_text, padding=True, return_tensors=\"pt\")\n",
    "token_ids.pop(\"attention_mask\")\n",
    "embedding_output_ref = bert_model.embeddings(**token_ids)\n",
    "print(f\"diff: {(embedding_output != embedding_output_ref).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "first_layer = bert_model.encoder.layer[0]\n",
    "print(first_layer)\n",
    "attention = first_layer.attention\n",
    "intermediate = first_layer.intermediate\n",
    "encoder_output = first_layer.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder-self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heads: 12, hidden_size: 768, head_size: 64\n"
     ]
    }
   ],
   "source": [
    "num_heads = bert_model.config.num_attention_heads\n",
    "hidden_size = embedding_output.size(-1)\n",
    "head_size = hidden_size // num_heads\n",
    "\n",
    "print(f\"heads: {num_heads}, hidden_size: {hidden_size}, head_size: {head_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Q = linearQ(X)\n",
    "# K = linearK(X)\n",
    "# V = linearV(X)\n",
    "query = attention.self.query(embedding_output)\n",
    "key = attention.self.key(embedding_output)\n",
    "value = attention.self.value(embedding_output)\n",
    "\n",
    "\n",
    "def multihead_transpose(x, nheads):\n",
    "    b, s, d = x.shape\n",
    "    head_size = d // nheads\n",
    "    # (b, s, d_model) -> (b, s, nheads, head_size)\n",
    "    x = torch.reshape(x, (b, s, nheads, head_size))\n",
    "    # (b, s, nheads, head_size)-> (b, nheads, s, head_size)\n",
    "    return torch.permute(x, (0, 2, 1, 3))\n",
    "\n",
    "\n",
    "# Softmax(Q @ K.T) / sqrt(d))\n",
    "query = multihead_transpose(query, num_heads)\n",
    "key = multihead_transpose(key, num_heads)\n",
    "value = multihead_transpose(value, num_heads)\n",
    "attention_scores = query @ (key.transpose(2, 3)) / math.sqrt(head_size)\n",
    "attention_scores = attention_scores + extended_attention_mask\n",
    "attention_probs = torch.softmax(attention_scores, -1)\n",
    "\n",
    "attention_probs = attention.self.dropout(attention_probs)\n",
    "\n",
    "self_out = attention_probs @ value\n",
    "self_out = torch.permute(self_out, (0, 2, 1, 3))\n",
    "self_out = torch.reshape(self_out, (self_out.size(0), self_out.size(1), -1))\n",
    "print(self_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 和库中的实现进行比较\n",
    "\n",
    "self_out_ref, attention_probs_ref = attention.self(\n",
    "    embedding_output, attention_mask=extended_attention_mask, output_attentions=True\n",
    ")\n",
    "assert (self_out != self_out_ref).sum() < 1e-4\n",
    "assert (attention_probs != attention_probs_ref).sum() < 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder-attention-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atten_output: torch.Size([2, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "atten_output = attention.output.dense(self_out)\n",
    "atten_output = attention.output.dropout(atten_output)\n",
    "atten_output = atten_output + embedding_output\n",
    "atten_output = attention.output.LayerNorm(atten_output)\n",
    "print(f\"atten_output: {atten_output.shape}\")\n",
    "\n",
    "assert (\n",
    "    atten_output\n",
    "    != attention(embedding_output, attention_mask=extended_attention_mask)[0]\n",
    ").sum() < 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder-intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate_output: torch.Size([2, 7, 3072])\n"
     ]
    }
   ],
   "source": [
    "intermediate_output = intermediate.dense(atten_output)\n",
    "intermediate_output = F.gelu(intermediate_output)\n",
    "print(f\"intermediate_output: {intermediate_output.shape}\")\n",
    "\n",
    "assert (intermediate_output != intermediate(atten_output)).sum() < 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_layer_output: torch.Size([2, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "first_layer_output = encoder_output.dense(intermediate_output)\n",
    "first_layer_output = encoder_output.dropout(first_layer_output)\n",
    "first_layer_output = encoder_output.LayerNorm(first_layer_output + atten_output)\n",
    "print(f\"first_layer_output: {first_layer_output.shape}\")\n",
    "\n",
    "assert (\n",
    "    first_layer_output != encoder_output(intermediate_output, atten_output)\n",
    ").sum() < 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state: torch.Size([2, 7, 768])\n",
      "pooler_output: torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "model_output = bert_model(**tokenizer_output)\n",
    "\n",
    "last_hidden_state = model_output[\"last_hidden_state\"]\n",
    "pooler_output = model_output[\"pooler_output\"]\n",
    "\n",
    "print(\n",
    "    f\"last_hidden_state: {last_hidden_state.shape}\",\n",
    "    f\"pooler_output: {pooler_output.shape}\",\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooler_out: torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "# 取出最后一层BertLayer的输出（last_hidden_state），取出CLS token对应的向量\n",
    "pooler_out = bert_model.pooler.dense(last_hidden_state[:, 0])\n",
    "pooler_out = F.tanh(pooler_out)\n",
    "assert (pooler_out != pooler_output).sum() < 1e-5\n",
    "print(f\"pooler_out: {pooler_out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 shape: torch.Size([2, 7, 768])\n",
      "01 shape: torch.Size([2, 7, 768])\n",
      "02 shape: torch.Size([2, 7, 768])\n",
      "03 shape: torch.Size([2, 7, 768])\n",
      "04 shape: torch.Size([2, 7, 768])\n",
      "05 shape: torch.Size([2, 7, 768])\n",
      "06 shape: torch.Size([2, 7, 768])\n",
      "07 shape: torch.Size([2, 7, 768])\n",
      "08 shape: torch.Size([2, 7, 768])\n",
      "09 shape: torch.Size([2, 7, 768])\n",
      "10 shape: torch.Size([2, 7, 768])\n",
      "11 shape: torch.Size([2, 7, 768])\n",
      "12 shape: torch.Size([2, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "# 模型输出Embedding的输出，以及每一个BertLayer的输出\n",
    "model_with_hs = BertModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "model_output = model_with_hs(**tokenizer_output)\n",
    "hidden_states = model_output[\"hidden_states\"]\n",
    "\n",
    "for i, hs in enumerate(hidden_states):\n",
    "    print(f\"{i:02} shape: {hs.shape}\")\n",
    "\n",
    "\n",
    "assert (hidden_states[-1] != last_hidden_state).sum() < 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] after abraham lincoln won the november 1860 presidential election on an anti - slavery platform , an initial seven slave states declared their secession from the country to form the confederacy . war broke out in april 1861 when secession ##ist forces attacked fort sum ##ter in south carolina , just over a month after lincoln ' s inauguration . [SEP]\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = (\n",
    "    \"After Abraham Lincoln won the November 1860 presidential \"\n",
    "    \"election on an anti-slavery platform, an initial seven \"\n",
    "    \"slave states declared their secession from the country \"\n",
    "    \"to form the Confederacy. War broke out in April 1861 \"\n",
    "    \"when secessionist forces attacked Fort Sumter in South \"\n",
    "    \"Carolina, just over a month after Lincoln's \"\n",
    "    \"inauguration.\"\n",
    ")\n",
    "\n",
    "mask_llm_input = tokenizer(input_text, return_tensors=\"pt\")\n",
    "mask_llm_input[\"labels\"] = mask_llm_input[\"input_ids\"].detach().clone()\n",
    "mask_llm_input.keys()\n",
    "\n",
    "\" \".join(tokenizer.convert_ids_to_tokens(mask_llm_input[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对输出的token进行随机mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] after [MASK] [MASK] won [MASK] november 1860 [MASK] election [MASK] an [MASK] [MASK] slavery platform , an initial seven slave states [MASK] their secession from the country to form [MASK] confederacy . war broke out in april 1861 when secession ##ist forces attacked fort sum ##ter in south carolina [MASK] just over a [MASK] after lincoln ' s inauguration . [SEP]\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.rand(mask_llm_input[\"input_ids\"].shape) < 0.15\n",
    "mask = (\n",
    "    mask * (mask_llm_input[\"input_ids\"] != 101) * (mask_llm_input[\"input_ids\"] != 102)\n",
    ")\n",
    "selection = mask.nonzero().numpy()\n",
    "mask_llm_input[\"input_ids\"][selection[:, 0], selection[:, 1]] = tokenizer.vocab[\n",
    "    \"[MASK]\"\n",
    "]\n",
    "\" \".join(tokenizer.convert_ids_to_tokens(mask_llm_input[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertOnlyMLMHead Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlm_hidden_states: 13\n",
      "mlm_logits: torch.Size([1, 62, 30522])\n",
      "mlm_loss: 0.8455974459648132\n"
     ]
    }
   ],
   "source": [
    "mlm_output = mask_lm_model(**mask_llm_input)\n",
    "mlm_hidden_states = mlm_output[\"hidden_states\"]\n",
    "mlm_logits = mlm_output[\"logits\"]\n",
    "mlm_loss = mlm_output[\"loss\"]\n",
    "\n",
    "print(f\"mlm_hidden_states: {len(mlm_hidden_states)}\")\n",
    "print(f\"mlm_logits: {mlm_logits.shape}\")\n",
    "print(f\"mlm_loss: {mlm_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_last_hidden_state = mlm_hidden_states[-1]\n",
    "\n",
    "mlm_prediction_out = mask_lm_model.cls.predictions.transform.dense(\n",
    "    mlm_last_hidden_state\n",
    ")\n",
    "mlm_prediction_out = F.gelu(mlm_prediction_out)\n",
    "mlm_prediction_out = mask_lm_model.cls.predictions.transform.LayerNorm(\n",
    "    mlm_prediction_out\n",
    ")\n",
    "\n",
    "mlm_decoder_out = mask_lm_model.cls.predictions.decoder(mlm_prediction_out)\n",
    "\n",
    "assert (mlm_decoder_out != mlm_logits).sum() < 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\". after president lincoln won the november 1860 presidential election on an anti anti slavery platform , an initial seven slave states declared their secession from the country to form the confederacy . war broke out in april 1861 when secession ##ist forces attacked fort sum ##ter in south carolina , just over a month after lincoln ' s inauguration . s\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对模型预测的结果进行 decode\n",
    "mlm_predicts = torch.argmax(mlm_logits, -1)\n",
    "\" \".join(tokenizer.convert_ids_to_tokens(mlm_predicts[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertOnlyMLMHead Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 62, 30522])\n",
      "torch.Size([1, 62])\n",
      "tensor(0.8456, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mlm_loss = torch.nn.CrossEntropyLoss()\n",
    "print(mlm_logits.shape)\n",
    "print(mask_llm_input[\"labels\"].shape)\n",
    "loss = mlm_loss(\n",
    "    mlm_logits.view(-1, mlm_logits.shape[-1]), mask_llm_input[\"labels\"].view(-1)\n",
    ")\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
