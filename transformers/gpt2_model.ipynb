{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|model        |参数量       |hidden dim        |  block 数量 |\n",
    "| ----------- |----------- | ---------------- | -----------|\n",
    "|gpt2         |124M        |    768  (64*12) |          12|\n",
    "|gpt2-medium  |355M        |    1024 (64*16) |          24|\n",
    "|gpt2-large   |774M        |    1280 (64*20) |          36|\n",
    "|gpt2-xl      |1.56B       |    1600 (64*25) |          48|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "\n",
    "gpt2_config = AutoConfig.from_pretrained(model_name)\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "gpt2_model = AutoModel.from_pretrained(model_name)\n",
    "gpt2_clm_model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_name_or_path\": \"gpt2\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.31.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_clm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/gpt2.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 tokenizer vocab size: 50257\n",
      "gpt2 tokenizer speical token map: {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"gpt2 tokenizer vocab size: {len(gpt2_tokenizer.vocab)}\")\n",
    "print(f\"gpt2 tokenizer speical token map: {gpt2_tokenizer.special_tokens_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置tokenizer的补齐规则\n",
    "gpt2_tokenizer.padding_side = \"left\"\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 6511,   890,  2084,    11,   314,   373,   257,  2933,    11,   314,\n",
      "          1842],\n",
      "        [50256, 50256, 50256, 50256, 50256,    70,   457,    17,    13, 19334,\n",
      "           952]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "input_texts = [\"long long ago, I was a boy, I love\", \"gpt2.drawio\"]\n",
    "tokenizer_out = gpt2_tokenizer(input_texts, padding=True, return_tensors=\"pt\")\n",
    "print(tokenizer_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['long', 'Ġlong', 'Ġago', ',', 'ĠI', 'Ġwas', 'Ġa', 'Ġboy', ',', 'ĠI', 'Ġlove']\n",
      "long long ago, I was a boy, I love\n",
      "['<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', 'g', 'pt', '2', '.', 'draw', 'io']\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>gpt2.drawio\n"
     ]
    }
   ],
   "source": [
    "print(gpt2_tokenizer.convert_ids_to_tokens(tokenizer_out.input_ids[0].tolist()))\n",
    "print(gpt2_tokenizer.decode(tokenizer_out.input_ids[0].tolist()))\n",
    "print(gpt2_tokenizer.convert_ids_to_tokens(tokenizer_out.input_ids[1].tolist()))\n",
    "print(gpt2_tokenizer.decode(tokenizer_out.input_ids[1].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的token以\"Ġ\"开头，应该表示该token前面有个空格。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 整体的输出\n",
    "\n",
    "GPT2模型的直接进行forward的输出有两个：\n",
    "\n",
    "* last_hidden_state\n",
    "* past_key_values\n",
    "\n",
    "其中`past_key_values`中保存的是一个GPT2Block中的Key和Value，而`last_hidden_state`则是一个 $N\\times T \\times d$ 的输出Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state: torch.Size([2, 11, 768])\n",
      "past_key_values: (12, 2, torch.Size([2, 12, 11, 64]))\n"
     ]
    }
   ],
   "source": [
    "gpt2_model_output = gpt2_model(**tokenizer_out)\n",
    "last_hidden_state = gpt2_model_output.last_hidden_state\n",
    "past_key_values = gpt2_model_output.past_key_values\n",
    "print(f\"last_hidden_state: {last_hidden_state.shape}\")\n",
    "print(\n",
    "    f\"past_key_values: ({len(past_key_values)}, {len(past_key_values[0])}, {past_key_values[0][0].shape})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeds = gpt2_model.wte(tokenizer_out[\"input_ids\"])\n",
    "batch_size, seq_len = tokenizer_out[\"input_ids\"].shape\n",
    "position_ids = torch.arange(seq_len).unsqueeze(0).repeat((batch_size, 1))\n",
    "position_embeds = gpt2_model.wpe(position_ids)\n",
    "\n",
    "embeds_out = token_embeds + position_embeds\n",
    "embeds_out = gpt2_model.drop(embeds_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GTP2Block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_key_values = tuple([None] * gpt2_config.n_layer)\n",
    "head_mask = gpt2_model.get_head_mask(None, gpt2_config.n_layer)\n",
    "\n",
    "hidden_states = embeds_out\n",
    "\n",
    "attent_mask = tokenizer_out[\"attention_mask\"]\n",
    "attent_mask = attent_mask[:, None, None, :]\n",
    "attent_mask = attent_mask.to(hidden_states.dtype)\n",
    "attent_mask = (1 - attent_mask) * torch.finfo(attent_mask.dtype).min\n",
    "\n",
    "for i, (block, layer_past) in enumerate(zip(gpt2_model.h, past_key_values)):\n",
    "    block_out = block(\n",
    "        hidden_states,\n",
    "        layer_past=layer_past,\n",
    "        attention_mask=attent_mask,\n",
    "        head_mask=head_mask[i],\n",
    "    )\n",
    "    hidden_states = block_out[0]\n",
    "\n",
    "hidden_states = gpt2_model.ln_f(hidden_states)\n",
    "\n",
    "assert (hidden_states != last_hidden_state).sum() < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM Head\n",
    "\n",
    "LM Head 对于backbone抽取到的hidden_states进行一次Linear变换，输出维度为词表的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>top 0</th>\n",
       "      <th>top 1</th>\n",
       "      <th>top 2</th>\n",
       "      <th>top 3</th>\n",
       "      <th>top 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>long long ago, I was a boy, I love</td>\n",
       "      <td>you(20.24%)</td>\n",
       "      <td>my(10.99%)</td>\n",
       "      <td>the(4.46%)</td>\n",
       "      <td>to(3.41%)</td>\n",
       "      <td>it(3.24%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endof...</td>\n",
       "      <td>pt(90.58%)</td>\n",
       "      <td>ip(0.86%)</td>\n",
       "      <td>\\n(0.65%)</td>\n",
       "      <td>,(0.48%)</td>\n",
       "      <td>ign(0.47%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input         top 0   \n",
       "0                 long long ago, I was a boy, I love   you(20.24%)  \\\n",
       "1  <|endoftext|><|endoftext|><|endoftext|><|endof...    pt(90.58%)   \n",
       "\n",
       "         top 1        top 2       top 3       top 4  \n",
       "0   my(10.99%)   the(4.46%)   to(3.41%)   it(3.24%)  \n",
       "1    ip(0.86%)    \\n(0.65%)    ,(0.48%)  ign(0.47%)  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head_logits = gpt2_clm_model.lm_head(hidden_states)\n",
    "lm_predicts = torch.argmax(lm_head_logits, dim=-1)\n",
    "\n",
    "# 取出最后一个位置的输出\n",
    "last_token_output_prob = torch.softmax(lm_head_logits[:, -1, :], -1)\n",
    "sorted_ids = torch.argsort(last_token_output_prob, dim=-1, descending=True)\n",
    "\n",
    "# 现在我们重点看一下，batch中第一个句子的预测结果，列出top5的预测的token\n",
    "\n",
    "lm_outputs = []\n",
    "for i in range(sorted_ids.size(0)):\n",
    "    lm_output = {}\n",
    "    lm_output[\"input\"] = gpt2_tokenizer.decode(tokenizer_out[\"input_ids\"][i])\n",
    "    for k in range(5):\n",
    "        token_id = sorted_ids[i][k]\n",
    "        prob = last_token_output_prob[i][token_id]\n",
    "        lm_output[f\"top {k}\"] = f\"{gpt2_tokenizer.decode(token_id)}({100*prob:.2f}%)\"\n",
    "    lm_outputs.append(lm_output)\n",
    "pd.DataFrame(lm_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate\n",
    "\n",
    "HuggingFace上的文章：[How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对一个句子进行decode，不支持batch\n",
    "def gpt2_causal_lm_decode(model, tokenizer, text, nsteps=5):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    for _ in range(nsteps):\n",
    "        out = model(input_ids=input_ids)\n",
    "        logits = out.logits[:, -1, :].unsqueeze(1)\n",
    "        token_id = torch.argmax(logits, dim=-1)\n",
    "        input_ids = torch.concat([input_ids, token_id], dim=-1)\n",
    "    return tokenizer.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'long long ago, I was a young man who had been raised in a family'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_causal_lm_decode(gpt2_clm_model, gpt2_tokenizer, \"long long ago, I was\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['long long ago, I was a young man who had been raised in a family']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gpt2_causal_lm_decode_ref(model, tokenizer, text, nsteps=5):\n",
    "    input_ids = tokenizer(text, padding=True, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**input_ids, max_new_tokens=nsteps, do_sample=False)\n",
    "    return [tokenizer.decode(output) for output in outputs]\n",
    "\n",
    "\n",
    "texts = [\"long long ago, I was\"]\n",
    "gpt2_causal_lm_decode_ref(gpt2_clm_model, gpt2_tokenizer, texts, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(model, input_ids, batch_size, beam_size, past_probs=None):\n",
    "    \"\"\"\n",
    "    beam_search 对输入的context tokens进行一次deocde，然后保留beam_size个\n",
    "    最大概率的预测token，进行整体概率的计算，排名后，每个样本保留beam_size个\n",
    "\n",
    "    input_ids: (N, seq_len) torch.Tenosr\n",
    "        表示输出的token id, 第一次调用时为原始的context token，N = batch_size\n",
    "        后续再进行search时，N = batch_size * beam_size\n",
    "    past_probs: (batch_size*beam_size,)\n",
    "        表示每条句子的过去累积概率，第一次调用时，past_probs为None\n",
    "    \"\"\"\n",
    "    logits = model(input_ids=input_ids).logits[:, -1, :]\n",
    "    probs = F.softmax(logits, -1)\n",
    "    beam_top_ids = torch.argsort(probs, -1, True)[:, :beam_size]\n",
    "    beam_top_probs = torch.gather(probs, -1, beam_top_ids)\n",
    "\n",
    "    output_ids = torch.concat(\n",
    "        [input_ids.repeat_interleave(beam_size, 0), beam_top_ids.reshape(-1, 1)], dim=-1\n",
    "    )\n",
    "\n",
    "    if past_probs is None:\n",
    "        past_probs = beam_top_probs.reshape(-1)\n",
    "    else:\n",
    "        past_probs = past_probs.repeat_interleave(beam_size, 0)\n",
    "        past_probs = past_probs * beam_top_probs.reshape(-1)\n",
    "\n",
    "    resorted_ids = past_probs.reshape(batch_size, -1).argsort(-1, True)\n",
    "    resorted_ids = resorted_ids[:, :beam_size]\n",
    "    past_probs = torch.gather(\n",
    "        past_probs.reshape(batch_size, -1), -1, resorted_ids\n",
    "    ).reshape(-1)\n",
    "\n",
    "    output_ids = output_ids[resorted_ids.reshape(-1)]\n",
    "    return output_ids, past_probs\n",
    "\n",
    "\n",
    "# 对一个句子进行decode，不支持batch\n",
    "def gpt2_causal_lm_decode_with_beam_search(model, tokenizer, text, nsteps=5, beam=3):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    past_probs = None\n",
    "    batch_size = input_ids.size(0)\n",
    "    for _ in range(nsteps):\n",
    "        input_ids, past_probs = beam_search(\n",
    "            model, input_ids, batch_size, beam, past_probs\n",
    "        )\n",
    "    return tokenizer.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'long long ago, I was in the middle of a fight with a man who'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_causal_lm_decode_with_beam_search(\n",
    "    gpt2_clm_model, gpt2_tokenizer, \"long long ago, I was\", 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['long long ago, I was in the middle of a fight with a man who']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gpt2_causal_lm_decode_with_beam_search_reef(model, tokenizer, text, nsteps=5):\n",
    "    input_ids = tokenizer(text, padding=True, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        **input_ids, max_new_tokens=nsteps, do_sample=False, num_beams=3\n",
    "    )\n",
    "    return [tokenizer.decode(output) for output in outputs]\n",
    "\n",
    "\n",
    "texts = [\"long long ago, I was\"]\n",
    "gpt2_causal_lm_decode_with_beam_search_reef(gpt2_clm_model, gpt2_tokenizer, texts, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decode Sampling\n",
    "\n",
    "GPT模型的`generate`接口，支持以下的参数，可以用于控制生成样本的多样性\n",
    "\n",
    "* do_sample: 表示是否进行随机性采样输出\n",
    "* temperature: 用于调整logits经过softmax后的概率分布，温度越高，则分布越平均，输出多样性越强，容易乱说；温度越低，分布越聚焦，输出更稳定。\n",
    "    $$q_i = \\frac{\\exp{z_i / T}}{\\sum_j\\exp{z_j/T}}$$\n",
    "* top_k: 控制采样时，只在topK的范围内进行采样\n",
    "* top_p: 控制采样时，只在累计概率在top_p之内的范围内进行采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0420, 0.1142, 0.8438])\n",
      "tensor([0.1955, 0.2729, 0.5315])\n"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([1, 2, 4], dtype=torch.float)\n",
    "print(F.softmax(logits, -1))\n",
    "temperature = 3\n",
    "print(F.softmax(logits / temperature, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/gpt_do_sample.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
