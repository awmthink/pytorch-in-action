{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "serial-compromise",
            "metadata": {},
            "source": [
                "# Tensor的属性"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "placed-poison",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "mineral-neighborhood",
            "metadata": {},
            "source": [
                "<img src=\"./images/tensor_attributes.png\" width=550px>"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "1c14402d",
            "metadata": {},
            "source": [
                "## dtype\n",
                "\n",
                "torch在创建Tensor时，`dtype`的指定只支持使用`torch.[DataType]`这样的方式去指定，而不能像numpy一样，可以直接使用字符串。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "28fefdc0",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([1, 2, 3])"
                        ]
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.tensor([1, 2, 3], dtype=torch.int64)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "5d363752",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([1, 2, 3])"
                        ]
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "np.array([1, 2, 3], dtype=\"int64\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "indian-vehicle",
            "metadata": {},
            "source": [
                "其中`torch.dtype`表示Tensor的数据类型，常见的有下面9种不同的数据类型，包括了\n",
                "\n",
                "- `torch.float32`或`torch.float`，对应的Tensor类型为`torch.[cuda].FloatTensor`\n",
                "- `torch.float64`或`torch.double`，对应的Tensor类型为`torch.[cuda].DoubleTensor`\n",
                "- `torch.float16`或`torch.half`，对应的Tensor类型为`torch.cuda.HalfTensor`，不存在`torch.HalfTensor`\n",
                "- `torch.uint8`，对应的Tensor类型为`torch.[cuda].ByteTensor`\n",
                "- `torch.int8`，对应的Tensor类型为`torch.[cuda].CharTensor`\n",
                "- `torch.int16`或`torch.short`，对应的Tensor类型为`torch.[cuda].ShortTensor`\n",
                "- `torch.int32`或`torch.int`，对应的Tensor类型为`torch.[cuda].IntTensor`\n",
                "- `torch.int64`或`torch.long`，对应的Tensor类型为`torch.[cuda].LongTensor`\n",
                "- `torch.bool`，对应的Tensor类型为`torch.[cuda].BoolTensor`\n",
                "\n",
                "还有几种数据类型，用的比较少：\n",
                "\n",
                "- `torch.bfloat16`，对应的Tensor类型为`torch.[cuda].BFloat16Tensor`\n",
                "- `torch.complex32`，对应的Tensor类型为`torch.[cuda].FloatTensor`\n",
                "- `torch.complex64`，对应的Tensor类型为`torch.[cuda].DoubleTensor`\n",
                "- `torch.complex128`或`torch.cdouble`，对应的Tensor类型为`torch.cuda.HalfTensor`，不存在`torch.HalfTensor`\n",
                "\n",
                "其中[`bfloat16`](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format)是一种和IEEE half-precision 16-bit float规定不一致的16Bit浮点数格式，它是直接对32位的IEEE 754规定的单精度float32的格式进行截取形成的，它是为机器学习系统特别定制的，它的组成是：\n",
                "- 1位符号位\n",
                "- 8个指数位\n",
                "- 7个小数位\n",
                "\n",
                "Neural networks are more sensitive to the size of the exponent than the size of the mantissa. To ensure identical behavior for underflows, overflows, and `NaNs`, `bfloat16` has the same exponent size as `float32`. `bfloat16` handles denormals differently from `float32`, it flushes them to zero. Unlike `float16`, which typically requires special handling like loss scaling, `bfloat16` is a drop-in replacement for `float32` when training and running deep neural networks.\n",
                "\n",
                "简而言之，`bfloat16`表示的数值范围更大，但是精度不如`float16`\n",
                "\n",
                "![floating point formats](./images/floating_point_formats.png)`\n",
                "\n",
                "在不同的机器上，因为CPU架构等不同，Tensor的很多构建函数，对上面的部分`dtype`有可能是不支持的，比如`arange`函数就不支持在`cpu`上创建一个`float16`的Tensor"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "joint-national",
            "metadata": {},
            "source": [
                "torch.arange(1,10, dtype=torch.float16, device=torch.device('cpu'))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "limiting-standing",
            "metadata": {},
            "source": [
                "由于CUDA对半精度支持的比较好，所以在'cuda'上创建，反而没有什么问题"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "ready-needle",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.], device='cuda:0',\n",
                            "       dtype=torch.float16)"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.arange(1, 10, dtype=torch.float16, device=torch.device(\"cuda\"))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "working-commerce",
            "metadata": {},
            "source": [
                "## device\n",
                "\n",
                "`torch.device`表示的是Tensor的数据存储的设备，其中分为'cpu'和'cuda'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "advisory-diagnosis",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "device(type='cpu')"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.device(\"cpu\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "funny-acoustic",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "device(type='cuda', index=0)"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.device(\"cuda:0\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "aggressive-inspection",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "device(type='cuda', index=0)"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.device(type=\"cuda\", index=0)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "cosmetic-baptist",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([1, 2, 3, 4, 5, 6], device='cuda:0')"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.tensor([1, 2, 3, 4, 5, 6], device=torch.device(\"cuda:0\"))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "sought-algorithm",
            "metadata": {},
            "source": [
                "## layout\n",
                "\n",
                "layout表示Tensor内部数据存储的内部布局，目前还是一个不成熟(beta)的特性，目前支持\n",
                "\n",
                "- torch.strided\n",
                "- torch.sparse_coo\n",
                "\n",
                "现在主要用的就是面向dense Tensor的`torch.strided`，Tensor的Strides是一个list，它代表每个dimension上两邻两个idx之间的跨度(元素个数)。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "abandoned-spanking",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(20, 5, 1)"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.arange(60).reshape(3, 4, 5).stride()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "127fa16a",
            "metadata": {},
            "source": [
                "和numpy不同的是，torch中的stride以元素的个数来表示跨度，而numpy则是用字节数量来表示跨度"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "467f02b8",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(160, 40, 8)"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "np.arange(60).reshape(3, 4, 5).strides"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "painful-authorization",
            "metadata": {},
            "source": [
                "## Tensor属性转换\n",
                "\n",
                "我们可以使用`to`方法来指定新的属性后，生成新的Tensor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "minute-cassette",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "torch.int64 cpu\n",
                        "torch.float32 cuda:0\n"
                    ]
                }
            ],
            "source": [
                "device_cuda = torch.device(\"cuda\")\n",
                "data = torch.tensor([1])\n",
                "print(data.dtype, data.device)\n",
                "data = data.to(dtype=torch.float32, device=device_cuda)\n",
                "print(data.dtype, data.device)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "entertaining-aquatic",
            "metadata": {},
            "source": [
                "## Tensor的形状\n",
                "\n",
                "Tensor除了具有3个标准的属性外，一旦我们创建了一个Tensor，那么它就会具有一些形状相关的属性。\n",
                "\n",
                "- t.shape: 返回的是一个torch.Size(tuple)类型的结果，表示每一维的维度值\n",
                "- t.size(): 和t.shape一致\n",
                "- t.ndim：返回Tensor有多少维\n",
                "- t.numel()：它是一个方法，返回Tensor内有多少个元素\n",
                "- len(t)：返回的是Tensor在第0维上的维度值"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "headed-dylan",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "shape of t is torch.Size([2, 3, 4])\n",
                        "size of t is torch.Size([2, 3, 4])\n",
                        "strides of t is (12, 4, 1)\n",
                        "strides of axes1 of t is 4\n",
                        "ndim of t is 3\n",
                        "numel of t is 24\n",
                        "len of t is 2\n"
                    ]
                }
            ],
            "source": [
                "t = torch.empty(2, 3, 4)\n",
                "print(f\"shape of t is {t.shape}\")\n",
                "print(f\"size of t is {t.size()}\")\n",
                "print(f\"strides of t is {t.stride()}\")\n",
                "print(f\"strides of axes{1} of t is {t.stride(1)}\")\n",
                "print(f\"ndim of t is {t.ndim}\")\n",
                "print(f\"numel of t is {t.numel()}\")\n",
                "print(f\"len of t is {len(t)}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "moderate-country",
            "metadata": {},
            "source": [
                "# Tensor的创建"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "initial-marine",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "neural-outline",
            "metadata": {},
            "source": [
                "在Pytorch中我们可以有多种方法来创建Tensor，常用的包括下面几种：\n",
                "\n",
                "- 从已有的scalar、list、tuple、numpy.array来创建\n",
                "- 用`arange`、`linspace`、`logspace`等创建一维数列Tensor\n",
                "- 用`ones`、`zeros`、`eye`、`full`、`empty`等来创建特别填充值的多维Tensor\n",
                "- 用随机数来创建指定形状的Tensor\n",
                "\n",
                "![](./images/tensor_creation.png)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "personal-energy",
            "metadata": {},
            "source": [
                "## 从现有数据来创建一个Tensor\n",
                "\n",
                "我们可以使用`torch.tensor()`函数来从已有的一个array_like的data来创建一个Tensor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "awful-thesis",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([1, 2, 3, 4, 5])"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# 从list创建\n",
                "torch.tensor([1, 2, 3, 4, 5])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "random-advocacy",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([1, 2, 3])"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# 从tuple创建\n",
                "torch.tensor((1, 2, 3))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "conventional-visibility",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([1., 2., 3., 4., 5.], device='cuda:0')"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# 从numpy.array创建，同时指定dtype和device\n",
                "torch.tensor(np.array([1, 2, 3, 4, 5]), dtype=torch.float32, device=\"cuda:0\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "stable-peeing",
            "metadata": {},
            "source": [
                "需要注意的是，无论是从python的内置序列创建，还是从numpy.array来创建，创建出来的Tensor都是复制了原数据的内容。\n",
                "\n",
                "如果我们希望，创建的Tensor不额外分配存储空间，而是和之前的numpy.array共享存储，那么可以使用`as_tensor`方法"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "secret-geneva",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[6 2 3 4 5]\n"
                    ]
                }
            ],
            "source": [
                "arr = np.array([1, 2, 3, 4, 5])\n",
                "t = torch.as_tensor(arr)\n",
                "# 对于Tensor的数据改动，也会影响在ndarray上\n",
                "t[0] = 6\n",
                "print(arr)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "established-mystery",
            "metadata": {},
            "source": [
                "不过使用`as_tensor`后，能共享底层存储的，前提是，as_type方法中指定的`dtype`和`device`和原ndarry是一致的。由于numpy不支持cuda，所以这样只能创建cpu上的tensor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "fresh-extreme",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[1 2 3 4 5]\n"
                    ]
                }
            ],
            "source": [
                "arr = np.array([1, 2, 3, 4, 5])\n",
                "t = torch.as_tensor(arr, dtype=torch.float32)  # 这种情况下，并不会共享底层存储\n",
                "t[0] = 6\n",
                "print(arr)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "circular-district",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ndarray的默认整数类型为:int64\n",
                        "tensor的默认整数类型为: torch.int64\n",
                        "ndarray的默认整数类型为:float64\n",
                        "tensor的默认整数类型为: torch.float32\n"
                    ]
                }
            ],
            "source": [
                "il = [1, 2, 3, 4, 5]\n",
                "print(f\"ndarray的默认整数类型为:{np.array(il).dtype}\")\n",
                "print(f\"tensor的默认整数类型为: {torch.tensor(il).dtype}\")\n",
                "\n",
                "fl = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
                "print(f\"ndarray的默认整数类型为:{np.array(fl).dtype}\")\n",
                "print(f\"tensor的默认整数类型为: {torch.tensor(fl).dtype}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "fece622f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/tmp/ipykernel_879717/3616407157.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
                        "  b = torch.tensor(a, dtype=torch.float, device=\"cuda:1\")\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "tensor([1., 2., 3.], device='cuda:1')"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# 从另外一个tensor来创建tensor\n",
                "a = torch.tensor([1, 2, 3])\n",
                "b = torch.tensor(a, dtype=torch.float, device=\"cuda:1\")\n",
                "b"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "8b1990fc",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([1, 2, 3], device='cuda:1')\n"
                    ]
                }
            ],
            "source": [
                "b = a.clone()\n",
                "b = b.to(device=\"cuda:1\")\n",
                "print(b)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "unauthorized-today",
            "metadata": {},
            "source": [
                "## `torch.tensor()`和`torch.Tensor()`的区别\n",
                "\n",
                "`torch.Tensor`实际上是`torch.FloatTensor`，用它来创建新的Tensor时，实际调用的是构造函数，它会默认以`torch.float32`来作为`dtype`。而`torch.tensor`会根据`data`的类型自动推断。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "right-andrews",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "torch.float32\n",
                        "torch.int64\n"
                    ]
                }
            ],
            "source": [
                "l = [1, 2, 3, 4, 5]\n",
                "print(torch.Tensor(l).dtype)\n",
                "print(torch.tensor(l).dtype)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "stainless-garden",
            "metadata": {},
            "source": [
                "## 创建特别填充值的Tensor"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "dramatic-driver",
            "metadata": {},
            "source": [
                "### torch.arange\n",
                "\n",
                "torch.arange(start=0, end, step=1)用于创建一个区间范围的Tensor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "perfect-alpha",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([0, 1, 2, 3, 4])\n",
                        "tensor([1, 2, 3, 4])\n",
                        "tensor([ 1,  4,  7, 10, 13, 16, 19])\n"
                    ]
                }
            ],
            "source": [
                "print(torch.arange(5))\n",
                "print(torch.arange(1, 5))\n",
                "print(torch.arange(1, 20, 3))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "crucial-mission",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([1.0000, 1.5000, 2.0000, 2.5000, 3.0000])"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# 如果start、end以及step中有浮点数，则创建出来的是FloatTensor\n",
                "torch.arange(1, 3.5, 0.5)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "indie-excellence",
            "metadata": {},
            "source": [
                "注意上面是没有包括3.5那个点的"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "robust-general",
            "metadata": {},
            "source": [
                "### torch.linspace\n",
                "\n",
                "`torch.linspace`与`torch.arange`有点类似，都指定一个起点，一个终点，和一个步长。但`linspace`里步长最终指定了生成的一维Tensor中元素的个数\n",
                "\n",
                "```python\n",
                "linspace(start(float),end(float),steps(int))\n",
                "```\n",
                "另外需要注意的是`torch.linspace`生成的一定是一个浮点数的Tensor，而且和`torch.arange`不同的是：`linspace`生成的Tensor是包括末点值的（inclusive）"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "cognitive-origin",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([ 3.0000,  4.7500,  6.5000,  8.2500, 10.0000])"
                        ]
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.linspace(3, 10, 5)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "enhanced-breach",
            "metadata": {},
            "source": [
                "### torch.logspace\n",
                "\n",
                "`torch.logspace`和`torch.linspace`行为类似，区别在于`logspace`生成的序列的范围的起始与终点是一个以`base`为底，`start`和`end`为指数的数字。\n",
                "\n",
                "```python\n",
                "logspace(start, end, stpes, base=10.0) -> Tensor\n",
                "```"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "innovative-graduation",
            "metadata": {},
            "source": [
                "### torch.ones、torch.zeros、torch.emtpy\n",
                "\n",
                "它们三个都是用于创建一个指定`size`的Tensor，分别以1、0和未初始化的值来填充\n",
                "\n",
                "它们三个返回的都是`FloatTensor`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "invisible-lesbian",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([[1., 1.],\n",
                        "        [1., 1.]])\n",
                        "tensor([[0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0.]])\n",
                        "tensor([[1.8560e-27, 4.5563e-41, 2.6275e-33],\n",
                        "        [0.0000e+00, 1.9838e-31, 0.0000e+00],\n",
                        "        [5.2258e-27, 4.5563e-41, 1.9818e-31]])\n"
                    ]
                }
            ],
            "source": [
                "print(torch.ones((2, 2)))\n",
                "print(torch.zeros((3, 4)))\n",
                "print(torch.empty((3, 3)))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "54e6d4db",
            "metadata": {},
            "source": [
                "`torch.ones/zeros/empty`支持`torch.ones(d1,d2,...)`这种调用方法，而`numpy`则不支持。"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "statewide-brooklyn",
            "metadata": {},
            "source": [
                "### torch.eye\n",
                "\n",
                "`torch.eye`返回的是一个2d的对角线为1，其他值都为0的Float矩阵Tensor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "fatal-happiness",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[1., 0., 0., 0.],\n",
                            "        [0., 1., 0., 0.],\n",
                            "        [0., 0., 1., 0.],\n",
                            "        [0., 0., 0., 1.]])"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.eye(4)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "interim-following",
            "metadata": {},
            "source": [
                "### torch.full\n",
                "\n",
                "`torch.full`返回的是一个指定`size`和填充值的Tensor，Tensor的dtype是由填充值的类型来推导的。\n",
                "\n",
                "```python\n",
                "'''\n",
                "Args:\n",
                "  size(int...): a list ,tuple or torch.Size\n",
                "  fill_vale(Scalar)\n",
                "'''\n",
                "full(size, fill_value)\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "technological-highway",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[1., 1., 1.],\n",
                            "        [1., 1., 1.]])"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.full((2, 3), 1.0)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "chinese-ranch",
            "metadata": {},
            "source": [
                "## 使用随机数来创建Tensor"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "resident-physiology",
            "metadata": {},
            "source": [
                "### torch.normal\n",
                "\n",
                "`torch.normal`返回一个正态分布产生在的随机数填充的Tensor，它一共有4种参数传递方式\n",
                "\n",
                "第一种是:\n",
                "\n",
                "```python\n",
                "'''\n",
                "Args:\n",
                "    mean (Tensor): the tensor of per-element means\n",
                "    std (Tensor): the tensor of per-element standard deviations\n",
                "'''\n",
                "norm(mean, std)\n",
                "```\n",
                "生成的Tensor的size和mean和std的size是一致的，其中每个元素都是通过对应位置的mean和std形成的正态分布来随机产生的。\n",
                "\n",
                "mean和std两个Tensor的shape不需要一致，但是元数数量需要一致，当shape不一致时，以mean的shape作为最终生成的Tensor的shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "super-louisville",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "<ipython-input-14-49245019fc67>:3: UserWarning: std and mean have the same number of elements, but are not broadcastable. This was previously a supported mode of operation, but is now deprecated and the support will be removed in version 1.6 release. Note that the current implementation reshapes std to the shape of mean, which may be incur data copies. Please ensure that std and mean are broadcastable to avoid these issues. (Triggered internally at  /opt/conda/conda-bld/pytorch_1616554793803/work/aten/src/ATen/native/DistributionTemplates.h:189.)\n",
                        "  torch.normal(mean, std)\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "tensor([[ 0.0000,  0.9364,  1.9420,  2.9102],\n",
                            "        [ 4.0290,  4.3689,  5.2698,  7.6781],\n",
                            "        [ 7.8305,  8.5575, 10.7068, 12.5202]])"
                        ]
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "mean = torch.arange(12).reshape(3, 4).to(dtype=torch.float32)\n",
                "std = torch.linspace(0, 1, mean.numel())\n",
                "torch.normal(mean, std)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "tutorial-contractor",
            "metadata": {},
            "source": [
                "第二种是：\n",
                "\n",
                "```python\n",
                "'''\n",
                "Args:\n",
                "    mean (float, optional): the mean for all distributions\n",
                "    std (Tensor): the tensor of per-element standard deviations\n",
                "'''\n",
                "normal(mean=0.0, std, *, out=None) -> Tensor\n",
                "```\n",
                "这种参数传递用法，与上面的区别就是mean变成一个Scalar，那么说明每个元素来共享一个mean值。\n",
                "\n",
                "在这种情况下，生成的Tensor的shape就行std保持一致的了。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "olive-third",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([ 1.0000,  0.9540,  0.7664,  1.0074,  0.7044,  0.8077,  1.3776,  2.2947,\n",
                            "         2.7416,  2.0305,  2.3241, -0.1520])"
                        ]
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.normal(1.0, std)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "excellent-sociology",
            "metadata": {},
            "source": [
                "第三种：\n",
                "\n",
                "```python\n",
                "'''\n",
                "Args:\n",
                "    mean (Tensor): the tensor of per-element means\n",
                "    std (float, optional): the standard deviation for all distributions\n",
                "'''\n",
                "normal(mean, std=1.0, *, out=None) -> Tensor\n",
                "```\n",
                "这种情况和第二种情况，恰恰相反了，std变成了每个元素共享的。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "ambient-shame",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[-0.2012,  1.6828,  1.9805,  2.8949],\n",
                            "        [ 2.7657,  6.4303,  5.6291,  7.1419],\n",
                            "        [ 8.1036,  9.3677, 10.5676, 10.7179]])"
                        ]
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.normal(mean, 0.5)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "acoustic-function",
            "metadata": {},
            "source": [
                "第四种：\n",
                "\n",
                "```python\n",
                "'''\n",
                "Args:\n",
                "    mean (float): the mean for all distributions\n",
                "    std (float): the standard deviation for all distributions\n",
                "    size (int...): a sequence of integers defining the shape of the output tensor.\n",
                "'''\n",
                "normal(mean, std, size, *, out=None) -> Tensor\n",
                "```\n",
                "这种情况下，所有的元素都共享mean和std，最终Tensor的形状是由`size`来决定的"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "statewide-defeat",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[ 0.3704, -1.0554, -0.4917,  0.3783],\n",
                            "        [ 0.3406,  1.5351, -0.5526, -0.9879],\n",
                            "        [-0.5488,  1.2171, -0.1122,  0.2516]])"
                        ]
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.normal(0, 1, (3, 4))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "expensive-ground",
            "metadata": {},
            "source": [
                "### torch.rand、torch.randn\n",
                "\n",
                "`rand`直接生成指定形状的Tensor，其中每个元素都是由`[0,1)`均匀分布来随机产生。\n",
                "\n",
                "`randn`直接生成指定形状的Tensor，其中每个元素都是由标准正态分布来随机产生。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "difficult-exclusive",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[0.3252, 0.5300, 0.3352, 0.1053],\n",
                            "        [0.3589, 0.9020, 0.8210, 0.5692],\n",
                            "        [0.3691, 0.9678, 0.1090, 0.3802]])"
                        ]
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.rand(3, 4)  # 或者 torch.randn((3,4))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "resident-austin",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[-1.8126,  0.3052, -1.5136,  0.6127],\n",
                            "        [-0.5126,  0.1974, -0.3502, -1.2904],\n",
                            "        [-0.2174, -0.3294,  0.3271, -1.2221]])"
                        ]
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.randn(3, 4)  # 或者 torch.randn((3,4))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "regional-afternoon",
            "metadata": {},
            "source": [
                "### torch.randint\n",
                "\n",
                "产生一个由`[low,high)`区间均匀分布随机数填充的LongTensor\n",
                "\n",
                "```python\n",
                "randint(low=0,high,size,...)\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "large-vaccine",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[2, 3, 8, 6],\n",
                            "        [4, 4, 8, 3],\n",
                            "        [7, 5, 8, 1]])"
                        ]
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.randint(1, 10, (3, 4))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "danish-prime",
            "metadata": {},
            "source": [
                "### torch.randperm\n",
                "\n",
                "生成一个随机全排列的一维的LongTensor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "strange-biography",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([ 9, 10,  3,  5,  7,  4, 11,  0,  6,  1,  2,  8])"
                        ]
                    },
                    "execution_count": 21,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.randperm(12)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "directed-simon",
            "metadata": {},
            "source": [
                "## 使用`xx_like`系列创建相同形态的Tensor\n",
                "\n",
                "除了shape保持一致外，`dtype`、`layout`、`device`等，若无特别指定，则也与源Tensor保持一致。\n",
                "\n",
                "```python\n",
                "torch.zeros_like(input, ..) # 返回与input相同size的零矩阵\n",
                "\n",
                "torch.ones_like(input, ..) #返回与input相同size的单位矩阵\n",
                "\n",
                "torch.full_like(input, fill_value, …) #返回与input相同size，单位值为fill_value的矩阵\n",
                "\n",
                "torch.empty_like(input, …) # 返回与input相同size,并被未初始化的数值填充的tensor\n",
                "\n",
                "torch.rand_like(input, dtype=None, …) #返回与input相同size的tensor, 填充均匀分布的随机数值\n",
                "\n",
                "torch.randint_like(input, low=0, high, dtype=None, …) #返回与input相同size的tensor, 填充[low, high)均匀分布的随机数值\n",
                "\n",
                "torch.randn_like(input, dtype=None, …) #返回与input相同size的tensor, 填充标准正态分布的随机数值\n",
                "\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "id": "spatial-montgomery",
            "metadata": {},
            "outputs": [],
            "source": [
                "src = torch.randn(4, 5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "id": "forty-membrane",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[0., 0., 0., 0., 0.],\n",
                            "        [0., 0., 0., 0., 0.],\n",
                            "        [0., 0., 0., 0., 0.],\n",
                            "        [0., 0., 0., 0., 0.]])"
                        ]
                    },
                    "execution_count": 23,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.zeros_like(src)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "id": "overall-dragon",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[1, 1, 1, 1, 1],\n",
                            "        [1, 1, 1, 1, 1],\n",
                            "        [1, 1, 1, 1, 1],\n",
                            "        [1, 1, 1, 1, 1]], dtype=torch.int32)"
                        ]
                    },
                    "execution_count": 24,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.ones_like(src, dtype=torch.int)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "id": "backed-budapest",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[1., 2., 3., 4., 5.],\n",
                            "        [0., 0., 0., 0., 0.],\n",
                            "        [0., 0., 0., 0., 0.],\n",
                            "        [0., 0., 0., 0., 0.]], device='cuda:0')"
                        ]
                    },
                    "execution_count": 25,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.empty_like(src, device=\"cuda:0\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "id": "pointed-williams",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[42., 42., 42., 42., 42.],\n",
                            "        [42., 42., 42., 42., 42.],\n",
                            "        [42., 42., 42., 42., 42.],\n",
                            "        [42., 42., 42., 42., 42.]])"
                        ]
                    },
                    "execution_count": 26,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# 这里即使full_value是int类型，但生成的Tensor，依然是用的src的dtype\n",
                "torch.full_like(src, 42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "id": "noticed-spanking",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[0.6990, 0.0617, 0.0418, 0.0900, 0.9830],\n",
                            "        [0.8164, 0.1852, 0.2386, 0.2956, 0.3999],\n",
                            "        [0.4655, 0.1092, 0.7640, 0.1811, 0.5279],\n",
                            "        [0.9435, 0.7238, 0.1428, 0.0752, 0.1228]])"
                        ]
                    },
                    "execution_count": 27,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.rand_like(src)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "id": "norwegian-notice",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[ 0.9366,  0.1560,  0.6002,  1.3912,  0.5083],\n",
                            "        [ 1.1414,  2.5705, -0.3684, -0.0108, -0.2299],\n",
                            "        [-0.7489,  0.0045,  0.2403,  1.0501,  1.1373],\n",
                            "        [-0.4165,  1.2640,  0.7514, -0.1586,  0.4076]])"
                        ]
                    },
                    "execution_count": 28,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.randn_like(src)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "id": "committed-soundtrack",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[2., 1., 8., 1., 5.],\n",
                            "        [7., 7., 2., 6., 4.],\n",
                            "        [9., 7., 6., 5., 3.],\n",
                            "        [4., 4., 8., 7., 6.]])"
                        ]
                    },
                    "execution_count": 29,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.randint_like(src, 1, 10)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "amateur-dream",
            "metadata": {},
            "source": [
                "# Tensor的操作\n",
                "\n",
                "Pytorch中的Tensor大约支持100种以上的操作，其中包括了数学运算、线性代数、矩阵操作（转置、索引、切片等），这些操作都可以跑在CPU或GPU上，这也是Pytorch Tensor的强大之处。\n",
                "\n",
                "![](./images/tensor_operatrions.png)\n",
                "\n",
                "我们可以通过这个[页面](https://pytorch.org/docs/stable/torch.html)，来对Tensor支持的所有操作做个大概的了解。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "passing-kelly",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "derived-holocaust",
            "metadata": {},
            "source": [
                "## 索引访值\n",
                "\n",
                "我们可以像访问Numpy.ndarray一样，对torch.Tensor进行各种下标索引与范围切片。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "canadian-insulin",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "t: tensor([[ 0,  1,  2,  3],\n",
                        "        [ 4,  5,  6,  7],\n",
                        "        [ 8,  9, 10, 11]])\n",
                        "取t的第2行的所有元素: tensor([4, 5, 6, 7])\n",
                        "取t的最后一列的所有元素: tensor([ 3,  7, 11])\n",
                        "取t的第2列到最后一列的所有元素: tensor([[ 2,  3],\n",
                        "        [ 6,  7],\n",
                        "        [10, 11]])\n",
                        "取t的位置(2,3)上的元素: 11\n"
                    ]
                }
            ],
            "source": [
                "t = torch.arange(12).reshape(3, 4)\n",
                "print(f\"t: {t}\")\n",
                "print(f\"取t的第2行的所有元素: {t[1]}\")\n",
                "print(f\"取t的最后一列的所有元素: {t[:, -1]}\")\n",
                "print(f\"取t的第2列到最后一列的所有元素: {t[:, 2:]}\")\n",
                "print(f\"取t的位置(2,3)上的元素: {t[2, 3]}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "necessary-adoption",
            "metadata": {},
            "source": [
                "**单一元素的Tensor**\n",
                "\n",
                "当我们通过索引访问Tensor的单一元素时，得到的实际是一个`Tensor`类型的对象，它并不是python中的内置数据类型，我们可以通过Tensor的`item()`方法来获取python对象的标量。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "sound-pattern",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "torch.Tensor"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "type(t[2, 3])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "95e58023",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "torch.Size([])"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "t[2, 3].shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "chief-thermal",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "int"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "type(t[2, 3].item())"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "through-marketplace",
            "metadata": {},
            "source": [
                "## 组合与分片\n",
                "\n",
                "### torch.cat\n",
                "\n",
                "```\n",
                "cat(tensors, dim=0) -> Tensor\n",
                "```\n",
                "\n",
                "`torch.cat`将给定义的tensor的序列(tensors)，按给定义的维度上合并起来，这就要求，这些tensor，除了合并的维度，其他的维度必须一致。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "every-parade",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[ 1.8282, -1.3961, -1.6330],\n",
                            "        [-0.5860, -1.5504,  0.5470],\n",
                            "        [-0.4857, -0.4318, -0.0308],\n",
                            "        [ 0.1696, -0.7582,  0.8282],\n",
                            "        [-0.1592, -0.7631,  2.8051]])"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "t1 = torch.randn(2, 3)\n",
                "t2 = torch.randn(3, 3)\n",
                "torch.cat([t1, t2], dim=0)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "cardiac-luther",
            "metadata": {},
            "source": [
                "### torch.stack\n",
                "\n",
                "`torch.stack`和`torch.cat`接口用法一致，但它并不是在原有的维度上拼接，而是直接扩展一个新的维度。\n",
                "\n",
                "这就要求，序列中的tensor在维度上必须一致。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "peripheral-membrane",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[[ 0.9310, -2.5397,  0.7603],\n",
                            "         [ 0.5423,  0.0121,  2.4951]],\n",
                            "\n",
                            "        [[-0.9212,  1.1312,  0.4553],\n",
                            "         [-0.3836, -2.2080, -0.8785]]])"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "t1 = torch.randn(2, 3)\n",
                "t2 = torch.randn(2, 3)\n",
                "torch.stack([t1, t2], dim=0)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "complimentary-ordinance",
            "metadata": {},
            "source": [
                "### torch.split\n",
                "```python\n",
                "split(tensor, split_size_or_sections, dim=0)\n",
                "```\n",
                "`split`将tensor按指定的维度，分拆为多个Tensor的元组，拆分的块chunk的大小是splite_size指定的。可能出现不能整分的情况，这时候最后一块大小一般小于splite_size\n",
                "\n",
                "split出来的Tensor是原tensor的一个view"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "identical-discipline",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[0, 1],\n",
                            "        [2, 3],\n",
                            "        [4, 5],\n",
                            "        [6, 7],\n",
                            "        [8, 9]])"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "a = torch.arange(10).view(5, 2)\n",
                "a"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "subject-progressive",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(tensor([[0, 1],\n",
                            "         [2, 3]]),\n",
                            " tensor([[4, 5],\n",
                            "         [6, 7]]),\n",
                            " tensor([[8, 9]]))"
                        ]
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.split(a, 2)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "narrative-hearts",
            "metadata": {},
            "source": [
                "`split_size_or_sections`也可能是一个list(int)，这时候，它的每个元素，代表每个chunk的大小"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "starting-algorithm",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(tensor([[0, 1]]),\n",
                            " tensor([[2, 3],\n",
                            "         [4, 5],\n",
                            "         [6, 7]]),\n",
                            " tensor([[8, 9]]))"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "a1, a2, a3 = torch.split(a, (1, 3, 1))\n",
                "a1, a2, a3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "specified-messaging",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[42,  1],\n",
                            "        [ 2,  3],\n",
                            "        [ 4,  5],\n",
                            "        [ 6,  7],\n",
                            "        [ 8,  9]])"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# 切分出来的tensor和原tensor是共享存储的\n",
                "a1[0, 0] = 42\n",
                "a"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "confidential-charger",
            "metadata": {},
            "source": [
                "### torch.chunk\n",
                "\n",
                "```python\n",
                "chunk(input, chunks, dim=0) -> List of Tensors\n",
                "```\n",
                "`chunk`和`split`功能类似，不同在于，chunk的第二的参数，直接指定的是chunk的数量，最后一个chunk的数量可能会少一些。也有可能`axis[dim]<chunks`，那么就直接切分为`axis[dim]`个。\n",
                "\n",
                "切分出来的这些Tensor和原Tensor都是共享底层存储的，也就是说每个chunk都是原Tensor的一个view。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "ba0a82b6",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "torch.Size([5, 2])\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "2"
                        ]
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "print(a.shape)\n",
                "len(a.chunk(3, dim=1))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "general-sitting",
            "metadata": {},
            "source": [
                "## 变换操作\n",
                "\n",
                "### torch.reshape\n",
                "\n",
                "```python\n",
                "reshape(input, shape) -> Tensor\n",
                "```\n",
                "`reshape`返回一个和原Tensor具有相同数据，相同数量的Tensor，只是shape不一致。"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "gross-botswana",
            "metadata": {},
            "source": [
                "### torch.view\n",
                "\n",
                "torch.view vs. torch.reshape\n",
                "\n",
                "`reshape`可以用在`compact`或`non-compact`的tensor上，而`view`只能用在`compact`的tensor上。`reshape`如果作用于`non-compact`的tensor上，则会产生一个copy\n",
                "\n",
                "torch.view has existed for a long time. It will return a tensor with the new shape. The returned tensor will share the underling data with the original tensor. See the documentation here.\n",
                "\n",
                "On the other hand, it seems that torch.reshape has been introduced recently in version 0.4. According to the document, this method will\n",
                "\n",
                "> Returns a tensor with the same data and number of elements as input, but with the specified shape. When possible, the returned tensor will be a view of input. Otherwise, it will be a copy. Contiguous inputs and inputs with compatible strides can be reshaped without copying, but you should not depend on the copying vs. viewing behavior.\n",
                "\n",
                "It means that torch.reshape may return a copy or a view of the original tensor. You can not count on that to return a view or a copy. According to the developer:\n",
                "\n",
                "> if you need a copy use clone() if you need the same storage use view(). The semantics of reshape() are that it may or may not share the storage and you don't know beforehand.\n",
                "\n",
                "Another difference is that reshape() can operate on both contiguous and non-contiguous tensor while view() can only operate on contiguous tensor. Also see here about the meaning of contiguous."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "herbal-opening",
            "metadata": {},
            "source": [
                "### torch.transpose\n",
                "\n",
                "```python\n",
                "transpose(input, dim0, dim1) -> Tensor\n",
                "```\n",
                "转置input的指定的2个维度，返回的Tensor和原来的Tensor共享存储"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "toxic-delay",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[[0.3844, 0.1842, 0.3928, 0.7579],\n",
                            "         [0.8694, 0.9604, 0.4230, 0.6530],\n",
                            "         [0.8197, 0.6693, 0.0917, 0.8025]],\n",
                            "\n",
                            "        [[0.5022, 0.3150, 0.9407, 0.3442],\n",
                            "         [0.0475, 0.8234, 0.0500, 0.8506],\n",
                            "         [0.1328, 0.9862, 0.7935, 0.4214]]])"
                        ]
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "x = torch.rand(2, 3, 4)\n",
                "x"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "boxed-jaguar",
            "metadata": {},
            "outputs": [],
            "source": [
                "y = torch.transpose(x, 0, 2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "c5d32c94",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(12, 4, 1)\n",
                        "(1, 4, 12)\n"
                    ]
                }
            ],
            "source": [
                "print(x.stride())\n",
                "print(y.stride())"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "built-beach",
            "metadata": {},
            "source": [
                "### torch.permute"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "id": "c2b403c3",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "torch.Size([4, 2, 3])"
                        ]
                    },
                    "execution_count": 23,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.permute(x, (2, 0, 1)).shape"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "93080d1e",
            "metadata": {},
            "source": [
                "### squeeze和unsqueeze\n",
                "\n",
                "squeeze在指定的维度上添加一维，而unsqueeze则在指定的维度上去掉`size=1`的维度，如果对应维度上的size不等于1，则不做任何操作"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "id": "7c8896e5",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "x shape: torch.Size([2, 3]), \n",
                        "y (x.unsqueeze) shape: torch.Size([1, 2, 1, 3])\n",
                        "unsqueeze shape torch.Size([2, 1, 3])\n"
                    ]
                }
            ],
            "source": [
                "x = torch.randn(2,3)\n",
                "y = x.unsqueeze(dim=1).unsqueeze(dim=0)\n",
                "print(f'x shape: {x.shape}, \\ny (x.unsqueeze) shape: {y.shape}')\n",
                "print('unsqueeze shape', y.squeeze(dim=0).shape)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "clinical-phoenix",
            "metadata": {},
            "source": [
                "### contiguous\n",
                "\n",
                "There are a few operations on Tensors in PyTorch that do not change the contents of a tensor, but change the way the data is organized. These operations include:\n",
                "\n",
                "`narrow()`, `view()`, `expand()` and `transpose()`\n",
                "\n",
                "For example: when you call transpose(), PyTorch doesn't generate a new tensor with a new layout, it just modifies meta information in the Tensor object so that the offset and stride describe the desired new shape. In this example, the transposed tensor and original tensor share the same memory:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "practical-example",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor(42.)\n"
                    ]
                }
            ],
            "source": [
                "x = torch.randn(3, 2)\n",
                "y = torch.transpose(x, 0, 1)\n",
                "x[0, 0] = 42\n",
                "print(y[0, 0])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "75c7c0ae",
            "metadata": {},
            "source": [
                "This is where the concept of contiguous comes in. In the example above, x is contiguous but y is not because its memory layout is different to that of a tensor of same shape made from scratch. Note that the word \"contiguous\" is a bit misleading because it's not that the content of the tensor is spread out around disconnected blocks of memory. Here bytes are still allocated in one block of memory but the order of the elements is different!\n",
                "\n",
                "When you call contiguous(), it actually makes a copy of the tensor such that the order of its elements in memory is the same as if it had been created from scratch with the same data.\n",
                "\n",
                "Normally you don't need to worry about this. You're generally safe to assume everything will work, and wait until you get a RuntimeError: input is not contiguous where PyTorch expects a contiguous tensor to add a call to contiguous()."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "interior-morrison",
            "metadata": {},
            "source": [
                "## 降维操作"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "hundred-syria",
            "metadata": {},
            "source": [
                "### torch.mean\n",
                "\n",
                "```python\n",
                "'''\n",
                "Args:\n",
                "  input (Tensor): the input tensor.\n",
                "  dim (int or tuple of ints): the dimension or dimensions to reduce.\n",
                "  keepdim (bool): whether the output tensor has :attr:`dim` retained or not.\n",
                "'''\n",
                "mean(input, dim, keepdim=False, *, out=None) -> Tensor\n",
                "```\n",
                "\n",
                "对input沿着`dim`的维度求均值，这样的话，指定的那个维度就会被压缩掉，如果指定了`keepdim=True`的话，那个维度会保留，值为1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "centered-murray",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[ 0.8801,  0.3556,  1.3300, -0.8489, -1.8893, -0.7004],\n",
                            "        [-1.1341,  0.7043,  0.0767, -0.9126, -0.9413, -0.5077],\n",
                            "        [-0.8743, -2.0277,  0.5664,  0.4266,  2.9812,  0.9459],\n",
                            "        [ 0.1711, -2.1501, -1.3418, -1.8992,  0.6031, -0.8814],\n",
                            "        [ 0.8263,  1.1446, -1.6875,  1.1150, -0.2767, -0.7673]])"
                        ]
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "t = torch.randn(5, 6)\n",
                "t"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "demographic-country",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([-0.0262, -0.3947, -0.2112, -0.4238,  0.0954, -0.3822])"
                        ]
                    },
                    "execution_count": 21,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# 按列的方向(dim=0)将整个Tenoor压缩成为1维的\n",
                "torch.mean(t, dim=0)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "id": "important-politics",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[-0.1455],\n",
                            "        [-0.4524],\n",
                            "        [ 0.3364],\n",
                            "        [-0.9164],\n",
                            "        [ 0.0591]])"
                        ]
                    },
                    "execution_count": 22,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.mean(t, dim=1, keepdim=True)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "appointed-october",
            "metadata": {},
            "source": [
                "对于高维Tensor，我们还可以同时对多个维度进行Reduce，求其均值。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "id": "statistical-tonight",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[[ 1.0852,  1.2012,  0.0452,  0.3130],\n",
                            "         [-0.8747, -0.2138,  1.4990, -0.9035],\n",
                            "         [-0.0255,  1.0016, -1.6872,  1.2495]],\n",
                            "\n",
                            "        [[ 1.3288,  0.9973, -1.3797,  1.6270],\n",
                            "         [ 2.6580, -0.2791,  0.3662,  1.7222],\n",
                            "         [ 2.5107,  0.3394,  1.1392,  0.2362]]])"
                        ]
                    },
                    "execution_count": 23,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "t = torch.randn(2, 3, 4)\n",
                "t"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "id": "written-setup",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([0.6523, 0.4968, 0.5955])"
                        ]
                    },
                    "execution_count": 24,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# 等价于reduce第0维，得到一个3x4的Tensor后，再reduce第1维，得到(3,)的Vector\n",
                "torch.mean(t, dim=(0, 2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "id": "million-drive",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([0.6523, 0.4968, 0.5955])"
                        ]
                    },
                    "execution_count": 25,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "t.mean(0).mean(1)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "secondary-explorer",
            "metadata": {},
            "source": [
                "### torch.sum\n",
                "\n",
                "`torch.sum`是一个和`torch.mean`用法上很像的操作，只是`sum`的reduce op变成了求和，而不是求均值。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "id": "separated-illustration",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([5.2181, 3.9745, 4.7637])"
                        ]
                    },
                    "execution_count": 26,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.sum(t, dim=(0, 2))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "31707904",
            "metadata": {},
            "source": [
                "### torch.argmax"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "id": "a96184a7",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "x: tensor([[0.0860, 0.1838, 0.4938],\n",
                        "        [0.7860, 0.8440, 0.6280]])\n",
                        "Argmax: tensor([2, 1])\n"
                    ]
                }
            ],
            "source": [
                "x = torch.rand((2,3))\n",
                "print('x:', x)\n",
                "print('Argmax:', x.argmax(dim=1))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "98fbd95c",
            "metadata": {},
            "source": [
                "### torch.maxmimu"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "id": "fe8a0a3f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "x:\n",
                        "\ttensor([[-0.0520, -1.1005,  0.4070],\n",
                        "        [-0.4881,  1.2309,  2.3318]]) \n",
                        "relu(x):\n",
                        "\ttensor([[0.0000, 0.0000, 0.4070],\n",
                        "        [0.0000, 1.2309, 2.3318]])\n"
                    ]
                }
            ],
            "source": [
                "def relu(x):\n",
                "    return torch.maximum(x, torch.tensor(0))\n",
                "\n",
                "x = torch.randn((2,3))\n",
                "print(f'x:\\n\\t{x} \\nrelu(x):\\n\\t{relu(x)}')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "ec47a468",
            "metadata": {},
            "source": [
                "## 排序"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "super-still",
            "metadata": {},
            "source": [
                "### torch.sort\n",
                "\n",
                "```python\n",
                "sort(input, dim=-1, descending=False, *, out=None) -> (Tensor, LongTensor)\n",
                "```\n",
                "`sort`对input按给定义的dim进行升序排列，返回排列后的Tensor的同时，也返回一个对应的下标的重排后的Tensor\n",
                "\n",
                "dim的默认值是Tensor的最后一维"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "id": "potential-morrison",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "torch.return_types.sort(\n",
                            "values=tensor([[ 1.1975, -0.6680, -0.8859],\n",
                            "        [ 1.7126,  0.5976,  0.1704],\n",
                            "        [ 1.4601,  1.1260, -0.7816]]),\n",
                            "indices=tensor([[0, 2, 1],\n",
                            "        [0, 1, 2],\n",
                            "        [1, 2, 0]]))"
                        ]
                    },
                    "execution_count": 29,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.sort(a, dim=1, descending=True)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "martial-petersburg",
            "metadata": {},
            "source": [
                "### torch.topk\n",
                "\n",
                "```python\n",
                "topk(input, k, dim=None, largest=True, sorted=True, *, out=None) -> (Tensor, LongTensor)\n",
                "```\n",
                "`topk`返回input中指定维度上，最大的k个元素，以及对应的索引。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "id": "pending-nickname",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([ 0.4168, -1.7439, -0.4161, -0.0458,  0.5801])"
                        ]
                    },
                    "execution_count": 30,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "a = torch.randn(5)\n",
                "a"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "id": "practical-review",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "torch.return_types.topk(\n",
                            "values=tensor([ 0.5801,  0.4168, -0.0458]),\n",
                            "indices=tensor([4, 0, 3]))"
                        ]
                    },
                    "execution_count": 31,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.topk(a, 3)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "sonic-groove",
            "metadata": {},
            "source": [
                "### torch.kthvalue\n",
                "\n",
                "```python\n",
                "kthvalue(input, k, dim=None, keepdim=False, *, out=None) -> (Tensor, LongTensor)\n",
                "```\n",
                "`kthvalue`计算输出Tensor的指定维度上第`k`小的元素以及下标。如果dim没有指定，则默认为Tensor的最后一维。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "august-chamber",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[ 0.7333,  0.4090, -0.4306],\n",
                            "        [-1.7655, -1.2268, -0.9078],\n",
                            "        [ 2.7441, -0.9277, -0.4792],\n",
                            "        [-0.7800, -0.5171,  0.1977]])"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "a = torch.randn(4, 3)\n",
                "a"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "cardiovascular-ticket",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "torch.return_types.kthvalue(\n",
                            "values=tensor([-0.7800, -0.9277, -0.4792]),\n",
                            "indices=tensor([3, 2, 2]))"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.kthvalue(a, 2, dim=0)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "acknowledged-invitation",
            "metadata": {},
            "source": [
                "## 原地操作(in-place)\n",
                "\n",
                "pytorch的Tensor支持了很多原地操作，它们的特点就是在方法末尾以`_`结束"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "id": "convinced-graphics",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "t1 = tensor([[1., 1., 1.],\n",
                        "        [1., 1., 1.]])\n",
                        "after plus 2: t1 = tensor([[3., 3., 3.],\n",
                        "        [3., 3., 3.]])\n"
                    ]
                }
            ],
            "source": [
                "t1 = torch.ones(2, 3)\n",
                "print(f\"t1 = {t1}\")\n",
                "t1.add_(2)\n",
                "print(f\"after plus 2: t1 = {t1}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "electric-austria",
            "metadata": {},
            "source": [
                "## 转换为其他数据类型\n",
                "\n",
                "我们可以调用`numpy`接口,返回一个numpy.ndarray的对象，可以调用`tolist`接口，返回一个list的对象"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "id": "clean-nashville",
            "metadata": {},
            "outputs": [],
            "source": [
                "t = torch.tensor([1, 2, 3, 4, 5, 6])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "id": "multiple-adapter",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([1, 2, 3, 4, 5, 6])"
                        ]
                    },
                    "execution_count": 36,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# 返回的ndarray还是和t是共享存储的\n",
                "t.numpy()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "id": "unlimited-patio",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[[1, 2, 3], [4, 5, 6]]"
                        ]
                    },
                    "execution_count": 37,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "t.reshape(2, 3).tolist()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4994f221",
            "metadata": {},
            "outputs": [],
            "source": [
                "torch.repeat_interleave"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "207b647a",
            "metadata": {},
            "source": [
                "## repeat和repeat_interleave"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "id": "b70e577b",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[0, 1, 2],\n",
                            "        [3, 4, 5]])"
                        ]
                    },
                    "execution_count": 24,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "a = torch.arange(6).reshape((2, 3))\n",
                "a"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "cefff4c1",
            "metadata": {},
            "source": [
                "`repeat(d0, d1, d2)` 将对应的维度复制多份，如果之前没有对应的维度，则可以当作原来维度为1，处理。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "id": "2432d308",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[[0, 1, 2, 0, 1, 2],\n",
                            "         [3, 4, 5, 3, 4, 5]],\n",
                            "\n",
                            "        [[0, 1, 2, 0, 1, 2],\n",
                            "         [3, 4, 5, 3, 4, 5]]])"
                        ]
                    },
                    "execution_count": 28,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "a.repeat((2, 1, 2))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "73f6fc4c",
            "metadata": {},
            "source": [
                "`repeat_interleave(n, dim)` 在对应的维度上进行复制，但复制的方式不是`[a b c a b c ]`这种，而是`[a a b b c c]`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "id": "4a924af5",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[0, 1, 2],\n",
                            "        [0, 1, 2],\n",
                            "        [3, 4, 5],\n",
                            "        [3, 4, 5]])"
                        ]
                    },
                    "execution_count": 29,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "a.repeat_interleave(2, dim=0)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "pyml",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.16"
        },
        "vscode": {
            "interpreter": {
                "hash": "3c37a17f5960fe44e0e933f0a97f50106b021053c3b378a6f2025e9a4390c58c"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
