{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "prescribed-relation",
   "metadata": {},
   "source": [
    "# Pytorch中的神经网络基本单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imperial-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "rough-amplifier",
   "metadata": {},
   "source": [
    "# nn.Module\n",
    "\n",
    "nn.Module是神经网络结构的表示，它可以表示一个层，也可以表示一个结构块，也可以表示一个完整的模型结构。\n",
    "\n",
    "> Modules can also contain other Modules, allowing to nest them in a tree structure. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "committed-acting",
   "metadata": {},
   "source": [
    "## 自定义一个layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "protected-google",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReluLayer()\n"
     ]
    }
   ],
   "source": [
    "class ReluLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.maximum(x, torch.tensor(0))\n",
    "\n",
    "relu = ReluLayer()\n",
    "print(relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "conceptual-writer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 1.4337, 2.0141],\n",
       "        [0.6990, 0.0000, 1.0046]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2, 3)\n",
    "relu(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fd7b80",
   "metadata": {},
   "source": [
    "如果我们自定义的 \"Layer\" 中有需要训练的参数，需要定义为 `nn.Parameter`类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "celtic-breach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyFCLayer()\n"
     ]
    }
   ],
   "source": [
    "# 带参数的Layer\n",
    "class MyFCLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(in_dim, out_dim))\n",
    "        self.bias = nn.Parameter(torch.randn(out_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.matmul(self.weights.data) + self.bias.data\n",
    "\n",
    "\n",
    "fclayer = MyFCLayer(25, 10)\n",
    "print(fclayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "light-archives",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  4.6710,   6.3528,   3.4600,   7.2031,  -0.0789,  -4.6998,  -1.9341,\n",
       "         -16.8910,  -7.7743,  -4.8531]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(1, 25)\n",
    "fclayer(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "vulnerable-gilbert",
   "metadata": {},
   "source": [
    "## 自定义一个Block\n",
    "\n",
    "一般情况下一个 Block 由有若干个 Layer 形成的一种特别的结构，比如 TransformerBlock, VGGBlock 等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "weird-potato",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearReluStack(\n",
      "  (stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=100, bias=True)\n",
      "    (1): ReluLayer()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReluLayer()\n",
      "    (4): MyFCLayer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LinearReluStack(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 100),\n",
    "            ReluLayer(),\n",
    "            nn.Linear(100, 100),\n",
    "            ReluLayer(),\n",
    "            MyFCLayer(100, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.stack(x)\n",
    "\n",
    "\n",
    "linear_relu_stack = LinearReluStack()\n",
    "print(linear_relu_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "systematic-lewis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9470, -2.1320, -2.2663, -2.2337,  2.5418, -3.2080, -3.0094, -2.6686,\n",
       "         -1.1289, -0.5292]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(1, 28 * 28)\n",
    "linear_relu_stack(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "italian-redhead",
   "metadata": {},
   "source": [
    "## 自定义一个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "attended-grant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (stack): LinearReluStack(\n",
      "    (stack): Sequential(\n",
      "      (0): Linear(in_features=784, out_features=100, bias=True)\n",
      "      (1): ReluLayer()\n",
      "      (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (3): ReluLayer()\n",
      "      (4): MyFCLayer()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.stack = LinearReluStack()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.stack(self.flatten(x))\n",
    "\n",
    "\n",
    "model = NeuralNetwork()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "official-listing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.4962,  1.1806, -0.8129, -0.5095, -0.9446, -2.6407, -0.1641,  0.6045,\n",
       "         -1.3634, -1.4108]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(1, 28, 28)\n",
    "model(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "regulated-scout",
   "metadata": {},
   "source": [
    "# 参数\n",
    "\n",
    "每一层的参数，我们可以通过`layer.bias`和`layer.weight`来访问，得到的是一个`nn.parameter.Parameter`的类型对象。\n",
    "\n",
    "对于Sequential的模型，我们可以通过下标来访问每一层：`seqmodel[i]`\n",
    "\n",
    "我们也可以通过`state_dict`来获取nn.Module中的所有层的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "competent-pontiac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.OrderedDict"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = nn.Sequential(nn.Linear(25, 100), nn.ReLU(), nn.Linear(100, 10))\n",
    "# 通过索引获取 nn.Sequential 中的 Module\n",
    "first_layer = mlp[0]\n",
    "# 可以直接访问对应 module 的属性\n",
    "first_layer.bias\n",
    "first_layer.weight\n",
    "\n",
    "# state_dict 返回该 module 以及所有子 module 的状态数据（parameter & buffer)\n",
    "first_layer.state_dict()\n",
    "type(mlp.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ultimate-pulse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0.weight', torch.Size([100, 25])) ('0.bias', torch.Size([100])) ('2.weight', torch.Size([10, 100])) ('2.bias', torch.Size([10]))\n"
     ]
    }
   ],
   "source": [
    "# 获取所有参数\n",
    "print(*[(name, param.shape) for name, param in mlp.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "confirmed-screen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 访问OrderedDict\n",
    "mlp.state_dict()[\"2.weight\"].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "needed-pastor",
   "metadata": {},
   "source": [
    "对于`nn.parameter.Parameter`类型的对象，我们可以通过`.data`与`.grad`拿到其数据与梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "international-capacity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100]), None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_layer.bias.shape, first_layer.bias.grad  # (torch.Size([100]), None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "vital-great",
   "metadata": {},
   "source": [
    "# 参数初始化\n",
    "\n",
    "对整个网络应用某个初始化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "middle-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "mlp = mlp.apply(norm_init)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "unlike-operations",
   "metadata": {},
   "source": [
    "单独的某层layer应用初始化 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "stuck-multimedia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=100, out_features=10, bias=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def xiaver_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "\n",
    "mlp[2].apply(xiaver_init)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "configured-printing",
   "metadata": {},
   "source": [
    "# 多个layer共享参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "complete-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared = nn.Linear(8, 8)  # 需要共享参数的layer\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4, 8), nn.ReLU(), shared, nn.ReLU(), shared, nn.ReLU(), nn.Linear(8, 1)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "elementary-block",
   "metadata": {},
   "source": [
    "net[2]和net[4]是共享参数的，梯度累加。 共享的 module 在整个模型中获取 modules 或者 parameters 列表时，不会重复列出，内部会去重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a96db7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=4, out_features=8, bias=True)\n",
      "ReLU()\n",
      "Linear(in_features=8, out_features=8, bias=True)\n",
      "ReLU()\n",
      "ReLU()\n",
      "Linear(in_features=8, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for name, mod in net.named_children():\n",
    "    print (mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc39543",
   "metadata": {},
   "source": [
    "# nn.Module 常见接口"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d77c34c",
   "metadata": {},
   "source": [
    "## buffers 和 parameters\n",
    "\n",
    "<img src=\"../images/torch_module_api.png\" width=550px>\n",
    "\n",
    "buffers 和 parameters 是Module 中常见的两类数据成员，parameters 保存的是模型训练的参数，它会在反向传播中进行更新，另一类不需要计算梯度的数据是 buffers，一般主要用于前向计算中，比如 BatchNorm 中的 moving_mean 和 moving_var，以及在位置编码中的 cos 和 sin 频率表。\n",
    "\n",
    "buffers 和 parameters 本质上都是 `torch.Tensor`，只所以这里包装成了 buffers 和 parameters 两个概念，主要是方便`Module`对于这两种类型数据成员的管理，比如：可以通过接口一次性获取到所有子 Modules 的该类型的数据成员。\n",
    "\n",
    "我们可以通过 `nn.Modules` 的 `buffers`和`parameters`接口获取模块以及其子模块中保存的所有数据成员。或者通过 `named_buffers`以及`named_parameters`来同时获取对应数据成员的名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "890b2ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffers:\n",
      "\trunning_mean: torch.Size([128])\n",
      "\trunning_var: torch.Size([128])\n",
      "\tnum_batches_tracked: torch.Size([])\n",
      "Parameters:\n",
      "\tweight: torch.Size([128])\n",
      "\tbias: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "batch_norm = nn.BatchNorm2d(num_features=128)\n",
    "\n",
    "print(\"Buffers:\")\n",
    "for name, buffer in batch_norm.named_buffers():\n",
    "    print(f\"\\t{name}: {buffer.shape}\")\n",
    "print(\"Parameters:\")\n",
    "for name, param in batch_norm.named_parameters():\n",
    "    print(f\"\\t{name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819cc6c0",
   "metadata": {},
   "source": [
    "我们在实现像 `BatchNorm` 这样的类时，我们可以调用 `register_buffer` 来创建 buffer 类型的数据成员。\n",
    "\n",
    "```python\n",
    "class BatchNorm2D(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.mean = nn.Parameter(torch.randn(num_features))\n",
    "        self.var = nn.Parameter(torch.randn(num_features))\n",
    "        self.register_buffer(\"moving_mean\", torch.randn(num_features))\n",
    "        self.register_buffer(\"moving_var\", torch.randn(num_features))\n",
    "```\n",
    "\n",
    "经过 `register_buffer` 注册过的 buffer 成员，会自动的被 `named_buffers`这样的接口以及`state_dict`这样的接口捕获到。从上面的代码中，也可以看出对于`nn.Parameter`类型的数据，在构造函数中会被自动捕获到，如果某些参数是动态添加到 Module 当中的，则需要使用`register_parameter`。\n",
    "\n",
    "以下是 `nn.Parameter` 类型的成员自动注册到 `Module` 的`_parameters`详细流程：\n",
    "\n",
    "1. **创建 `nn.Parameter`**： 当你在 `nn.Module` 中定义一个参数时，如 `self.weight = nn.Parameter(torch.randn(10, 10))`，会创建一个 `Parameter` 对象。\n",
    "2. **调用 `Module.__setattr__`**： 当将这个 `Parameter` 赋值给模型（即 `self.weight = nn.Parameter(...)`），会触发 `Module` 的 `__setattr__` 方法。\n",
    "3. **注册到 `_parameters`**： 在 `__setattr__` 中，PyTorch 检查这个属性是否为 `Parameter` 类型。如果是，它会将这个参数的名称（例如 `weight`）和对应的 `Parameter` 对象一起注册到模型的 `_parameters` 字典中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7abeaff",
   "metadata": {},
   "source": [
    "## `apply` 调用\n",
    "\n",
    "Module 的 `apply(fn)` 函数用于在 Module 的所有子 Module应用一个自定义的函数。该函数常常用于我们来对模型的参数对应一些自定义的初始化。\n",
    "\n",
    "在`nn.Module`的源码里，还有一个`_apply`函数，它与`apply`的区别是，它的自定义函数是针对于 Module 中的数据成员的，也就是 parameters 和 buffers，比如当我们调用`model.cuda()`时，其内部实际上调用的就是`_apply`函数，将 Module 中的所有数据成员转换到 GPU 上。\n",
    "\n",
    "```python\n",
    "self._apply(lambda t: t.cuda(device))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5afe5701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "model = nn.Linear(100, 100)\n",
    "model = model.apply(norm_init)\n",
    "\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32253b52",
   "metadata": {},
   "source": [
    "# Containers\n",
    "\n",
    "`nn.ModuleList` 和 `nn.ModuleDict` 本身没有实现 `forward` 函数。这是因为它们只是用于存储一组子模块的容器，而不是一个完整的可执行模块。它们的主要作用是帮助你将多个子模块组织在一起，并自动注册到父模块中，但如何对这些子模块进行前向传播需要你在父类的 `forward` 函数中手动定义。\n",
    "\n",
    "## nn.Sequential\n",
    "\n",
    "`nn.Sequential` 是 PyTorch 中最常见的容器之一，通常用于将多个层（例如线性层、激活函数、卷积层等）按顺序连接。它的作用类似于一个列表，逐步将输入通过各个层传递。\n",
    "\n",
    "当网络结构相对简单时，可以利用 `nn.Sequential` 快速地构建网络。 在某些简单任务中，可以避免手动定义 forward 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20e1d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    nn.Linear(32 * 14 * 14, 10),  # 假设输入图像大小为28x28\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a97825",
   "metadata": {},
   "source": [
    "模仿nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ab1ec05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySequential(\n",
      "  (0): Linear(in_features=25, out_features=100, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self.add_module(str(idx), module)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # _modules是内部的一个OrderedDict\n",
    "        for module in self._modules.values():\n",
    "            x = module(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "mlp = MySequential(nn.Linear(25, 100), nn.ReLU(), nn.Linear(100, 10))\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab21769",
   "metadata": {},
   "source": [
    "## nn.ModuleList\n",
    "\n",
    "`nn.ModuleList` 用于存储一组子模块。与 Python 中的列表类似，但 ModuleList 中存储的模块会被 PyTorch 正确注册为子模块。这意味着它们的参数会被自动添加到父模块中，并能通过 `model.parameters()` 访问。它适用于那些模块数量或结构动态变化的情况。\n",
    "\n",
    "比如我们希望实验一个 50 层的全连接网络的梯度消失与梯度爆炸的问题，可以用 `ModuleList`来快速定义一个 50 层的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e752d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(100, 100) for _ in range(50)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6820946",
   "metadata": {},
   "source": [
    "## nn.ModuleDict\n",
    "\n",
    "`nn.ModuleDict` 与 Python 的字典类似，它将子模块以键值对的形式存储。与 `ModuleList` 不同的是，`ModuleDict` 的模块可以通过键名访问，适合需要灵活地通过名称调用特定模块的场景。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c469492",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleDict(\n",
    "            {\"conv\": nn.Conv2d(1, 32, 3, 1), \"fc\": nn.Linear(32 * 26 * 26, 10)}\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers[\"conv\"](x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.layers[\"fc\"](x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
