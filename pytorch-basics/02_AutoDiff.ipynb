{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "interesting-welding",
   "metadata": {},
   "source": [
    "# AutoGrad\n",
    "\n",
    "![](./images/tensor_autograd.png)\n",
    "\n",
    "当我们训练神经网络算法的时候，我们通常要用梯度的反向传播(back propagation)。在反向传播算法中，我们神经网络的参数会根据它们对于最终的损失函数的梯度来调整。\n",
    "\n",
    "$$\\text{output} = f(\\text{input}, W)$$\n",
    "$$\\text{Loss} = L(\\text{output}, \\text{groudtruth}) =  L(f(\\text{input}, W), \\text{groudtruth})$$\n",
    "$$W = W - \\eta * W_g$$\n",
    "\n",
    "为了计算公式中的损失函数$L$对于参数$W$的梯度$W_g$，Pytorch提供了一个很强大的自动求梯度的引擎：`torch.autograd`。它支持对于任何计算图的自动梯度计算。\n",
    "\n",
    "下面我们演示了，如果利用torch.autograd，来求于一个简单的函数的反向求梯度的过程，其中`w`是参数，最终求取的是loss对w的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "subsequent-transition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "devoted-genealogy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad of w is tensor([[0.3176, 0.2929, 0.3060],\n",
      "        [0.3176, 0.2929, 0.3060],\n",
      "        [0.3176, 0.2929, 0.3060],\n",
      "        [0.3176, 0.2929, 0.3060],\n",
      "        [0.3176, 0.2929, 0.3060]])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(f\"grad of w is {w.grad}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "sunset-consortium",
   "metadata": {},
   "source": [
    "在上面的例子中，因为我们想要求取`w`的梯度，所以在创建`w`的时候，设置了`requires_grad=True`，表明我们后续要求取`w`的梯度。\n",
    "\n",
    "我们可以在创建的时候指定`requires_grad=True`，也可以在创建后，通过`w.requires_grad_(True)`来修改。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "radio-mounting",
   "metadata": {},
   "source": [
    "# 计算图\n",
    "\n",
    "上面的计算过程，我们可以用下面的计算图来表示：\n",
    "\n",
    "![Compute Graph](./images/comp-graph.png)\n",
    "\n",
    "在这个示例网络计算中,w和b是parameters，是我们需要更新优化的部分。需要注意的是 Pytorh 的计算图是动态的，也就是一边创建一边会实时的计算。\n",
    "\n",
    "我们沿着计算图正方向计算，就可以计算出loss的值 ，而反向传播就是把梯度沿着计算图反方向计算。\n",
    "\n",
    "下面可以通过一个pytorch中一个标准的计算图计算函数，来大概看出来pytorch是怎么实现正向传播与反向传播的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "basic-corps",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearFunction(torch.autograd.Function):\n",
    "    # Note that both forward and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    # bias is an optional argument\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        output = input.mm(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "scientific-neutral",
   "metadata": {},
   "source": [
    "核心就是正向计算时，通过ctx把反向时，需要记录的Tenosr，都保存起来，反向时，再取出来用。\n",
    "\n",
    "另外 Pytorch 的计算图是保存在 Tensor 的 `grad_fn`中的，它是一个记录了 `BackwardOp`的结构，同时通过 `next_functions` 记录了backward 的整个调用链。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f49e4a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x7f19c5e813c0>\n",
      "((<SqueezeBackward4 object at 0x7f18f0003340>, 0), (<AccumulateGrad object at 0x7f18f0003520>, 0))\n"
     ]
    }
   ],
   "source": [
    "print(z.grad_fn)\n",
    "print(z.grad_fn.next_functions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "collective-supply",
   "metadata": {},
   "source": [
    "# 停止梯度的跟踪\n",
    "\n",
    "在有些时候，我们可能会对我们参数进行一些别的运算，这部分的运算并不是训练的一部分，不需要记录在整个计算图中，进行梯度跟踪。\n",
    "\n",
    "这时候我们可以用`torch.no_grad()` 块作用域来达到目的。比如在上面的简单的网络示例中，如果我们只是想对我们训练好的模型(w,b)，做一次testing。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "polished-company",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(z.requires_grad)\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "coordinated-poster",
   "metadata": {},
   "source": [
    "除此之外，我们还可以使用`detach()`方法，detatch返回的Tensor和原Tensor数据是相同的，但对返回Tensor的任何计算，都不会统计到计算图中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "enabling-bubble",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w.detach()) + b.detach()\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "selected-spine",
   "metadata": {},
   "source": [
    "但是如果我们对detach后返回的Tenosr进行了修改，那原Tensor也会被对应修改，这时候就破坏了原有的计算图，就会报错。\n",
    "\n",
    "```text\n",
    "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 5]] is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "alike-avatar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : tensor([1., 1., 1., 1., 1.])\n",
      "x : tensor([2., 1., 1., 1., 1.])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 5]] is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m x1[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mx : \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 5]] is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "print(f\"x : {x}\")\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "# 在形成计算图后，对原计算图中的一个，被用于grad_fn输入的Tensor: x，进行了修改\n",
    "x1 = x.detach()\n",
    "x1[0] = 2\n",
    "print(f\"x : {x}\")\n",
    "loss.backward()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8447996e",
   "metadata": {},
   "source": [
    "可以看出对于`detach()`的调用，pytorch底层应该是有维护有一个变量版本，对于`Tensor.data`属性，则没有这样的功能，所以现在已经不推荐使用`.data`属性了。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "spectacular-april",
   "metadata": {},
   "source": [
    "一旦backward被调用，这个计算图就被释放了，除非指定backward的参数`retain_graph`，这就可以让我们使用起来更加灵活。\n",
    "\n",
    "我们可以在每一次forward和backward后，改变网络的结构（DAG的样子），每次forward时会重新构建新的计算图。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b8c4d8",
   "metadata": {},
   "source": [
    "# 关于 Pytorch Autograd 原理的深度解析\n",
    "\n",
    "PyTorch Autograd Explained - In-depth Tutorial： https://www.youtube.com/watch?v=MswxJw-8PvE\n",
    "\n",
    "视频中详细介绍了：\n",
    "\n",
    "1. 计算图的创建过程\n",
    "2. grad_fn 的机制\n",
    "3. next_functions 中存的是什么\n",
    "4. 在形成计算图片，能不能对计算图中的 tensor 进行修改\n",
    "5. detach 的影响\n",
    "\n",
    "![](./images/pytorch_autograd.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('pyml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c37a17f5960fe44e0e933f0a97f50106b021053c3b378a6f2025e9a4390c58c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
