{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "64b3b5af",
            "metadata": {},
            "source": [
                "# Pytorch Tensor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "placed-poison",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "serial-compromise",
            "metadata": {},
            "source": [
                "# Tensor的属性\n",
                "\n",
                "<img src=\"../images/tensor_attributes.png\" width=550px>"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "1c14402d",
            "metadata": {},
            "source": [
                "## dtype\n",
                "\n",
                "torch在创建Tensor时，`dtype`的指定只支持使用`torch.[DataType]`这样的方式去指定，而不能像numpy一样，可以直接使用字符串。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "28fefdc0",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([1, 2, 3])"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.tensor([1, 2, 3], dtype=torch.int64)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "5d363752",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([1, 2, 3])"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "np.array([1, 2, 3], dtype=\"int64\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "6c20f704",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "torch.int8"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "a = torch.CharTensor([1, 2, 3])\n",
                "a.dtype"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5b55a947",
            "metadata": {},
            "source": [
                "其中`torch.dtype`表示Tensor的数据类型，常见的有下面9种不同的数据类型，包括了\n",
                "\n",
                "- `torch.float32`或`torch.float`，对应的Tensor类型为`torch.[cuda].FloatTensor`\n",
                "- `torch.float64`或`torch.double`，对应的Tensor类型为`torch.[cuda].DoubleTensor`\n",
                "- `torch.float16`或`torch.half`，对应的Tensor类型为`torch.[cuda].HalfTensor`\n",
                "- `torch.uint8`，对应的Tensor类型为`torch.[cuda].ByteTensor`\n",
                "- `torch.int8`，对应的Tensor类型为`torch.[cuda].CharTensor`\n",
                "- `torch.int16`或`torch.short`，对应的Tensor类型为`torch.[cuda].ShortTensor`\n",
                "- `torch.int32`或`torch.int`，对应的Tensor类型为`torch.[cuda].IntTensor`\n",
                "- `torch.int64`或`torch.long`，对应的Tensor类型为`torch.[cuda].LongTensor`\n",
                "- `torch.bool`，对应的Tensor类型为`torch.[cuda].BoolTensor`\n",
                "\n",
                "还有几种数据类型，用的比较少：\n",
                "\n",
                "- `torch.bfloat16`\n",
                "- `torch.complex32`: complex32 目前只是实验性的支持，后续可能会取消支持\n",
                "- `torch.complex64`\n",
                "- `torch.complex128`或`torch.cdouble`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "888128cf",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([[ 0.7482-0.1826j, -0.7404+0.7532j, -0.2283-0.3117j],\n",
                        "        [ 0.4588+1.8799j, -1.1133+0.1339j, -0.1241+0.0917j],\n",
                        "        [ 0.9690+0.4072j,  1.0455-0.2295j,  1.0468-1.6390j]])\n",
                        "torch.complex64\n",
                        "torch.complex128\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "\n",
                "# 创建一个 Complex64 张量\n",
                "real_part = torch.randn(3, 3)  # 实部\n",
                "imag_part = torch.randn(3, 3)  # 虚部\n",
                "complex_tensor = torch.complex(real_part, imag_part)\n",
                "\n",
                "print(complex_tensor)\n",
                "print(complex_tensor.dtype)  # 输出: torch.complex64\n",
                "complex_tensor = complex_tensor.to(dtype=torch.cdouble)\n",
                "print(complex_tensor.dtype)  # 输出: torch.complex128"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "indian-vehicle",
            "metadata": {},
            "source": [
                "[`bfloat16`](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format)是一种和IEEE half-precision 16-bit float规定不一致的16Bit浮点数格式，它是直接对32位的IEEE 754规定的单精度float32的格式进行截取形成的，它是为机器学习系统特别定制的，它的组成是：\n",
                "- 1位符号位\n",
                "- 8个指数位\n",
                "- 7个小数位\n",
                "\n",
                "神经网络对指数的大小比对尾数的大小更敏感。为了确保下溢、上溢和`NaN`的行为完全相同，`bfloat16`的指数大小与`float32`相同。`bfloat16`处理非规格化数的规则与`float32`不同，它会将它们截断为零。与通常需要特殊处理，如损失缩放的`float16`不同，`bfloat16`是训练和运行深度神经网络时`float32`的即插即用替换。\n",
                "\n",
                "简而言之，`bfloat16`表示的数值范围更大，但是精度不如`float16`\n",
                "\n",
                "<center style=\"width: 100%\"><img src=\"../images/floating_point_formats.png\" width=\"800px\"></center>\n",
                "\n",
                "\n",
                "在不同的机器上，因为CPU架构等不同，Tensor的很多构建函数，对上面的部分`dtype`有可能是不支持的，比如`arange`函数就不支持在`cpu`上创建一个`float16`的Tensor。（新版本已经支持了）"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "joint-national",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=torch.float16)"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.arange(1, 10, dtype=torch.float16, device=torch.device(\"cpu\"))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "limiting-standing",
            "metadata": {},
            "source": [
                "由于CUDA对半精度支持的比较好，所以在'cuda'上创建，反而没有什么问题"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "ready-needle",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.], device='cuda:0',\n",
                            "       dtype=torch.float16)"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.arange(1, 10, dtype=torch.float16, device=torch.device(\"cuda\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ae6105f4",
            "metadata": {},
            "source": [
                "另外，在 Torch 2.x 的版本中也开始实验性的支持`FP8`格式，目前支持两种类型的`FP8`格式，一种是`E3M4`一种是`E5M2`，其中`E5M2`和 IEEE 的 `fp16`的指数部分长度一致，这使得 E5M2 和 IEEE FP16 格式之间可以直接转换。`E5M2`它表达的数值范围更大，可以处理一些特殊值。\n",
                "\n",
                "一般来说，我们会在模型推理以及训练过程的前向阶段为了保持较高的精度会使用`E3M4`的格式，而在训练的反向阶段，使用`E5M2`来有较好的数值动态范围,避免梯度消失与爆炸。\n",
                "\n",
                "Paper: [FP8 Formats for Deep Learning](https://arxiv.org/abs/2209.05433)\n",
                "\n",
                "<img src=\"../images/torch_fp8.png\" width=\"600px\">\n",
                "\n",
                "上图展示了使用不同数值类型来表示“0.3952” 时，实际能够近似到的值。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "57d45c79",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "data type        bits              max               min    smallest normal          eps\n",
                        "-------------  ------  ---------------  ----------------  -----------------  -----------\n",
                        "float32            32      3.40282e+38      -3.40282e+38        1.17549e-38  1.19209e-07\n",
                        "bfloat16           16      3.38953e+38      -3.38953e+38        1.17549e-38  0.0078125\n",
                        "float8_e4m3fn       8    448              -448                  0.015625     0.125\n",
                        "float8_e5m2         8  57344            -57344                  6.10352e-05  0.25\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "from tabulate import tabulate\n",
                "\n",
                "f32_type = torch.float32\n",
                "bf16_type = torch.bfloat16\n",
                "e4m3_type = torch.float8_e4m3fn\n",
                "e5m2_type = torch.float8_e5m2\n",
                "\n",
                "# collect finfo for each type\n",
                "table = []\n",
                "for dtype in [f32_type, bf16_type, e4m3_type, e5m2_type]:\n",
                "    numbits = 32 if dtype == f32_type else 16 if dtype == bf16_type else 8\n",
                "    info = torch.finfo(dtype)\n",
                "    table.append(\n",
                "        [info.dtype, numbits, info.max, info.min, info.smallest_normal, info.eps]\n",
                "    )\n",
                "\n",
                "headers = [\"data type\", \"bits\", \"max\", \"min\", \"smallest normal\", \"eps\"]\n",
                "print(tabulate(table, headers=headers))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "working-commerce",
            "metadata": {},
            "source": [
                "## device\n",
                "\n",
                "`torch.device`表示的是Tensor的数据存储的设备，其中分为`cpu`和`cuda`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "advisory-diagnosis",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "device(type='cpu')"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.device(\"cpu\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "funny-acoustic",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "device(type='cuda', index=0)"
                        ]
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.device(\"cuda:0\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "aggressive-inspection",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "device(type='cuda', index=0)"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.device(type=\"cuda\", index=0)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "cosmetic-baptist",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([1, 2, 3, 4, 5, 6], device='cuda:0')"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.tensor([1, 2, 3, 4, 5, 6], device=torch.device(\"cuda:0\"))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "sought-algorithm",
            "metadata": {},
            "source": [
                "## layout\n",
                "\n",
                "layout表示Tensor内部数据存储的内部布局，目前还是一个不成熟(beta)的特性，目前支持\n",
                "\n",
                "- torch.strided\n",
                "- torch.sparse_coo\n",
                "\n",
                "现在主要用的就是面向dense Tensor的`torch.strided`，Tensor的Strides是一个list，它代表每个dimension上两邻两个idx之间的跨度(元素个数)。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "abandoned-spanking",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(20, 5, 1)"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.arange(60).reshape(3, 4, 5).stride()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "643e5b03",
            "metadata": {},
            "source": [
                "我们可以理解为 Tensor 底层的存储的是一个一维的数组，我们对于 `Tensor`的索引，全部是是通过一个下标对应的 stride 来计算出最终在一维数组上的偏移量。 这样实现的好处时，对于 `Tensor`的很多操作，它并不需要实际对 `Tensor`的内存数据进行变动。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "6cf2ce6d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(4, 1)\n",
                        "(1, 4)\n"
                    ]
                }
            ],
            "source": [
                "t = torch.arange(12).view(3, 4)\n",
                "# t_transposed 和 t 共享底层的数据\n",
                "t_transposed = t.transpose(0, 1)\n",
                "print(t.stride())\n",
                "print(t_transposed.stride())"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "127fa16a",
            "metadata": {},
            "source": [
                "和numpy不同的是，torch中的stride以元素的个数来表示跨度，而numpy则是用字节数量来表示跨度"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "467f02b8",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(160, 40, 8)"
                        ]
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "np.arange(60).reshape(3, 4, 5).strides"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "painful-authorization",
            "metadata": {},
            "source": [
                "## Tensor属性转换\n",
                "\n",
                "我们可以使用`to`方法来指定新的属性后，生成新的Tensor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "minute-cassette",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "torch.int64 cpu\n",
                        "torch.float32 cuda:0\n"
                    ]
                }
            ],
            "source": [
                "device_cuda = torch.device(\"cuda\")\n",
                "data = torch.tensor([1])\n",
                "print(data.dtype, data.device)\n",
                "data = data.to(dtype=torch.float32, device=device_cuda)\n",
                "print(data.dtype, data.device)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0ccddd5b",
            "metadata": {},
            "source": [
                "也可以通过 Tensor 的`dtype`方法来直接将返回新数据类型的 Tensor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "aba23467",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([True], device='cuda:0')"
                        ]
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "data.int()\n",
                "data.float()\n",
                "data.bool()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "entertaining-aquatic",
            "metadata": {},
            "source": [
                "## Tensor的形状\n",
                "\n",
                "Tensor除了具有3个标准的属性外，一旦我们创建了一个Tensor，那么它就会具有一些形状相关的属性。\n",
                "\n",
                "- t.shape: 返回的是一个torch.Size(tuple)类型的结果，表示每一维的维度值\n",
                "- t.size(): 和t.shape一致\n",
                "- t.size(i): 返回第 i 个维度的值\n",
                "- t.ndim：返回Tensor有多少维\n",
                "- t.numel()：它是一个方法，返回Tensor内有多少个元素\n",
                "- len(t)：返回的是Tensor在第0维上的维度值"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "headed-dylan",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "shape of t is torch.Size([2, 3, 4])\n",
                        "size of t is torch.Size([2, 3, 4])\n",
                        "size(1) of t is 3\n",
                        "strides of t is (12, 4, 1)\n",
                        "strides of axes1 of t is 4\n",
                        "ndim of t is 3\n",
                        "numel of t is 24\n",
                        "len of t is 2\n"
                    ]
                }
            ],
            "source": [
                "t = torch.empty(2, 3, 4)\n",
                "print(f\"shape of t is {t.shape}\")\n",
                "print(f\"size of t is {t.size()}\")\n",
                "print(f\"size(1) of t is {t.size(1)}\")\n",
                "print(f\"strides of t is {t.stride()}\")\n",
                "print(f\"strides of axes{1} of t is {t.stride(1)}\")\n",
                "print(f\"ndim of t is {t.ndim}\")\n",
                "print(f\"numel of t is {t.numel()}\")\n",
                "print(f\"len of t is {len(t)}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "moderate-country",
            "metadata": {},
            "source": [
                "# Tensor的创建"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "neural-outline",
            "metadata": {},
            "source": [
                "在Pytorch中我们可以有多种方法来创建Tensor，常用的包括下面几种：\n",
                "\n",
                "- 从已有的scalar、list、tuple、numpy.array来创建\n",
                "- 用`arange`、`linspace`、`logspace`等创建一维数列Tensor\n",
                "- 用`ones`、`zeros`、`eye`、`full`、`empty`等来创建特别填充值的多维Tensor\n",
                "- 用随机数来创建指定形状的Tensor\n",
                "\n",
                "![](../images/tensor_creation.png)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "personal-energy",
            "metadata": {},
            "source": [
                "## 从现有数据来创建一个Tensor\n",
                "\n",
                "我们可以使用`torch.tensor()`函数来从已有的一个array_like的data来创建一个Tensor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "awful-thesis",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([1, 2, 3, 4, 5])"
                        ]
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# 从list创建\n",
                "torch.tensor([1, 2, 3, 4, 5])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "random-advocacy",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([1, 2, 3])"
                        ]
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# 从tuple创建\n",
                "torch.tensor((1, 2, 3))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "conventional-visibility",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([1., 2., 3., 4., 5.], device='cuda:0')"
                        ]
                    },
                    "execution_count": 21,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# 从numpy.array创建，同时指定dtype和device\n",
                "torch.tensor(np.array([1, 2, 3, 4, 5]), dtype=torch.float32, device=\"cuda:0\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "stable-peeing",
            "metadata": {},
            "source": [
                "需要注意的是，无论是从python的内置序列创建，还是从numpy.array来创建，创建出来的Tensor都是复制了原数据的内容。\n",
                "\n",
                "如果我们希望，创建的Tensor不额外分配存储空间，而是和之前的numpy.array共享存储，那么可以使用`as_tensor`方法"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "id": "secret-geneva",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[6 2 3 4 5]\n"
                    ]
                }
            ],
            "source": [
                "arr = np.array([1, 2, 3, 4, 5])\n",
                "t = torch.as_tensor(arr)\n",
                "# 对于Tensor的数据改动，也会影响在ndarray上\n",
                "t[0] = 6\n",
                "print(arr)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "established-mystery",
            "metadata": {},
            "source": [
                "不过使用`as_tensor`后，能共享底层存储的，前提是，as_type方法中指定的`dtype`和`device`和原ndarry是一致的。由于numpy不支持cuda，所以这样只能创建cpu上的tensor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "id": "fresh-extreme",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[1 2 3 4 5]\n"
                    ]
                }
            ],
            "source": [
                "arr = np.array([1, 2, 3, 4, 5])\n",
                "t = torch.as_tensor(arr, dtype=torch.float32)  # 这种情况下，并不会共享底层存储\n",
                "t[0] = 6\n",
                "print(arr)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "id": "circular-district",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ndarray的默认整数类型为:int64\n",
                        "tensor的默认整数类型为: torch.int64\n",
                        "ndarray的默认整数类型为:float64\n",
                        "tensor的默认整数类型为: torch.float32\n"
                    ]
                }
            ],
            "source": [
                "il = [1, 2, 3, 4, 5]\n",
                "print(f\"ndarray的默认整数类型为:{np.array(il).dtype}\")\n",
                "print(f\"tensor的默认整数类型为: {torch.tensor(il).dtype}\")\n",
                "\n",
                "fl = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
                "print(f\"ndarray的默认整数类型为:{np.array(fl).dtype}\")\n",
                "print(f\"tensor的默认整数类型为: {torch.tensor(fl).dtype}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "id": "fece622f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/tmp/ipykernel_219404/2406181639.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
                        "  b = torch.tensor(a, dtype=torch.float, device=\"cuda:1\")\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "tensor([1., 2., 3.], device='cuda:1')"
                        ]
                    },
                    "execution_count": 25,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# 从另外一个tensor来创建tensor，无论 b 是否指新新的 dtype 和 device，b 都不和 a 共享数据\n",
                "a = torch.tensor([1, 2, 3])\n",
                "b = torch.tensor(a, dtype=torch.float, device=\"cuda:1\")\n",
                "b"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "id": "8b1990fc",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([1, 2, 3], device='cuda:1')\n"
                    ]
                }
            ],
            "source": [
                "b = a.clone()\n",
                "b = b.to(device=\"cuda:1\")\n",
                "print(b)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "unauthorized-today",
            "metadata": {},
            "source": [
                "## `torch.tensor()`和`torch.Tensor()`的区别\n",
                "\n",
                "`torch.Tensor`实际上是`torch.FloatTensor`，用它来创建新的Tensor时，实际调用的是构造函数，它会默认以`torch.float32`来作为`dtype`。而`torch.tensor`会根据`data`的类型自动推断。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "id": "right-andrews",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "torch.float32\n",
                        "torch.int64\n"
                    ]
                }
            ],
            "source": [
                "l = [1, 2, 3, 4, 5]\n",
                "print(torch.Tensor(l).dtype)\n",
                "print(torch.tensor(l).dtype)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "stainless-garden",
            "metadata": {},
            "source": [
                "## 创建特别填充值的Tensor"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "dramatic-driver",
            "metadata": {},
            "source": [
                "### torch.arange\n",
                "\n",
                "torch.arange(start=0, end, step=1)用于创建一个区间范围的Tensor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "id": "perfect-alpha",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([0, 1, 2, 3, 4])\n",
                        "tensor([1, 2, 3, 4])\n",
                        "tensor([ 1,  4,  7, 10, 13, 16, 19])\n"
                    ]
                }
            ],
            "source": [
                "print(torch.arange(5))\n",
                "print(torch.arange(1, 5))\n",
                "print(torch.arange(1, 20, 3))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "id": "crucial-mission",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([1.0000, 1.5000, 2.0000, 2.5000, 3.0000])"
                        ]
                    },
                    "execution_count": 29,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# 如果start、end以及step中有浮点数，则创建出来的是FloatTensor\n",
                "torch.arange(1, 3.5, 0.5)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "indie-excellence",
            "metadata": {},
            "source": [
                "注意上面是没有包括3.5那个点的"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "robust-general",
            "metadata": {},
            "source": [
                "### torch.linspace\n",
                "\n",
                "`torch.linspace`与`torch.arange`有点类似，都指定一个起点，一个终点，和一个步长。但`linspace`里步长最终指定了生成的一维Tensor中元素的个数\n",
                "\n",
                "```python\n",
                "linspace(start(float),end(float),steps(int))\n",
                "```\n",
                "另外需要注意的是`torch.linspace`生成的一定是一个浮点数的Tensor，而且和`torch.arange`不同的是：`linspace`生成的Tensor是包括末点值的（inclusive）"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "id": "cognitive-origin",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([ 3.0000,  4.7500,  6.5000,  8.2500, 10.0000])"
                        ]
                    },
                    "execution_count": 30,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.linspace(3, 10, 5)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "enhanced-breach",
            "metadata": {},
            "source": [
                "### torch.logspace\n",
                "\n",
                "`torch.logspace`和`torch.linspace`行为类似，区别在于`logspace`生成的序列的范围的起始与终点是一个以`base`为底，`start`和`end`为指数的数字。\n",
                "\n",
                "```python\n",
                "logspace(start, end, stpes, base=10.0) -> Tensor\n",
                "```"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "innovative-graduation",
            "metadata": {},
            "source": [
                "### torch.ones、torch.zeros、torch.emtpy\n",
                "\n",
                "它们三个都是用于创建一个指定`size`的Tensor，分别以1、0和未初始化的值来填充\n",
                "\n",
                "它们三个返回的都是`FloatTensor`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "id": "invisible-lesbian",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([[1., 1.],\n",
                        "        [1., 1.]])\n",
                        "tensor([[0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0.]])\n",
                        "tensor([[6.0555e-32, 0.0000e+00, 6.0717e-32],\n",
                        "        [0.0000e+00, 4.0357e-43, 0.0000e+00],\n",
                        "        [1.5835e-43, 0.0000e+00, 1.0483e-13]])\n"
                    ]
                }
            ],
            "source": [
                "print(torch.ones((2, 2)))\n",
                "print(torch.zeros((3, 4)))\n",
                "print(torch.empty((3, 3)))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "54e6d4db",
            "metadata": {},
            "source": [
                "`torch.ones/zeros/empty`支持`torch.ones(d1,d2,...)`这种调用方法，而`numpy`则不支持。"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "statewide-brooklyn",
            "metadata": {},
            "source": [
                "### torch.eye\n",
                "\n",
                "`torch.eye`返回的是一个2d的对角线为1，其他值都为0的Float矩阵Tensor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "id": "fatal-happiness",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[1., 0., 0., 0.],\n",
                            "        [0., 1., 0., 0.],\n",
                            "        [0., 0., 1., 0.],\n",
                            "        [0., 0., 0., 1.]])"
                        ]
                    },
                    "execution_count": 32,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.eye(4)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "interim-following",
            "metadata": {},
            "source": [
                "### torch.full\n",
                "\n",
                "`torch.full`返回的是一个指定`size`和填充值的Tensor，Tensor的dtype是由填充值的类型来推导的。\n",
                "\n",
                "```python\n",
                "'''\n",
                "Args:\n",
                "  size(int...): a list ,tuple or torch.Size\n",
                "  fill_vale(Scalar)\n",
                "'''\n",
                "full(size, fill_value)\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "id": "technological-highway",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[1., 1., 1.],\n",
                            "        [1., 1., 1.]])"
                        ]
                    },
                    "execution_count": 33,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.full((2, 3), 1.0)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "db37d49a",
            "metadata": {},
            "source": [
                "### torch.diag"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "218209a5",
            "metadata": {},
            "source": [
                "如果输入的是一个 1d 的 Tensor，则返回的是一个 2d 的对角矩阵，其对角线上的元素为传入的 Tensor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "id": "2cb6d99e",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[1, 0, 0],\n",
                            "        [0, 2, 0],\n",
                            "        [0, 0, 3]])"
                        ]
                    },
                    "execution_count": 34,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.diag(torch.tensor([1, 2, 3]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "id": "c3b9c9f4",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[0, 0, 0, 0],\n",
                            "        [1, 0, 0, 0],\n",
                            "        [0, 2, 0, 0],\n",
                            "        [0, 0, 3, 0]])"
                        ]
                    },
                    "execution_count": 35,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.diag(torch.tensor([1, 2, 3]), diagonal=-1)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bb37e666",
            "metadata": {},
            "source": [
                "传入 2d 的 Tensor，则返回 Tensor 的对角线上的元素，返回的是一个 1d 的 Tensor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "id": "77507e04",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([1, 5, 9])"
                        ]
                    },
                    "execution_count": 36,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.diag(torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "id": "4fd32bc2",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([2, 6])"
                        ]
                    },
                    "execution_count": 37,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.diag(torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), diagonal=1)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "chinese-ranch",
            "metadata": {},
            "source": [
                "## 使用随机数来创建Tensor"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "resident-physiology",
            "metadata": {},
            "source": [
                "### torch.normal\n",
                "\n",
                "`torch.normal`返回一个正态分布产生在的随机数填充的Tensor，它一共有4种参数传递方式\n",
                "\n",
                "第一种是:\n",
                "\n",
                "```python\n",
                "'''\n",
                "Args:\n",
                "    mean (Tensor): the tensor of per-element means\n",
                "    std (Tensor): the tensor of per-element standard deviations\n",
                "'''\n",
                "norm(mean, std)\n",
                "```\n",
                "生成的Tensor的size和mean和std的size是一致的，其中每个元素都是通过对应位置的mean和std形成的正态分布来随机产生的。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "id": "super-louisville",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[ 1.4107, -1.0148, -0.7799,  1.8444],\n",
                            "        [-0.3895,  1.2914, -1.8545, -0.2155],\n",
                            "        [-0.9079, -0.1627,  0.4883,  0.2391]])"
                        ]
                    },
                    "execution_count": 38,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "mean = torch.randn(3, 4)\n",
                "std = torch.rand((3, 4))\n",
                "torch.normal(mean, std)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "tutorial-contractor",
            "metadata": {},
            "source": [
                "第二种是：\n",
                "\n",
                "```python\n",
                "'''\n",
                "Args:\n",
                "    mean (float, optional): the mean for all distributions\n",
                "    std (Tensor): the tensor of per-element standard deviations\n",
                "'''\n",
                "normal(mean=0.0, std, *, out=None) -> Tensor\n",
                "```\n",
                "这种参数传递用法，与上面的区别就是mean变成一个Scalar，那么说明每个元素来共享一个mean值。\n",
                "\n",
                "在这种情况下，生成的Tensor的shape就行std保持一致的了。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "id": "olive-third",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[0.6667, 1.3038, 0.4871, 0.9182],\n",
                            "        [1.1713, 0.9795, 1.2223, 1.6504],\n",
                            "        [2.0215, 1.3112, 1.0539, 1.0291]])"
                        ]
                    },
                    "execution_count": 39,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.normal(1.0, std)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "excellent-sociology",
            "metadata": {},
            "source": [
                "第三种：\n",
                "\n",
                "```python\n",
                "'''\n",
                "Args:\n",
                "    mean (Tensor): the tensor of per-element means\n",
                "    std (float, optional): the standard deviation for all distributions\n",
                "'''\n",
                "normal(mean, std=1.0, *, out=None) -> Tensor\n",
                "```\n",
                "这种情况和第二种情况，恰恰相反了，std变成了每个元素共享的。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "id": "ambient-shame",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[ 1.1298,  0.1154, -1.1287,  1.5257],\n",
                            "        [-0.2515,  1.6378, -1.0825,  0.2520],\n",
                            "        [ 0.2799,  0.5922, -0.3907, -1.0569]])"
                        ]
                    },
                    "execution_count": 40,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.normal(mean, 0.5)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "acoustic-function",
            "metadata": {},
            "source": [
                "第四种：\n",
                "\n",
                "```python\n",
                "'''\n",
                "Args:\n",
                "    mean (float): the mean for all distributions\n",
                "    std (float): the standard deviation for all distributions\n",
                "    size (int...): a sequence of integers defining the shape of the output tensor.\n",
                "'''\n",
                "normal(mean, std, size, *, out=None) -> Tensor\n",
                "```\n",
                "这种情况下，所有的元素都共享mean和std，最终Tensor的形状是由`size`来决定的"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "id": "statewide-defeat",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[-0.3697, -1.0011,  0.2011, -0.3058],\n",
                            "        [-0.0142,  1.7952,  0.3282,  0.6413],\n",
                            "        [-1.2560, -0.4896,  0.7281, -0.1820]])"
                        ]
                    },
                    "execution_count": 41,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.normal(0, 1, (3, 4))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "expensive-ground",
            "metadata": {},
            "source": [
                "### torch.rand、torch.randn\n",
                "\n",
                "`rand`直接生成指定形状的Tensor，其中每个元素都是由`[0,1)`均匀分布来随机产生。\n",
                "\n",
                "`randn`直接生成指定形状的Tensor，其中每个元素都是由标准正态分布来随机产生。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "id": "difficult-exclusive",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[0.3890, 0.3621, 0.4936, 0.9490],\n",
                            "        [0.4722, 0.2002, 0.4395, 0.1839],\n",
                            "        [0.3801, 0.3354, 0.4724, 0.1334]])"
                        ]
                    },
                    "execution_count": 42,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.rand(3, 4)  # 或者 torch.randn((3,4))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "id": "resident-austin",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[ 0.6906, -0.3491,  0.7373, -0.7658],\n",
                            "        [-0.2358, -1.9815, -0.3103, -0.4552],\n",
                            "        [-0.4182,  0.3479, -2.6860,  0.4807]])"
                        ]
                    },
                    "execution_count": 43,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.randn(3, 4)  # 或者 torch.randn((3,4))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "regional-afternoon",
            "metadata": {},
            "source": [
                "### torch.randint\n",
                "\n",
                "产生一个由`[low,high)`区间均匀分布随机数填充的LongTensor\n",
                "\n",
                "```python\n",
                "randint(low=0,high,size,...)\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "id": "large-vaccine",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[6, 9, 6, 2],\n",
                            "        [6, 8, 6, 6],\n",
                            "        [7, 9, 6, 4]])"
                        ]
                    },
                    "execution_count": 44,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.randint(1, 10, (3, 4))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "danish-prime",
            "metadata": {},
            "source": [
                "### torch.randperm\n",
                "\n",
                "生成一个随机全排列的一维的LongTensor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "id": "strange-biography",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([10,  2,  7,  1, 11,  9,  4,  5,  0,  6,  3,  8])"
                        ]
                    },
                    "execution_count": 45,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.randperm(12)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "directed-simon",
            "metadata": {},
            "source": [
                "## 使用`xx_like`系列创建相同形态的Tensor\n",
                "\n",
                "除了shape保持一致外，`dtype`、`layout`、`device`等，若无特别指定，则也与源Tensor保持一致。\n",
                "\n",
                "```python\n",
                "torch.zeros_like(input, ..) # 返回与input相同size的零矩阵\n",
                "\n",
                "torch.ones_like(input, ..) #返回与input相同size的单位矩阵\n",
                "\n",
                "torch.full_like(input, fill_value, …) #返回与input相同size，单位值为fill_value的矩阵\n",
                "\n",
                "torch.empty_like(input, …) # 返回与input相同size,并被未初始化的数值填充的tensor\n",
                "\n",
                "torch.rand_like(input, dtype=None, …) #返回与input相同size的tensor, 填充均匀分布的随机数值\n",
                "\n",
                "torch.randint_like(input, low=0, high, dtype=None, …) #返回与input相同size的tensor, 填充[low, high)均匀分布的随机数值\n",
                "\n",
                "torch.randn_like(input, dtype=None, …) #返回与input相同size的tensor, 填充标准正态分布的随机数值\n",
                "\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "id": "spatial-montgomery",
            "metadata": {},
            "outputs": [],
            "source": [
                "src = torch.randn(4, 5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "id": "forty-membrane",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[0., 0., 0., 0., 0.],\n",
                            "        [0., 0., 0., 0., 0.],\n",
                            "        [0., 0., 0., 0., 0.],\n",
                            "        [0., 0., 0., 0., 0.]])"
                        ]
                    },
                    "execution_count": 47,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.zeros_like(src)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "id": "overall-dragon",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[1, 1, 1, 1, 1],\n",
                            "        [1, 1, 1, 1, 1],\n",
                            "        [1, 1, 1, 1, 1],\n",
                            "        [1, 1, 1, 1, 1]], dtype=torch.int32)"
                        ]
                    },
                    "execution_count": 48,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.ones_like(src, dtype=torch.int)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "id": "backed-budapest",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00, 5.0000e+00],\n",
                            "        [0.0000e+00, 4.2039e-45, 0.0000e+00, 5.6052e-45, 0.0000e+00],\n",
                            "        [7.0065e-45, 0.0000e+00, 8.4078e-45, 0.0000e+00, 9.8091e-45],\n",
                            "        [0.0000e+00, 1.1210e-44, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
                            "       device='cuda:0')"
                        ]
                    },
                    "execution_count": 49,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.empty_like(src, device=\"cuda:0\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "id": "pointed-williams",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[42., 42., 42., 42., 42.],\n",
                            "        [42., 42., 42., 42., 42.],\n",
                            "        [42., 42., 42., 42., 42.],\n",
                            "        [42., 42., 42., 42., 42.]])"
                        ]
                    },
                    "execution_count": 50,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# 这里即使full_value是int类型，但生成的Tensor，依然是用的src的dtype\n",
                "torch.full_like(src, 42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "id": "noticed-spanking",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[0.5314, 0.8197, 0.8071, 0.1591, 0.5537],\n",
                            "        [0.0079, 0.6328, 0.3513, 0.0229, 0.1560],\n",
                            "        [0.3215, 0.3350, 0.4526, 0.0278, 0.4012],\n",
                            "        [0.5541, 0.3188, 0.4859, 0.3072, 0.9894]])"
                        ]
                    },
                    "execution_count": 51,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.rand_like(src)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "id": "norwegian-notice",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[ 1.1692, -0.8948, -0.4549,  0.9164,  2.5042],\n",
                            "        [-0.5313,  0.0461,  0.1912, -0.9092, -0.2638],\n",
                            "        [-0.2223,  1.7269,  1.8062, -0.3885,  0.8033],\n",
                            "        [-1.1798, -0.3489,  0.1121,  0.5281,  0.6492]])"
                        ]
                    },
                    "execution_count": 52,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.randn_like(src)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "id": "committed-soundtrack",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[9., 9., 1., 1., 5.],\n",
                            "        [9., 5., 8., 7., 3.],\n",
                            "        [2., 1., 1., 5., 6.],\n",
                            "        [2., 3., 2., 2., 9.]])"
                        ]
                    },
                    "execution_count": 53,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.randint_like(src, 1, 10)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "amateur-dream",
            "metadata": {},
            "source": [
                "# Tensor的操作\n",
                "\n",
                "Pytorch中的Tensor大约支持100种以上的操作，其中包括了数学运算、线性代数、矩阵操作（转置、索引、切片等），这些操作都可以跑在CPU或GPU上，这也是Pytorch Tensor的强大之处。\n",
                "\n",
                "![](./images/tensor_operatrions.png)\n",
                "\n",
                "我们可以通过这个[页面](https://pytorch.org/docs/stable/torch.html)，来对Tensor支持的所有操作做个大概的了解。"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "derived-holocaust",
            "metadata": {},
            "source": [
                "## 索引访值\n",
                "\n",
                "### 基础索引\n",
                "\n",
                "我们可以像访问Numpy.ndarray一样，对torch.Tensor进行各种下标索引与范围切片。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "id": "canadian-insulin",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "t: tensor([[ 0,  1,  2,  3],\n",
                        "        [ 4,  5,  6,  7],\n",
                        "        [ 8,  9, 10, 11]])\n",
                        "取t的第2行的所有元素: tensor([4, 5, 6, 7])\n",
                        "取t的最后一列的所有元素: tensor([ 3,  7, 11])\n",
                        "取t的第2列到最后一列的所有元素: tensor([[ 2,  3],\n",
                        "        [ 6,  7],\n",
                        "        [10, 11]])\n",
                        "取t的位置(2,3)上的元素: 11\n"
                    ]
                }
            ],
            "source": [
                "t = torch.arange(12).reshape(3, 4)\n",
                "print(f\"t: {t}\")\n",
                "print(f\"取t的第2行的所有元素: {t[1]}\")\n",
                "print(f\"取t的最后一列的所有元素: {t[:, -1]}\")\n",
                "print(f\"取t的第2列到最后一列的所有元素: {t[:, 2:]}\")\n",
                "print(f\"取t的位置(2,3)上的元素: {t[2, 3]}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "necessary-adoption",
            "metadata": {},
            "source": [
                "**单一元素的Tensor**\n",
                "\n",
                "当我们通过索引访问Tensor的单一元素时，得到的实际是一个`Tensor`类型的对象，它并不是python中的内置数据类型，我们可以通过Tensor的`item()`方法来获取python对象的标量。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "id": "sound-pattern",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "torch.Tensor"
                        ]
                    },
                    "execution_count": 55,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "type(t[2, 3])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "id": "95e58023",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "torch.Size([])"
                        ]
                    },
                    "execution_count": 56,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "t[2, 3].shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "id": "chief-thermal",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "int"
                        ]
                    },
                    "execution_count": 57,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "type(t[2, 3].item())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "09a3d026",
            "metadata": {},
            "source": [
                "注意对如果某个维度上我们只取一行/列数据，那么有两种方式，这两种方式得到的结果的 Shape 会不一样"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "id": "f9e4eddc",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([[ 2],\n",
                        "        [ 6],\n",
                        "        [10]])\n",
                        "tensor([ 2,  6, 10])\n"
                    ]
                }
            ],
            "source": [
                "t1 = t[:, 2:3]\n",
                "t2 = t[:, 2]\n",
                "print(t1)  # 还是一个二维的Tensor\n",
                "print(t2)  # 一维的 Tensor"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "503e5459",
            "metadata": {},
            "source": [
                "### 高级索引\n",
                "\n",
                "Tensor 的高级索引，支持我们直接用一个 Long 型的 Tensor 作为索引来取原 Tensor 中的元素。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "id": "0e2d8bc2",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "torch.Size([3, 2, 10])"
                        ]
                    },
                    "execution_count": 59,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "t = torch.randn(8, 10)\n",
                "# indices 的所有元素都代表 t 的 dim=0 的下标\n",
                "indices = torch.randint(0, 8, (3, 2))\n",
                "t[indices].shape"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0858ac95",
            "metadata": {},
            "source": [
                "### torch.gather\n",
                "\n",
                "torch.gather 往往用于我们希望依次在输入 Tensor 的某个维度上取出其中一些索引的值。比如下面的 topK 的结果中，我们希望根据返回的 indices 取得对应的元素，也就是 values，这里可以用 gather"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "id": "1eac5e3f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([[ 0.5896,  0.2317,  0.5549,  0.5040,  1.3366],\n",
                        "        [ 0.3820,  0.5324,  0.8154, -1.6021,  0.8312],\n",
                        "        [-0.5906, -0.1350,  1.3965,  1.3246,  1.2807]])\n",
                        "tensor([[1.3366, 0.5896],\n",
                        "        [0.8312, 0.8154],\n",
                        "        [1.3965, 1.3246]])\n",
                        "tensor([[4, 0],\n",
                        "        [4, 2],\n",
                        "        [2, 3]])\n"
                    ]
                }
            ],
            "source": [
                "t = torch.randn(3, 5)\n",
                "values, indices = torch.topk(t, k=2, dim=1)\n",
                "print(t)\n",
                "print(values)\n",
                "print(indices)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "id": "ebd0d2bb",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[True, True],\n",
                            "        [True, True],\n",
                            "        [True, True]])"
                        ]
                    },
                    "execution_count": 61,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "values == torch.gather(t, dim=1, index=indices)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "through-marketplace",
            "metadata": {},
            "source": [
                "## 组合与分片\n",
                "\n",
                "### torch.cat\n",
                "\n",
                "```\n",
                "cat(tensors, dim=0) -> Tensor\n",
                "```\n",
                "\n",
                "`torch.cat`将给定义的tensor的序列(tensors)，按给定义的维度上合并起来，这就要求，这些tensor，除了合并的维度，其他的维度必须一致。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "id": "every-parade",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[-0.8458,  0.5044, -0.2978],\n",
                            "        [ 1.0015,  0.3516, -0.6523],\n",
                            "        [-0.8931, -1.1678,  0.1994],\n",
                            "        [ 0.4824,  0.0945, -0.2003],\n",
                            "        [-1.1476, -0.6038,  0.5421]])"
                        ]
                    },
                    "execution_count": 62,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "t1 = torch.randn(2, 3)\n",
                "t2 = torch.randn(3, 3)\n",
                "torch.cat([t1, t2], dim=0)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "cardiac-luther",
            "metadata": {},
            "source": [
                "### torch.stack\n",
                "\n",
                "`torch.stack`和`torch.cat`接口用法一致，但它并不是在原有的维度上拼接，而是直接扩展一个新的维度。\n",
                "\n",
                "这就要求，序列中的tensor在维度上必须一致。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 63,
            "id": "peripheral-membrane",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[[ 0.6103,  0.7689, -1.8726],\n",
                            "         [-0.8682,  0.2326,  0.8563]],\n",
                            "\n",
                            "        [[ 1.8399, -1.2037,  0.5311],\n",
                            "         [ 0.3968,  1.1138,  1.1531]]])"
                        ]
                    },
                    "execution_count": 63,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "t1 = torch.randn(2, 3)\n",
                "t2 = torch.randn(2, 3)\n",
                "torch.stack([t1, t2], dim=0)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "complimentary-ordinance",
            "metadata": {},
            "source": [
                "### torch.split\n",
                "```python\n",
                "split(tensor, split_size_or_sections, dim=0)\n",
                "```\n",
                "`split`将tensor按指定的维度，分拆为多个Tensor的元组，拆分的块chunk的大小是splite_size指定的。可能出现不能整分的情况，这时候最后一块大小一般小于splite_size\n",
                "\n",
                "split出来的Tensor是原tensor的一个view"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 64,
            "id": "identical-discipline",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[0, 1],\n",
                            "        [2, 3],\n",
                            "        [4, 5],\n",
                            "        [6, 7],\n",
                            "        [8, 9]])"
                        ]
                    },
                    "execution_count": 64,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "a = torch.arange(10).view(5, 2)\n",
                "a"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 65,
            "id": "subject-progressive",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(tensor([[0, 1],\n",
                            "         [2, 3]]),\n",
                            " tensor([[4, 5],\n",
                            "         [6, 7]]),\n",
                            " tensor([[8, 9]]))"
                        ]
                    },
                    "execution_count": 65,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.split(a, 2)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "narrative-hearts",
            "metadata": {},
            "source": [
                "`split_size_or_sections`也可能是一个list(int)，这时候，它的每个元素，代表每个chunk的大小"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 66,
            "id": "starting-algorithm",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(tensor([[0, 1]]),\n",
                            " tensor([[2, 3],\n",
                            "         [4, 5],\n",
                            "         [6, 7]]),\n",
                            " tensor([[8, 9]]))"
                        ]
                    },
                    "execution_count": 66,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "a1, a2, a3 = torch.split(a, (1, 3, 1))\n",
                "a1, a2, a3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 67,
            "id": "specified-messaging",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[42,  1],\n",
                            "        [ 2,  3],\n",
                            "        [ 4,  5],\n",
                            "        [ 6,  7],\n",
                            "        [ 8,  9]])"
                        ]
                    },
                    "execution_count": 67,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# 切分出来的tensor和原tensor是共享存储的\n",
                "a1[0, 0] = 42\n",
                "a"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "confidential-charger",
            "metadata": {},
            "source": [
                "### torch.chunk\n",
                "\n",
                "```python\n",
                "chunk(input, chunks, dim=0) -> List of Tensors\n",
                "```\n",
                "`chunk`和`split`功能类似，不同在于，chunk的第二的参数，直接指定的是chunk的数量，最后一个chunk的数量可能会少一些。也有可能`axis[dim]<chunks`，那么就直接切分为`axis[dim]`个。\n",
                "\n",
                "切分出来的这些Tensor和原Tensor都是共享底层存储的，也就是说每个chunk都是原Tensor的一个view。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 68,
            "id": "ba0a82b6",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "torch.Size([5, 2])\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "2"
                        ]
                    },
                    "execution_count": 68,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "print(a.shape)\n",
                "len(a.chunk(3, dim=1))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "general-sitting",
            "metadata": {},
            "source": [
                "## 变换操作\n",
                "\n",
                "### torch.reshape\n",
                "\n",
                "```python\n",
                "reshape(input, shape) -> Tensor\n",
                "```\n",
                "`reshape`返回一个和原Tensor具有相同数据，相同数量的Tensor，只是shape不一致。"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "gross-botswana",
            "metadata": {},
            "source": [
                "### torch.view\n",
                "\n",
                "torch.view vs. torch.reshape\n",
                "\n",
                "`reshape`可以用在`compact`或`non-compact`的tensor上，而`view`只能用在`compact`的tensor上。`reshape`如果作用于`non-compact`的tensor上，则会产生一个copy\n",
                "\n",
                "torch.view has existed for a long time. It will return a tensor with the new shape. The returned tensor will share the underling data with the original tensor. See the documentation here.\n",
                "\n",
                "On the other hand, it seems that torch.reshape has been introduced recently in version 0.4. According to the document, this method will\n",
                "\n",
                "> Returns a tensor with the same data and number of elements as input, but with the specified shape. When possible, the returned tensor will be a view of input. Otherwise, it will be a copy. Contiguous inputs and inputs with compatible strides can be reshaped without copying, but you should not depend on the copying vs. viewing behavior.\n",
                "\n",
                "It means that torch.reshape may return a copy or a view of the original tensor. You can not count on that to return a view or a copy. According to the developer:\n",
                "\n",
                "> if you need a copy use clone() if you need the same storage use view(). The semantics of reshape() are that it may or may not share the storage and you don't know beforehand.\n",
                "\n",
                "Another difference is that reshape() can operate on both contiguous and non-contiguous tensor while view() can only operate on contiguous tensor. Also see here about the meaning of contiguous."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "herbal-opening",
            "metadata": {},
            "source": [
                "### torch.transpose\n",
                "\n",
                "```python\n",
                "transpose(input, dim0, dim1) -> Tensor\n",
                "```\n",
                "转置input的指定的2个维度，返回的Tensor和原来的Tensor共享存储"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 69,
            "id": "toxic-delay",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[[0.1958, 0.6966, 0.5887, 0.5991],\n",
                            "         [0.2259, 0.0063, 0.9598, 0.5841],\n",
                            "         [0.0613, 0.7575, 0.9952, 0.8753]],\n",
                            "\n",
                            "        [[0.5962, 0.9140, 0.4718, 0.9588],\n",
                            "         [0.5693, 0.2644, 0.6633, 0.6033],\n",
                            "         [0.5709, 0.3676, 0.2916, 0.1691]]])"
                        ]
                    },
                    "execution_count": 69,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "x = torch.rand(2, 3, 4)\n",
                "x"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "id": "boxed-jaguar",
            "metadata": {},
            "outputs": [],
            "source": [
                "y = torch.transpose(x, 0, 2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 71,
            "id": "c5d32c94",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(12, 4, 1)\n",
                        "(1, 4, 12)\n"
                    ]
                }
            ],
            "source": [
                "print(x.stride())\n",
                "print(y.stride())"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "built-beach",
            "metadata": {},
            "source": [
                "### torch.permute"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 72,
            "id": "c2b403c3",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "torch.Size([4, 2, 3])"
                        ]
                    },
                    "execution_count": 72,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.permute(x, (2, 0, 1)).shape"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "93080d1e",
            "metadata": {},
            "source": [
                "### squeeze和unsqueeze\n",
                "\n",
                "squeeze在指定的维度上添加一维，而unsqueeze则在指定的维度上去掉`size=1`的维度，如果对应维度上的size不等于1，则不做任何操作"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 73,
            "id": "7c8896e5",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "x shape: torch.Size([2, 3]), \n",
                        "y (x.unsqueeze) shape: torch.Size([1, 2, 1, 3])\n",
                        "unsqueeze shape torch.Size([2, 1, 3])\n"
                    ]
                }
            ],
            "source": [
                "x = torch.randn(2, 3)\n",
                "y = x.unsqueeze(dim=1).unsqueeze(dim=0)\n",
                "print(f\"x shape: {x.shape}, \\ny (x.unsqueeze) shape: {y.shape}\")\n",
                "print(\"unsqueeze shape\", y.squeeze(dim=0).shape)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "clinical-phoenix",
            "metadata": {},
            "source": [
                "### contiguous\n",
                "\n",
                "There are a few operations on Tensors in PyTorch that do not change the contents of a tensor, but change the way the data is organized. These operations include:\n",
                "\n",
                "`narrow()`, `view()`, `expand()` and `transpose()`\n",
                "\n",
                "For example: when you call transpose(), PyTorch doesn't generate a new tensor with a new layout, it just modifies meta information in the Tensor object so that the offset and stride describe the desired new shape. In this example, the transposed tensor and original tensor share the same memory:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 74,
            "id": "practical-example",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor(42.)\n"
                    ]
                }
            ],
            "source": [
                "x = torch.randn(3, 2)\n",
                "y = torch.transpose(x, 0, 1)\n",
                "x[0, 0] = 42\n",
                "print(y[0, 0])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "75c7c0ae",
            "metadata": {},
            "source": [
                "This is where the concept of contiguous comes in. In the example above, x is contiguous but y is not because its memory layout is different to that of a tensor of same shape made from scratch. Note that the word \"contiguous\" is a bit misleading because it's not that the content of the tensor is spread out around disconnected blocks of memory. Here bytes are still allocated in one block of memory but the order of the elements is different!\n",
                "\n",
                "When you call contiguous(), it actually makes a copy of the tensor such that the order of its elements in memory is the same as if it had been created from scratch with the same data.\n",
                "\n",
                "Normally you don't need to worry about this. You're generally safe to assume everything will work, and wait until you get a RuntimeError: input is not contiguous where PyTorch expects a contiguous tensor to add a call to contiguous()."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "interior-morrison",
            "metadata": {},
            "source": [
                "## 降维操作"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "hundred-syria",
            "metadata": {},
            "source": [
                "### torch.mean\n",
                "\n",
                "```python\n",
                "'''\n",
                "Args:\n",
                "  input (Tensor): the input tensor.\n",
                "  dim (int or tuple of ints): the dimension or dimensions to reduce.\n",
                "  keepdim (bool): whether the output tensor has :attr:`dim` retained or not.\n",
                "'''\n",
                "mean(input, dim, keepdim=False, *, out=None) -> Tensor\n",
                "```\n",
                "\n",
                "对input沿着`dim`的维度求均值，这样的话，指定的那个维度就会被压缩掉，如果指定了`keepdim=True`的话，那个维度会保留，值为1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 75,
            "id": "centered-murray",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[ 0.7664, -0.8458,  0.2225, -1.0404,  0.9716,  0.6827],\n",
                            "        [-0.6373,  1.3663,  0.1353,  1.0554,  0.5993, -1.8393],\n",
                            "        [-0.2648,  0.2339,  0.9827,  1.4707, -0.4960,  0.0141],\n",
                            "        [ 1.3204, -0.9046, -0.0559,  1.2988, -0.3727, -0.7442],\n",
                            "        [-1.5427, -1.3474, -0.8282,  0.7764,  2.6069, -0.3903]])"
                        ]
                    },
                    "execution_count": 75,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "t = torch.randn(5, 6)\n",
                "t"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 76,
            "id": "demographic-country",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([-0.0716, -0.2995,  0.0913,  0.7122,  0.6618, -0.4554])"
                        ]
                    },
                    "execution_count": 76,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# 按列的方向(dim=0)将整个Tenoor压缩成为1维的\n",
                "torch.mean(t, dim=0)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 77,
            "id": "important-politics",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[ 0.1262],\n",
                            "        [ 0.1133],\n",
                            "        [ 0.3234],\n",
                            "        [ 0.0903],\n",
                            "        [-0.1209]])"
                        ]
                    },
                    "execution_count": 77,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.mean(t, dim=1, keepdim=True)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "appointed-october",
            "metadata": {},
            "source": [
                "对于高维Tensor，我们还可以同时对多个维度进行Reduce，求其均值。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 78,
            "id": "statistical-tonight",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[[-0.6024,  0.4861, -0.3549,  1.2723],\n",
                            "         [ 0.7566, -0.3582, -1.0746,  2.0221],\n",
                            "         [-0.5265, -1.8632, -1.2744, -0.2883]],\n",
                            "\n",
                            "        [[-0.9824,  1.8946, -0.5908, -0.8086],\n",
                            "         [ 0.2823, -0.5457,  0.9007,  1.5686],\n",
                            "         [ 0.2141,  0.1007,  0.3016,  0.2697]]])"
                        ]
                    },
                    "execution_count": 78,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "t = torch.randn(2, 3, 4)\n",
                "t"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 79,
            "id": "written-setup",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([ 0.0392,  0.4440, -0.3833])"
                        ]
                    },
                    "execution_count": 79,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# 等价于reduce第0维，得到一个3x4的Tensor后，再reduce第1维，得到(3,)的Vector\n",
                "torch.mean(t, dim=(0, 2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 80,
            "id": "million-drive",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([ 0.0392,  0.4440, -0.3833])"
                        ]
                    },
                    "execution_count": 80,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "t.mean(0).mean(1)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "secondary-explorer",
            "metadata": {},
            "source": [
                "### torch.sum\n",
                "\n",
                "`torch.sum`是一个和`torch.mean`用法上很像的操作，只是`sum`的reduce op变成了求和，而不是求均值。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 81,
            "id": "separated-illustration",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([ 0.3139,  3.5517, -3.0662])"
                        ]
                    },
                    "execution_count": 81,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.sum(t, dim=(0, 2))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "31707904",
            "metadata": {},
            "source": [
                "### torch.argmax"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 82,
            "id": "a96184a7",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "x: tensor([[0.5064, 0.0923, 0.3140],\n",
                        "        [0.4148, 0.5013, 0.0254]])\n",
                        "Argmax: tensor([0, 1])\n"
                    ]
                }
            ],
            "source": [
                "x = torch.rand((2, 3))\n",
                "print(\"x:\", x)\n",
                "print(\"Argmax:\", x.argmax(dim=1))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "98fbd95c",
            "metadata": {},
            "source": [
                "### torch.maxmimu\n",
                "\n",
                "相同 Shape 的 Tensor 和 Tensor 按元素逐个比大小，保留最大的"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 83,
            "id": "fe8a0a3f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "x:\n",
                        "\ttensor([[ 0.2393, -0.8319,  0.1255],\n",
                        "        [ 1.7573, -1.0650,  0.1229]]) \n",
                        "relu(x):\n",
                        "\ttensor([[0.2393, 0.0000, 0.1255],\n",
                        "        [1.7573, 0.0000, 0.1229]])\n"
                    ]
                }
            ],
            "source": [
                "def relu(x):\n",
                "    return torch.maximum(x, torch.tensor(0))\n",
                "\n",
                "\n",
                "x = torch.randn((2, 3))\n",
                "print(f\"x:\\n\\t{x} \\nrelu(x):\\n\\t{relu(x)}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "ec47a468",
            "metadata": {},
            "source": [
                "## 排序"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "super-still",
            "metadata": {},
            "source": [
                "### torch.sort\n",
                "\n",
                "```python\n",
                "sort(input, dim=-1, descending=False, *, out=None) -> (Tensor, LongTensor)\n",
                "```\n",
                "`sort`对input按给定义的dim进行升序排列，返回排列后的Tensor的同时，也返回一个对应的下标的重排后的Tensor\n",
                "\n",
                "dim的默认值是Tensor的最后一维"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 84,
            "id": "potential-morrison",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([[0.6248, 0.7040, 0.7312],\n",
                        "        [0.9658, 0.5388, 0.5723]])\n",
                        "tensor([[0.7312, 0.7040, 0.6248],\n",
                        "        [0.9658, 0.5723, 0.5388]])\n",
                        "tensor([[2, 1, 0],\n",
                        "        [0, 2, 1]])\n"
                    ]
                }
            ],
            "source": [
                "a = torch.rand(2, 3)\n",
                "print(a)\n",
                "values, indices = torch.sort(a, dim=1, descending=True)\n",
                "print(values)\n",
                "print(indices)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "martial-petersburg",
            "metadata": {},
            "source": [
                "### torch.topk\n",
                "\n",
                "```python\n",
                "topk(input, k, dim=None, largest=True, sorted=True, *, out=None) -> (Tensor, LongTensor)\n",
                "```\n",
                "`topk`返回input中指定维度上，最大的k个元素，以及对应的索引。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 85,
            "id": "pending-nickname",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([-1.1304, -1.0381, -1.1502,  1.2661,  0.4508])"
                        ]
                    },
                    "execution_count": 85,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "a = torch.randn(5)\n",
                "a"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 86,
            "id": "practical-review",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([ 1.2661,  0.4508, -1.0381])\n",
                        "tensor([3, 4, 1])\n"
                    ]
                }
            ],
            "source": [
                "values, indices = torch.topk(a, 3)\n",
                "print(values)\n",
                "print(indices)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "sonic-groove",
            "metadata": {},
            "source": [
                "### torch.kthvalue\n",
                "\n",
                "```python\n",
                "kthvalue(input, k, dim=None, keepdim=False, *, out=None) -> (Tensor, LongTensor)\n",
                "```\n",
                "`kthvalue`计算输出Tensor的指定维度上第`k`小的元素以及下标。如果dim没有指定，则默认为Tensor的最后一维。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 87,
            "id": "august-chamber",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[ 0.3197,  0.4240,  0.3617],\n",
                            "        [-2.1741,  2.1711,  0.4256],\n",
                            "        [ 0.5445, -0.3068, -1.1504],\n",
                            "        [ 0.5853, -0.8309,  1.4965]])"
                        ]
                    },
                    "execution_count": 87,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "a = torch.randn(4, 3)\n",
                "a"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 88,
            "id": "cardiovascular-ticket",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "torch.return_types.kthvalue(\n",
                            "values=tensor([ 0.3197, -0.3068,  0.3617]),\n",
                            "indices=tensor([0, 2, 0]))"
                        ]
                    },
                    "execution_count": 88,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.kthvalue(a, 2, dim=0)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "acknowledged-invitation",
            "metadata": {},
            "source": [
                "## 原地操作(in-place)\n",
                "\n",
                "pytorch的Tensor支持了很多原地操作，它们的特点就是在方法末尾以`_`结束"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 89,
            "id": "convinced-graphics",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "t1 = tensor([[1., 1., 1.],\n",
                        "        [1., 1., 1.]])\n",
                        "after plus 2: t1 = tensor([[3., 3., 3.],\n",
                        "        [3., 3., 3.]])\n"
                    ]
                }
            ],
            "source": [
                "t1 = torch.ones(2, 3)\n",
                "print(f\"t1 = {t1}\")\n",
                "t1.add_(2)\n",
                "print(f\"after plus 2: t1 = {t1}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "electric-austria",
            "metadata": {},
            "source": [
                "## 转换为其他数据类型\n",
                "\n",
                "我们可以调用`numpy`接口,返回一个numpy.ndarray的对象，可以调用`tolist`接口，返回一个list的对象"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 90,
            "id": "clean-nashville",
            "metadata": {},
            "outputs": [],
            "source": [
                "t = torch.tensor([1, 2, 3, 4, 5, 6])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 91,
            "id": "multiple-adapter",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([1, 2, 3, 4, 5, 6])"
                        ]
                    },
                    "execution_count": 91,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# 返回的ndarray还是和t是共享存储的\n",
                "t.numpy()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 92,
            "id": "unlimited-patio",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[[1, 2, 3], [4, 5, 6]]"
                        ]
                    },
                    "execution_count": 92,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "t.reshape(2, 3).tolist()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "207b647a",
            "metadata": {},
            "source": [
                "## repeat和repeat_interleave"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 93,
            "id": "b70e577b",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[0, 1, 2],\n",
                            "        [3, 4, 5]])"
                        ]
                    },
                    "execution_count": 93,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "a = torch.arange(6).reshape((2, 3))\n",
                "a"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "cefff4c1",
            "metadata": {},
            "source": [
                "`repeat(d0, d1, d2)` 将对应的维度复制多份，如果之前没有对应的维度，则可以当作原来维度为1，处理。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 94,
            "id": "2432d308",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[[0, 1, 2, 0, 1, 2],\n",
                            "         [3, 4, 5, 3, 4, 5]],\n",
                            "\n",
                            "        [[0, 1, 2, 0, 1, 2],\n",
                            "         [3, 4, 5, 3, 4, 5]]])"
                        ]
                    },
                    "execution_count": 94,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "a.repeat((2, 1, 2))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "73f6fc4c",
            "metadata": {},
            "source": [
                "`repeat_interleave(n, dim)` 在对应的维度上进行复制，但复制的方式不是`[a b c a b c ]`这种，而是`[a a b b c c]`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 95,
            "id": "4a924af5",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[0, 1, 2],\n",
                            "        [0, 1, 2],\n",
                            "        [3, 4, 5],\n",
                            "        [3, 4, 5]])"
                        ]
                    },
                    "execution_count": 95,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "a.repeat_interleave(2, dim=0)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a6a3a4f5",
            "metadata": {},
            "source": [
                "# 爱因斯坦标识"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "683f0574",
            "metadata": {},
            "outputs": [],
            "source": [
                "from einops import rearrange, reduce, repeat"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c088bd39",
            "metadata": {},
            "source": [
                "## 实现 Transpose 和 Permute 的功能"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "f745fab5",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "x = torch.randn(2, 3, 8, 8)\n",
                "\n",
                "# Transose\n",
                "torch.allclose(rearrange(x, \"b c h w->b h w c\"), x.permute((0, 2, 3, 1)))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7d9dd32c",
            "metadata": {},
            "source": [
                "## 一步实现 Transpose + Reshape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "ca839bae",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# 一步完成 Transpose + Reshape\n",
                "torch.allclose(\n",
                "    rearrange(x, \"b c h w -> (b h w) c\"), x.permute(0, 2, 3, 1).reshape(-1, x.size(1))\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "be7690a5",
            "metadata": {},
            "source": [
                "## 实现维度拆分"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "id": "58e2386c",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 24,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "x = torch.randn(2, 3, 64)\n",
                "\n",
                "torch.allclose(rearrange(x, \"b c (h w) -> b c h w\", h=8), x.reshape(2, 3, 8, 8))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e6e92d7f",
            "metadata": {},
            "source": [
                "## 实现 Image2Patch 的功能\n",
                "\n",
                "将 二维图像转换为 $B\\times N \\times D$ 的序列 Patches 的形式。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "id": "fde29237",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "torch.Size([2, 1024, 192])\n"
                    ]
                }
            ],
            "source": [
                "image = torch.randn(2, 3, 256, 256)\n",
                "\n",
                "patches = rearrange(image, \"b c (h1 ph) (w1 pw) -> b (h1 w1) (ph pw c)\", ph=8, pw=8)\n",
                "print(patches.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "84091f14",
            "metadata": {},
            "source": [
                "## Reduce 操作"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "id": "c41a5946",
            "metadata": {},
            "outputs": [],
            "source": [
                "x = torch.randn(8, 10)\n",
                "\n",
                "# mean\n",
                "x_mean = reduce(x, \"b d -> b\", reduction=\"mean\")\n",
                "# sum\n",
                "x_sum = reduce(x, \"b d -> 1 d\", reduction=\"sum\")\n",
                "\n",
                "x = torch.randn(2, 3, 4)\n",
                "x_max = reduce(x, \"b n d -> d\", reduction=\"max\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "48792104",
            "metadata": {},
            "source": [
                "## 扩维与复制"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "id": "d0a07612",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "torch.Size([1, 5, 5])"
                        ]
                    },
                    "execution_count": 33,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "x = torch.randn(5, 5)\n",
                "rearrange(x, \"i j -> 1 i j\").shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "id": "e313f184",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "torch.Size([3, 5, 25])"
                        ]
                    },
                    "execution_count": 35,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "x = torch.randn(1, 5, 5)\n",
                "repeat(x, \"1 i j -> 3 i (5 j)\").shape"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "pyml",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.13"
        },
        "vscode": {
            "interpreter": {
                "hash": "3c37a17f5960fe44e0e933f0a97f50106b021053c3b378a6f2025e9a4390c58c"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
