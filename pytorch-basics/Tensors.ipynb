{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64b3b5af",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "\n",
    "Tensor 是 Pytorch 中最核心的数据结构，在 Pytorch 中几乎所有的形式的数据都在用 Tensor表示，模型的输入与输出格式都是 Tensor。它是一种多维数组结构，类似于 NumPy 的 `ndarray`，但具有更高的灵活性和 GPU 加速的能力。Tensor 的特点和操作在计算图（Computational Graph）中可以高效地支持梯度计算，适合自动微分（Autograd），因此在构建和训练神经网络时被广泛使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "placed-poison",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "serial-compromise",
   "metadata": {},
   "source": [
    "## Tensor的属性"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c14402d",
   "metadata": {},
   "source": [
    "### Tensor 的数据类型\n",
    "\n",
    "torch 在创建 Tensor 时，`dtype`的指定只支持使用`torch.[DataType]`这样的方式去指定，而不能像numpy一样，可以直接使用字符串。\n",
    "\n",
    "以下显示了从 `List` 创建一个新的 Tensor 时，同时指定了创建的 Tensor 的数值类型为 `int`，如果不显示的指定，那么默认是 `torch.int64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28fefdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype of t1: torch.int32\n",
      "dtype of t2: torch.int64\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([1, 2, 3], dtype=torch.int)\n",
    "t2 = torch.tensor([1, 2, 3])\n",
    "print(f\"dtype of t1: {t1.dtype}\")\n",
    "print(f\"dtype of t2: {t2.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2434a369",
   "metadata": {},
   "source": [
    "区别于 Torch，在 Numpy 中 \"int\" 默认代表的是 `int64`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d363752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype of t: int64\n"
     ]
    }
   ],
   "source": [
    "t = np.array([1, 2, 3], dtype=\"int\")\n",
    "print(f\"dtype of t: {t.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b55a947",
   "metadata": {},
   "source": [
    "其中`torch.dtype`表示Tensor的数据类型，常见的有下面几种不同的数据类型，和 c/c++ 中的表示是比较一致的。\n",
    "\n",
    "浮点类型：\n",
    "\n",
    "- `torch.float32`或`torch.float`\n",
    "- `torch.float64`或`torch.double`\n",
    "- `torch.float16`或`torch.half`\n",
    "- `torch.bfloat16`\n",
    "\n",
    "整数类型\n",
    "\n",
    "- `torch.uint8`\n",
    "- `torch.int8`\n",
    "- `torch.int16`\n",
    "- `torch.int32`\n",
    "- `torch.int64`\n",
    "\n",
    "布尔类型\n",
    "\n",
    "- `torch.bool`\n",
    "\n",
    "虚数类型\n",
    "\n",
    "\n",
    "- `torch.complex32`: complex32 目前只是实验性的支持，后续可能会取消支持\n",
    "- `torch.complex64`\n",
    "- `torch.complex128`或`torch.cdouble`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b7f425",
   "metadata": {},
   "source": [
    "虚数类型的 Tensor 在深度学习中使用的比较少，最近随机大语言模型的兴起，在相对旋转位置编码（RoPE）中有所使用。下面的代码演示了虚数 Tensor 的创建。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "888128cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5276+0.7824j, -0.3142+0.1675j,  0.4610+0.3706j],\n",
      "        [-0.0702+0.9847j,  0.3647+0.0814j,  0.6853-1.1555j],\n",
      "        [-0.3324+0.0389j,  1.4756+0.8922j, -0.2835+0.4831j]])\n",
      "torch.complex64\n",
      "torch.complex128\n"
     ]
    }
   ],
   "source": [
    "# 创建一个 Complex64 张量\n",
    "real_part = torch.randn(3, 3)  # 实部\n",
    "imag_part = torch.randn(3, 3)  # 虚部\n",
    "complex_tensor = torch.complex(\n",
    "    real_part, imag_part\n",
    ")  # 构建出来的 Tensor 默认是 complex64 类型\n",
    "\n",
    "print(complex_tensor)\n",
    "print(complex_tensor.dtype)  # 输出: torch.complex64\n",
    "complex_tensor = complex_tensor.to(dtype=torch.cdouble)\n",
    "print(complex_tensor.dtype)  # 输出: torch.complex128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "indian-vehicle",
   "metadata": {},
   "source": [
    "[`bfloat16`](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format)是一种和IEEE half-precision 16-bit float规定不一致的16Bit浮点数格式，它是直接对32位的IEEE 754规定的单精度float32的格式进行截取形成的，它是为机器学习系统特别定制的，它的组成是：\n",
    "- 1位符号位\n",
    "- 8个指数位\n",
    "- 7个小数位\n",
    "\n",
    "神经网络对指数的大小比对尾数的大小更敏感，也就是对数值的表示范围比较敏感。为了确保下溢、上溢和`NaN`的行为完全相同，`bfloat16`的指数大小与`float32`相同。`bfloat16`处理非规格化数的规则与`float32`不同，它会将它们截断为零。与通常需要特殊处理，如损失缩放的`float16`不同，`bfloat16`是训练和运行深度神经网络时`float32`的即插即用替换。\n",
    "\n",
    "简而言之，`bfloat16`表示的数值范围更大，但是精度不如`float16`。\n",
    "\n",
    "<div class=\"wy-nav-content-img\">\n",
    "    <img src=\"assets/Tensors_floating_point_formats.png\" width=\"800px\" alt=\"34 层的ResNet架构与 VGG19 以及线性连接的结构之间的对比\">\n",
    "    <p>图1: 三种浮点数格式的表示与区别</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6105f4",
   "metadata": {},
   "source": [
    "另外，在 Torch 2.x 的版本中也开始实验性的支持`FP8`格式，目前支持两种类型的`FP8`格式，一种是`E3M4`一种是`E5M2`，其中`E5M2`和 IEEE 的 `fp16`的指数部分长度一致，这使得 E5M2 和 IEEE FP16 格式之间可以直接转换。`E5M2`它表达的数值范围更大，可以处理一些特殊值。\n",
    "\n",
    "一般来说，我们会在模型推理以及训练过程的前向阶段为了保持较高的精度会使用`E3M4`的格式，而在训练的反向阶段，使用`E5M2`来有较好的数值动态范围，避免梯度消失与爆炸。\n",
    "\n",
    "Paper: [FP8 Formats for Deep Learning](https://arxiv.org/abs/2209.05433)\n",
    "\n",
    "<div class=\"wy-nav-content-img\">\n",
    "    <img src=\"assets/Tensors_FP8_format.png\" width=\"600px\" alt=\"二种半精度浮点数与二种 FP8 精度在存储格式上的区别\">\n",
    "    <p>图2: 二种半精度浮点数与二种 FP8 精度在存储格式上的区别</p>\n",
    "</div>\n",
    "\n",
    "上图展示了使用不同数值类型来表示“0.3952” 时，实际能够近似到的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57d45c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type        bits              max               min    smallest normal          eps\n",
      "-------------  ------  ---------------  ----------------  -----------------  -----------\n",
      "float32            32      3.40282e+38      -3.40282e+38        1.17549e-38  1.19209e-07\n",
      "float16            16  65504            -65504                  6.10352e-05  0.000976562\n",
      "bfloat16           16      3.38953e+38      -3.38953e+38        1.17549e-38  0.0078125\n",
      "float8_e4m3fn       8    448              -448                  0.015625     0.125\n",
      "float8_e5m2         8  57344            -57344                  6.10352e-05  0.25\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tabulate import tabulate\n",
    "\n",
    "f32_type = torch.float32\n",
    "f16_type = torch.float16\n",
    "bf16_type = torch.bfloat16\n",
    "e4m3_type = torch.float8_e4m3fn\n",
    "e5m2_type = torch.float8_e5m2\n",
    "\n",
    "# collect finfo for each type\n",
    "table = []\n",
    "for dtype in [f32_type, f16_type, bf16_type, e4m3_type, e5m2_type]:\n",
    "    numbits = (\n",
    "        32\n",
    "        if dtype == f32_type\n",
    "        else 16 if dtype == bf16_type or dtype == f16_type else 8\n",
    "    )\n",
    "    info = torch.finfo(dtype)\n",
    "    table.append(\n",
    "        [info.dtype, numbits, info.max, info.min, info.smallest_normal, info.eps]\n",
    "    )\n",
    "\n",
    "headers = [\"data type\", \"bits\", \"max\", \"min\", \"smallest normal\", \"eps\"]\n",
    "print(tabulate(table, headers=headers))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "working-commerce",
   "metadata": {},
   "source": [
    "### Tensor 的设备类型\n",
    "\n",
    "`torch.device`表示的是Tensor的数据存储的设备，其中分为`cpu`和`cuda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "advisory-diagnosis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "funny-acoustic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aggressive-inspection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(type=\"cuda\", index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cosmetic-baptist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1, 2, 3, 4, 5, 6], device=torch.device(\"cuda:0\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "sought-algorithm",
   "metadata": {},
   "source": [
    "### Tensor 的内存布局\n",
    "\n",
    "Tensor 的 `layout` 属性表示Tensor内部数据存储的内部布局，目前还是一个不成熟(beta)的特性，目前支持\n",
    "\n",
    "- torch.strided\n",
    "- torch.sparse_coo\n",
    "\n",
    "现在主要用的就是面向 dense Tensor的`torch.strided`，Tensor的 `strides` 是一个list，它代表每个dimension上两邻两个元素之间的跨度(元素个数)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abandoned-spanking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 5, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(60).reshape(3, 4, 5).stride()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643e5b03",
   "metadata": {},
   "source": [
    "我们可以理解为 Tensor 底层的存储的是一个一维的数组，我们对于 `Tensor`的索引，全部是是通过一个下标对应的 stride 来计算出最终在一维数组上的偏移量。 这样实现的好处时，对于 `Tensor`的很多操作，它并不需要实际对 `Tensor`的内存数据进行变动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cf2ce6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n",
      "(1, 4)\n"
     ]
    }
   ],
   "source": [
    "t = torch.arange(12).view(3, 4)\n",
    "# t_transposed 和 t 共享底层的数据\n",
    "t_transposed = t.transpose(0, 1)\n",
    "print(t.stride())\n",
    "print(t_transposed.stride())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fff0e6",
   "metadata": {},
   "source": [
    "`stride()`方法除了可以直接返回一个`List`之外，还可以通过维度的索引，返回对应维度上相邻两个元素之间在底层数组上的跨度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e9d63ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stride on dim 0: 4\n",
      "stride on dim 1: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"stride on dim 0: {t.stride(0)}\")\n",
    "print(f\"stride on dim 1: {t.stride(1)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "127fa16a",
   "metadata": {},
   "source": [
    "和 Numpy 不同的是，torch 中的 `stride` 以元素的个数来表示跨度，而 Numpy 则是用字节数量来表示相邻两个元素之间的跨度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "467f02b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 40, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(60).reshape(3, 4, 5).strides"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "painful-authorization",
   "metadata": {},
   "source": [
    "### Tensor属性转换\n",
    "\n",
    "我们可以使用`to`方法来进行 Tensor 的相关属性的转换，包括：数值类型、设备类型等，接口返回的是一个新的转换后的 Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "minute-cassette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64 cpu\n",
      "torch.float32 cuda:0\n"
     ]
    }
   ],
   "source": [
    "device_cuda = torch.device(\"cuda\")\n",
    "data = torch.tensor([1])\n",
    "print(data.dtype, data.device)\n",
    "data = data.to(dtype=torch.float32, device=device_cuda)\n",
    "print(data.dtype, data.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccddd5b",
   "metadata": {},
   "source": [
    "也可以通过 Tensor 的`dtype`方法来直接将返回新数据类型的 Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aba23467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.int() # 将 data 转换为 int 类型\n",
    "data.float() # 将 data 转换为 bool 类型\n",
    "data.bool() # 将 data 转换为 bool 类型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "entertaining-aquatic",
   "metadata": {},
   "source": [
    "### Tensor的形状\n",
    "\n",
    "Tensor除了具有3个标准的属性外，一旦我们创建了一个Tensor，那么它就会具有一些形状相关的属性，我们可以通过下面这些接口获取 `Tensor` 不同的尺寸相关的信息。\n",
    "\n",
    "- `t.shape`: 返回的是一个 `torch.Size(tuple)` 类型的结果，表示每一维的维度值\n",
    "- `t.size()`: 和 `t.shape` 一致\n",
    "- `t.size(i)`: 返回第 `i` 个维度的值\n",
    "- `t.ndim`：返回 `Tensor` 有多少维\n",
    "- `t.numel()`：它是一个方法，返回 `Tensor` 内有多少个元素\n",
    "- `len(t)`：返回的是 `Tensor` 在第`0`维上的维度值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "headed-dylan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of t: torch.Size([2, 3, 4])\n",
      "size of t: torch.Size([2, 3, 4])\n",
      "size(1) of t: 3\n",
      "strides of t: (12, 4, 1)\n",
      "strides of axes1 of t: 4\n",
      "ndim of t: 3\n",
      "numel of t: 24\n",
      "len of t: 2\n"
     ]
    }
   ],
   "source": [
    "t = torch.empty(2, 3, 4)\n",
    "print(f\"shape of t: {t.shape}\")\n",
    "print(f\"size of t: {t.size()}\")\n",
    "print(f\"size(1) of t: {t.size(1)}\")\n",
    "print(f\"strides of t: {t.stride()}\")\n",
    "print(f\"strides of axes{1} of t: {t.stride(1)}\")\n",
    "print(f\"ndim of t: {t.ndim}\")\n",
    "print(f\"numel of t: {t.numel()}\")\n",
    "print(f\"len of t: {len(t)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "moderate-country",
   "metadata": {},
   "source": [
    "## Tensor的创建"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "neural-outline",
   "metadata": {},
   "source": [
    "在Pytorch中我们可以有多种方法来创建Tensor，常用的包括下面几种：\n",
    "\n",
    "- 从已有的 `scalar`、`list`、`tuple`、`numpy.array` 来创建\n",
    "- 用`arange`、`linspace`、`logspace`等创建一维数列 `Tensor`\n",
    "- 用`ones`、`zeros`、`eye`、`full`、`empty`等来创建特别填充值的多维 `Tensor`\n",
    "- 用随机数来创建指定形状的 `Tensor`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "personal-energy",
   "metadata": {},
   "source": [
    "### 从现有数据来创建\n",
    "\n",
    "我们可以使用`torch.tensor()`函数来从已有的一个 array_like 的 data 来创建一个 Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "awful-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从list创建\n",
    "t1 = torch.tensor([1, 2, 3, 4, 5])\n",
    "# 从tuple创建\n",
    "t2 = torch.tensor((1, 2, 3))\n",
    "# 从numpy.array创建，同时指定dtype和device\n",
    "t3 = torch.tensor(np.array([1, 2, 3, 4, 5]), dtype=torch.float32, device=\"cuda:0\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "stable-peeing",
   "metadata": {},
   "source": [
    "需要注意的是，无论是从 `python` 的内置序列创建，还是从 `numpy.array` 来创建，创建出来的 `Tensor` 都是复制了原数据的内容。如果我们希望，创建的 `Tensor` 不额外分配存储空间，而是和之前的 `numpy.array` 共享存储，那么可以使用`as_tensor`方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "secret-geneva",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始的array:  [1 2 3 4 5]\n",
      "修改后的arr:  [6 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([1, 2, 3, 4, 5])\n",
    "print(f\"原始的array: \", arr)\n",
    "t = torch.as_tensor(arr)\n",
    "# 对于Tensor的数据改动，也会影响在ndarray上\n",
    "t[0] = 6\n",
    "print(f\"修改后的arr: \", arr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "established-mystery",
   "metadata": {},
   "source": [
    "不过使用`as_tensor`后，能共享底层存储的，前提是，`as_tensor` 方法中指定的`dtype`和`device`和原 `ndarry` 是一致的。由于 `numpy` 不支持 CUDA，所以这样只能创建 cpu 上的 Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fresh-extreme",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([1, 2, 3, 4, 5]) # 原 arr 的数值类型是 int64\n",
    "t = torch.as_tensor(arr, dtype=torch.float32)  # 这种情况下，并不会共享底层存储\n",
    "t[0] = 6\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4160ee27",
   "metadata": {},
   "source": [
    "当我们使用 List 来创建 Tensor 时，需要注意对于数据类型的转换处理：对于 int 类型，NDArray 和 Tensor 都会转换为 `int64`，对于浮点数 float，Tensor 默认转换为`float32`，而 NDArray 则默认转换为 `float64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-district",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndarray的默认整数类型为:int64\n",
      "tensor的默认整数类型为: torch.int64\n",
      "ndarray的默认整数类型为:float64\n",
      "tensor的默认整数类型为: torch.float32\n"
     ]
    }
   ],
   "source": [
    "il = [1, 2, 3, 4, 5]\n",
    "print(f\"ndarray的默认整数类型为:{np.array(il).dtype}\")\n",
    "print(f\"tensor的默认整数类型为: {torch.tensor(il).dtype}\")\n",
    "\n",
    "fl = [1.0, 2.0, 3.0, 4.0, 5.0] # List[float]\n",
    "print(f\"ndarray的默认整数类型为:{np.array(fl).dtype}\")\n",
    "print(f\"tensor的默认整数类型为: {torch.tensor(fl).dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19507a0e",
   "metadata": {},
   "source": [
    "从另外一个 Tensor 来创建 Tensor，无论 b 是否指新新的 `dtype` 和 `device`，b 都不和 a 共享数据，根据 Warning 提示，我们知道这种用法目标在 Pytorch 中是不推荐的，如果要复制 Tensor，可以直接用 `clone` 方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fece622f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4590/1755634830.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  b = torch.tensor(a, dtype=torch.float, device=\"cuda:0\")\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor(a, dtype=torch.float, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b1990fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "b = a.clone()\n",
    "b = b.to(device=\"cuda:0\")\n",
    "print(b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "unauthorized-today",
   "metadata": {},
   "source": [
    "### torch.Tensor()\n",
    "\n",
    "请注意 `torch.Tensor`和 `torch.tensor`的不同。 `torch.Tensor` 实际上是`torch.FloatTensor`，用它来创建新的Tensor时，实际调用的是构造函数，它会默认以`torch.float32`来作为`dtype`。而`torch.tensor`会根据`data`的类型自动推断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "right-andrews",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "l = [1, 2, 3, 4, 5]\n",
    "print(torch.Tensor(l).dtype)\n",
    "print(torch.tensor(l).dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "stainless-garden",
   "metadata": {},
   "source": [
    "### 创建特别填充值的Tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dramatic-driver",
   "metadata": {},
   "source": [
    "#### `torch.arange`\n",
    "\n",
    "`torch.arange(start=0, end, step=1)` 用于创建一个区间范围的 Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "perfect-alpha",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([1, 2, 3, 4])\n",
      "tensor([ 1,  4,  7, 10, 13, 16, 19])\n"
     ]
    }
   ],
   "source": [
    "print(torch.arange(5))\n",
    "print(torch.arange(1, 5))\n",
    "print(torch.arange(1, 20, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03b62d7",
   "metadata": {},
   "source": [
    "如果 `start`、`end` 以及 `step` 中有浮点数，则创建出来的是 `FloatTensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "crucial-mission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.5000, 2.0000, 2.5000, 3.0000])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1, 3.5, 0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "indie-excellence",
   "metadata": {},
   "source": [
    "注意上面是没有包括 3.5 那个点的"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "robust-general",
   "metadata": {},
   "source": [
    "#### `torch.linspace`\n",
    "\n",
    "`torch.linspace`与`torch.arange`有点类似，都指定一个起点，一个终点，和一个步长。但`linspace`里步长最终指定了生成的一维 Tensor 中元素的个数\n",
    "\n",
    "```python\n",
    "def linspace(start:float ,end:float ,steps:int) -> Tensor:\n",
    "    pass\n",
    "```\n",
    "\n",
    "另外需要注意的是`torch.linspace`生成的一定是一个浮点数的Tensor，而且和`torch.arange`不同的是：`linspace`生成的Tensor是包括末点值的（inclusive）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cognitive-origin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.0000,  4.7500,  6.5000,  8.2500, 10.0000])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(3, 10, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "enhanced-breach",
   "metadata": {},
   "source": [
    "#### `torch.logspace`\n",
    "\n",
    "`torch.logspace`和`torch.linspace`行为类似，区别在于`logspace`生成的序列的范围的起始与终点是一个以`base`为底，`start`和`end`为指数的数字。\n",
    "\n",
    "```python\n",
    "def logspace(start:float ,end:float ,steps:int, base=10.0) -> Tensor:\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "innovative-graduation",
   "metadata": {},
   "source": [
    "#### `torch.ones`、`torch.zeros`、`torch.emtpy`\n",
    "\n",
    "它们三个都是用于创建一个指定 `size` 的Tensor，分别以`1`、`0`和未初始化的值来填充创建好的 `Tensor`。它们三个返回的都是 `FloatTensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "invisible-lesbian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[2.5159e-33, 0.0000e+00, 0.0000e+00],\n",
      "        [1.4013e-45, 0.0000e+00, 0.0000e+00],\n",
      "        [4.6243e-44, 0.0000e+00, 2.5176e-33]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.ones((2, 2)))\n",
    "print(torch.zeros((3, 4)))\n",
    "print(torch.empty((3, 3)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54e6d4db",
   "metadata": {},
   "source": [
    "`torch.ones/zeros/empty`支持`torch.ones(d1,d2,...)`这种调用方法，而 `numpy` 则不支持。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "statewide-brooklyn",
   "metadata": {},
   "source": [
    "#### `torch.eye`\n",
    "\n",
    "`torch.eye` 返回的是一个 2D 的对角线为 `1`，其他值都为`0` 的 `FloatTensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fatal-happiness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "interim-following",
   "metadata": {},
   "source": [
    "#### `torch.full`\n",
    "\n",
    "`torch.full`返回的是一个指定`size`和填充值的Tensor，Tensor的 `dtype` 是由填充值的类型来推导的。\n",
    "\n",
    "```python\n",
    "def full(size, fill_value) -> Tensor:\n",
    "  '''\n",
    "  Args:\n",
    "    size(int...): a list ,tuple or torch.Size\n",
    "    fill_vale(Scalar)\n",
    "  '''\n",
    "  pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "technological-highway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full((2, 3), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db37d49a",
   "metadata": {},
   "source": [
    "#### `torch.diag`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218209a5",
   "metadata": {},
   "source": [
    "如果输入的是一个 `1d` 的 Tensor，则返回的是一个 `2d` 的对角矩阵，其对角线上的元素为传入的 `Tensor`，也可以通过`diagonal`来指定对角线元素的轴偏值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2cb6d99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 3]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(torch.tensor([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b9c9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [0, 2, 0, 0],\n",
       "        [0, 0, 3, 0]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(torch.tensor([1, 2, 3]), diagonal=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb37e666",
   "metadata": {},
   "source": [
    "传入 `2d` 的 `Tensor`，则返回 `Tensor` 的对角线上的元素，返回的是一个 `1d` 的 `Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77507e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 5, 9])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4fd32bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 6])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), diagonal=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "chinese-ranch",
   "metadata": {},
   "source": [
    "### 使用随机数来创建 `Tensor`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "resident-physiology",
   "metadata": {},
   "source": [
    "#### `torch.normal`\n",
    "\n",
    "`torch.normal`返回一个正态分布产生在的随机数填充的Tensor，它一共有4种参数传递方式\n",
    "\n",
    "* 第一种是: `torch.norm(mean, std)` 其中 `mean` 和 `std` 都是一个 Tensor，生成的 `Tensor` 的形状和 `mean` 和`std` 的形状是一致的，其中每个元素都是通过对应位置的 `mean` 和 `std` 形成的正态分布来随机产生的。\n",
    "* 第二种是: `normal(mean=0.0, std, *, out=None)`。这种参数传递用法，与上面的区别就是 `mean` 变成一个 `scalar`，那么说明每个元素来共享一个 `mean` 值。在这种情况下。\n",
    "* 第三种是: `normal(mean, std=1.0, *, out=None)`。这种情况和第 2 种情况，恰恰相反了，`std` 变成了每个元素共享的。\n",
    "* 第四种是: `normal(mean, std, size, *, out=None)`。这种情况下，所有的元素都共享 `mean` 和 `std`，最终 `Tensor` 的形状是由 `size` 来决定的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "super-louisville",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4458, -0.8187,  2.4465,  2.0837],\n",
       "        [ 1.6485,  0.4339, -0.8377, -0.0096],\n",
       "        [ 0.2849, -0.7824, -0.7216,  0.4097]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = torch.randn(3, 4)\n",
    "std = torch.rand((3, 4))\n",
    "torch.normal(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "olive-third",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0569,  0.8937,  1.3969,  1.0340],\n",
       "        [ 0.7369,  1.0320, -0.1283,  1.2739],\n",
       "        [ 0.8920,  0.8142, -0.0419,  2.0956]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(1.0, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ambient-shame",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5882, -0.3526,  0.6500,  1.8875],\n",
       "        [ 0.9137,  0.3099, -0.3028,  0.6313],\n",
       "        [-0.0521, -0.6022, -0.5374, -0.8275]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(mean, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "statewide-defeat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2031, -2.4626,  0.4687,  0.1620],\n",
       "        [-1.9686,  0.6622,  1.1888,  0.3248],\n",
       "        [-0.0875,  1.9568, -0.1608, -2.3748]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(0, 1, (3, 4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "expensive-ground",
   "metadata": {},
   "source": [
    "#### `torch.rand`、`torch.randn`\n",
    "\n",
    "* `rand`直接生成指定形状的 `Tensor`，其中每个元素都是由`[0,1)`均匀分布来随机产生。\n",
    "* `randn`直接生成指定形状的 `Tensor`，其中每个元素都是由标准正态分布来随机产生。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "difficult-exclusive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0958, 0.9279, 0.8892, 0.1625],\n",
       "        [0.6920, 0.3723, 0.9931, 0.8634],\n",
       "        [0.4582, 0.4525, 0.1676, 0.4868]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(3, 4)  # 或者 torch.randn((3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "resident-austin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5794,  0.4422,  1.1399, -0.0520],\n",
       "        [-0.8622, -0.9455, -1.0510, -0.3753],\n",
       "        [-0.6126,  1.3127,  1.1930,  1.3659]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 4)  # 或者 torch.randn((3,4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "regional-afternoon",
   "metadata": {},
   "source": [
    "#### `torch.randint`\n",
    "\n",
    "```python\n",
    "randint(low=0, high, size, **kwargs)\n",
    "```\n",
    "产生一个由 `[low,high)` 区间均匀分布随机数填充的 `LongTensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "large-vaccine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 8, 3, 7],\n",
       "        [1, 1, 1, 3],\n",
       "        [2, 7, 1, 6]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(1, 10, (3, 4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "danish-prime",
   "metadata": {},
   "source": [
    "#### `torch.randperm`\n",
    "\n",
    "生成一个随机全排列的一维的 `LongTensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "strange-biography",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3,  1, 10,  9, 11,  6,  7,  0,  8,  5,  2,  4])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randperm(12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "directed-simon",
   "metadata": {},
   "source": [
    "### 使用`xx_like`系列创建相同形态的Tensor\n",
    "\n",
    "除了shape保持一致外，`dtype`、`layout`、`device`等，若无特别指定，则也与源Tensor保持一致。\n",
    "\n",
    "```python\n",
    "torch.zeros_like(input, **kwargs) # 返回与input相同size的零矩阵\n",
    "torch.ones_like(input, **kwargs) #返回与input相同size的单位矩阵\n",
    "torch.full_like(input, fill_value, **kwargs) #返回与input相同size，单位值为fill_value的矩阵\n",
    "torch.empty_like(input, **kwargs) # 返回与input相同size,并被未初始化的数值填充的tensor\n",
    "torch.rand_like(input, dtype=None, **kwargs) #返回与input相同size的tensor, 填充均匀分布的随机数值\n",
    "torch.randint_like(input, low=0, high, dtype=None, **kwargs) #返回与input相同size的tensor, 填充[low, high)均匀分布的随机数值\n",
    "torch.randn_like(input, dtype=None, **kwargs) #返回与input相同size的tensor, 填充标准正态分布的随机数值\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "spatial-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.randn(4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "forty-membrane",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros_like(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "overall-dragon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(src, dtype=torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "backed-budapest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty_like(src, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "pointed-williams",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42., 42., 42., 42., 42.],\n",
       "        [42., 42., 42., 42., 42.],\n",
       "        [42., 42., 42., 42., 42.],\n",
       "        [42., 42., 42., 42., 42.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里即使full_value是int类型，但生成的Tensor，依然是用的src的dtype\n",
    "torch.full_like(src, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "noticed-spanking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5867, 0.2891, 0.5580, 0.1204, 0.0054],\n",
       "        [0.1634, 0.9599, 0.0095, 0.0271, 0.5853],\n",
       "        [0.5226, 0.4830, 0.8212, 0.2810, 0.7092],\n",
       "        [0.5244, 0.4347, 0.5706, 0.8941, 0.2371]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand_like(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "norwegian-notice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3273, -1.6254,  0.2422, -0.0030, -1.1903],\n",
       "        [-0.9231,  1.0337,  0.6556,  0.5820,  0.3665],\n",
       "        [-0.0355,  1.1695,  1.5428,  0.4138,  0.6371],\n",
       "        [ 0.4081, -1.8035, -0.9763, -0.6170,  0.4965]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn_like(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "committed-soundtrack",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8., 9., 2., 9., 3.],\n",
       "        [8., 8., 3., 3., 9.],\n",
       "        [3., 1., 4., 3., 9.],\n",
       "        [8., 6., 8., 7., 6.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint_like(src, 1, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "amateur-dream",
   "metadata": {},
   "source": [
    "## Tensor的操作\n",
    "\n",
    "Pytorch中的Tensor大约支持100种以上的操作，其中包括了数学运算、线性代数、矩阵操作（转置、索引、切片等），这些操作都可以跑在CPU或GPU上，这也是 Pytorch Tensor的强大之处。\n",
    "\n",
    "![](../images/tensor_operatrions.png)\n",
    "\n",
    "我们可以通过这个[页面](https://pytorch.org/docs/stable/torch.html)，来对Tensor支持的所有操作做个大概的了解。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "derived-holocaust",
   "metadata": {},
   "source": [
    "### 索引访值\n",
    "\n",
    "#### 基础索引\n",
    "\n",
    "我们可以像访问Numpy.ndarray一样，对torch.Tensor进行各种下标索引与范围切片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "canadian-insulin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "取t的第2行的所有元素: tensor([4, 5, 6, 7])\n",
      "取t的最后一列的所有元素: tensor([ 3,  7, 11])\n",
      "取t的第2列到最后一列的所有元素: tensor([[ 2,  3],\n",
      "        [ 6,  7],\n",
      "        [10, 11]])\n",
      "取t的位置(2,3)上的元素: 11\n"
     ]
    }
   ],
   "source": [
    "t = torch.arange(12).reshape(3, 4)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"取t的第2行的所有元素: {t[1]}\")\n",
    "print(f\"取t的最后一列的所有元素: {t[:, -1]}\")\n",
    "print(f\"取t的第2列到最后一列的所有元素: {t[:, 2:]}\")\n",
    "print(f\"取t的位置(2,3)上的元素: {t[2, 3]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "necessary-adoption",
   "metadata": {},
   "source": [
    "**单一元素的Tensor**\n",
    "\n",
    "当我们通过索引访问Tensor的单一元素时，得到的实际是一个`Tensor`类型的对象，它并不是python中的内置数据类型，我们可以通过Tensor的`item()`方法来获取python对象的标量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "sound-pattern",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t[2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "95e58023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[2, 3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "chief-thermal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t[2, 3].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a3d026",
   "metadata": {},
   "source": [
    "注意对如果某个维度上我们只取一行/列数据，那么有两种方式，这两种方式得到的结果的 Shape 会不一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f9e4eddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2],\n",
      "        [ 6],\n",
      "        [10]])\n",
      "tensor([ 2,  6, 10])\n"
     ]
    }
   ],
   "source": [
    "t1 = t[:, 2:3]\n",
    "t2 = t[:, 2]\n",
    "print(t1)  # 还是一个二维的Tensor\n",
    "print(t2)  # 一维的 Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503e5459",
   "metadata": {},
   "source": [
    "#### 高级索引\n",
    "\n",
    "Tensor 的高级索引，支持我们直接用一个 Long 型的 Tensor 作为索引来取原 Tensor 中的元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0e2d8bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 10])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(8, 10)\n",
    "# indices 的所有元素都代表 t 的 dim=0 的下标\n",
    "indices = torch.randint(0, 8, (3, 2))\n",
    "t[indices].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0858ac95",
   "metadata": {},
   "source": [
    "#### torch.gather\n",
    "\n",
    "torch.gather 往往用于我们希望依次在输入 Tensor 的某个维度上取出其中一些索引的值。比如下面的 topK 的结果中，我们希望根据返回的 indices 取得对应的元素，也就是 values，这里可以用 gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1eac5e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.2757, -0.3290,  0.7861,  1.6445, -0.3443],\n",
      "        [-1.5337,  0.8690,  2.1442,  1.0928, -0.7280],\n",
      "        [ 0.4658, -0.8848,  1.4970,  0.2225, -0.6478]])\n",
      "tensor([[1.6445, 0.7861],\n",
      "        [2.1442, 1.0928],\n",
      "        [1.4970, 0.4658]])\n",
      "tensor([[3, 2],\n",
      "        [2, 3],\n",
      "        [2, 0]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn(3, 5)\n",
    "values, indices = torch.topk(t, k=2, dim=1)\n",
    "print(t)\n",
    "print(values)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ebd0d2bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True],\n",
       "        [True, True],\n",
       "        [True, True]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values == torch.gather(t, dim=1, index=indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "through-marketplace",
   "metadata": {},
   "source": [
    "### 组合与分片\n",
    "\n",
    "#### torch.cat\n",
    "\n",
    "```\n",
    "cat(tensors, dim=0) -> Tensor\n",
    "```\n",
    "\n",
    "`torch.cat`将给定义的tensor的序列(tensors)，按给定义的维度上合并起来，这就要求，这些tensor，除了合并的维度，其他的维度必须一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "every-parade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8229, -0.4819, -0.0436],\n",
       "        [ 1.1926,  0.3573,  0.6702],\n",
       "        [-1.3167,  0.1392, -0.3058],\n",
       "        [ 1.3075,  0.8602, -0.2774],\n",
       "        [-0.4457,  1.1762, -0.4505]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.randn(2, 3)\n",
    "t2 = torch.randn(3, 3)\n",
    "torch.cat([t1, t2], dim=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cardiac-luther",
   "metadata": {},
   "source": [
    "#### torch.stack\n",
    "\n",
    "`torch.stack`和`torch.cat`接口用法一致，但它并不是在原有的维度上拼接，而是直接扩展一个新的维度。\n",
    "\n",
    "这就要求，序列中的tensor在维度上必须一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "peripheral-membrane",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1507, -0.6556, -0.5804],\n",
       "         [-0.9632, -0.4136, -0.6468]],\n",
       "\n",
       "        [[ 0.0827, -0.3218,  0.6109],\n",
       "         [ 1.9358, -0.6337,  0.6983]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.randn(2, 3)\n",
    "t2 = torch.randn(2, 3)\n",
    "torch.stack([t1, t2], dim=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "complimentary-ordinance",
   "metadata": {},
   "source": [
    "#### torch.split\n",
    "\n",
    "```python\n",
    "split(tensor, split_size_or_sections, dim=0)\n",
    "```\n",
    "\n",
    "`split`将tensor按指定的维度，分拆为多个Tensor的元组，拆分的块chunk的大小是splite_size指定的。可能出现不能整分的情况，这时候最后一块大小一般小于splite_size\n",
    "\n",
    "split出来的Tensor是原tensor的一个view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "identical-discipline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5],\n",
       "        [6, 7],\n",
       "        [8, 9]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(10).view(5, 2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "subject-progressive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1],\n",
       "         [2, 3]]),\n",
       " tensor([[4, 5],\n",
       "         [6, 7]]),\n",
       " tensor([[8, 9]]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(a, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "narrative-hearts",
   "metadata": {},
   "source": [
    "`split_size_or_sections`也可能是一个list(int)，这时候，它的每个元素，代表每个chunk的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "starting-algorithm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1]]),\n",
       " tensor([[2, 3],\n",
       "         [4, 5],\n",
       "         [6, 7]]),\n",
       " tensor([[8, 9]]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1, a2, a3 = torch.split(a, (1, 3, 1))\n",
    "a1, a2, a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "specified-messaging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42,  1],\n",
       "        [ 2,  3],\n",
       "        [ 4,  5],\n",
       "        [ 6,  7],\n",
       "        [ 8,  9]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 切分出来的tensor和原tensor是共享存储的\n",
    "a1[0, 0] = 42\n",
    "a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "confidential-charger",
   "metadata": {},
   "source": [
    "#### torch.chunk\n",
    "\n",
    "```python\n",
    "chunk(input, chunks, dim=0) -> List of Tensors\n",
    "```\n",
    "`chunk`和`split`功能类似，不同在于，chunk的第二的参数，直接指定的是chunk的数量，最后一个chunk的数量可能会少一些。也有可能`axis[dim]<chunks`，那么就直接切分为`axis[dim]`个。\n",
    "\n",
    "切分出来的这些Tensor和原Tensor都是共享底层存储的，也就是说每个chunk都是原Tensor的一个view。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ba0a82b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(a.shape)\n",
    "len(a.chunk(3, dim=1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "general-sitting",
   "metadata": {},
   "source": [
    "### 变换操作\n",
    "\n",
    "#### torch.reshape\n",
    "\n",
    "```python\n",
    "reshape(input, shape) -> Tensor\n",
    "```\n",
    "`reshape`返回一个和原Tensor具有相同数据，相同数量的Tensor，只是shape不一致。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "gross-botswana",
   "metadata": {},
   "source": [
    "#### torch.view\n",
    "\n",
    "torch.view vs. torch.reshape\n",
    "\n",
    "`reshape`可以用在`compact`或`non-compact`的tensor上，而`view`只能用在`compact`的tensor上。`reshape`如果作用于`non-compact`的tensor上，则会产生一个copy\n",
    "\n",
    "torch.view has existed for a long time. It will return a tensor with the new shape. The returned tensor will share the underling data with the original tensor. See the documentation here.\n",
    "\n",
    "On the other hand, it seems that torch.reshape has been introduced recently in version 0.4. According to the document, this method will\n",
    "\n",
    "> Returns a tensor with the same data and number of elements as input, but with the specified shape. When possible, the returned tensor will be a view of input. Otherwise, it will be a copy. Contiguous inputs and inputs with compatible strides can be reshaped without copying, but you should not depend on the copying vs. viewing behavior.\n",
    "\n",
    "It means that torch.reshape may return a copy or a view of the original tensor. You can not count on that to return a view or a copy. According to the developer:\n",
    "\n",
    "> if you need a copy use clone() if you need the same storage use view(). The semantics of reshape() are that it may or may not share the storage and you don't know beforehand.\n",
    "\n",
    "Another difference is that reshape() can operate on both contiguous and non-contiguous tensor while view() can only operate on contiguous tensor. Also see here about the meaning of contiguous."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "herbal-opening",
   "metadata": {},
   "source": [
    "#### torch.transpose\n",
    "\n",
    "```python\n",
    "transpose(input, dim0, dim1) -> Tensor\n",
    "```\n",
    "转置input的指定的2个维度，返回的Tensor和原来的Tensor共享存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "toxic-delay",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5260, 0.9209, 0.5547, 0.3062],\n",
       "         [0.3510, 0.7916, 0.8831, 0.1895],\n",
       "         [0.6874, 0.8094, 0.3321, 0.4644]],\n",
       "\n",
       "        [[0.9875, 0.1983, 0.8904, 0.6557],\n",
       "         [0.8619, 0.5538, 0.3221, 0.7695],\n",
       "         [0.5407, 0.2839, 0.0598, 0.8637]]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 3, 4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "boxed-jaguar",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.transpose(x, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c5d32c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 4, 1)\n",
      "(1, 4, 12)\n"
     ]
    }
   ],
   "source": [
    "print(x.stride())\n",
    "print(y.stride())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "built-beach",
   "metadata": {},
   "source": [
    "#### torch.permute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c2b403c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 3])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.permute(x, (2, 0, 1)).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93080d1e",
   "metadata": {},
   "source": [
    "#### squeeze和unsqueeze\n",
    "\n",
    "squeeze在指定的维度上添加一维，而unsqueeze则在指定的维度上去掉`size=1`的维度，如果对应维度上的size不等于1，则不做任何操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7c8896e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([2, 3]), \n",
      "y (x.unsqueeze) shape: torch.Size([1, 2, 1, 3])\n",
      "unsqueeze shape torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "y = x.unsqueeze(dim=1).unsqueeze(dim=0)\n",
    "print(f\"x shape: {x.shape}, \\ny (x.unsqueeze) shape: {y.shape}\")\n",
    "print(\"unsqueeze shape\", y.squeeze(dim=0).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "clinical-phoenix",
   "metadata": {},
   "source": [
    "#### contiguous\n",
    "\n",
    "There are a few operations on Tensors in PyTorch that do not change the contents of a tensor, but change the way the data is organized. These operations include:\n",
    "\n",
    "`narrow()`, `view()`, `expand()` and `transpose()`\n",
    "\n",
    "For example: when you call transpose(), PyTorch doesn't generate a new tensor with a new layout, it just modifies meta information in the Tensor object so that the offset and stride describe the desired new shape. In this example, the transposed tensor and original tensor share the same memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "practical-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(42.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 2)\n",
    "y = torch.transpose(x, 0, 1)\n",
    "x[0, 0] = 42\n",
    "print(y[0, 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75c7c0ae",
   "metadata": {},
   "source": [
    "This is where the concept of contiguous comes in. In the example above, x is contiguous but y is not because its memory layout is different to that of a tensor of same shape made from scratch. Note that the word \"contiguous\" is a bit misleading because it's not that the content of the tensor is spread out around disconnected blocks of memory. Here bytes are still allocated in one block of memory but the order of the elements is different!\n",
    "\n",
    "When you call contiguous(), it actually makes a copy of the tensor such that the order of its elements in memory is the same as if it had been created from scratch with the same data.\n",
    "\n",
    "Normally you don't need to worry about this. You're generally safe to assume everything will work, and wait until you get a RuntimeError: input is not contiguous where PyTorch expects a contiguous tensor to add a call to contiguous()."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "interior-morrison",
   "metadata": {},
   "source": [
    "### 降维操作"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "hundred-syria",
   "metadata": {},
   "source": [
    "#### torch.mean\n",
    "\n",
    "```python\n",
    "'''\n",
    "Args:\n",
    "  input (Tensor): the input tensor.\n",
    "  dim (int or tuple of ints): the dimension or dimensions to reduce.\n",
    "  keepdim (bool): whether the output tensor has :attr:`dim` retained or not.\n",
    "'''\n",
    "mean(input, dim, keepdim=False, *, out=None) -> Tensor\n",
    "```\n",
    "\n",
    "对input沿着`dim`的维度求均值，这样的话，指定的那个维度就会被压缩掉，如果指定了`keepdim=True`的话，那个维度会保留，值为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "centered-murray",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2039,  1.2231, -0.3430,  0.3751, -0.7528, -1.1104],\n",
       "        [ 0.4561, -0.9779, -0.3400,  0.6377,  0.8848, -1.5154],\n",
       "        [ 1.4089, -1.1560, -0.8236, -0.5375,  1.2653, -0.3280],\n",
       "        [ 0.3497,  0.5302, -1.1714, -0.9095, -1.1539, -0.7436],\n",
       "        [-0.0800, -0.4530,  0.6272,  0.7534, -0.2480,  1.8906]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(5, 6)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "demographic-country",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3862, -0.1667, -0.4102,  0.0638, -0.0009, -0.3613])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 按列的方向(dim=0)将整个Tenoor压缩成为1维的\n",
    "torch.mean(t, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "important-politics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1353],\n",
       "        [-0.1424],\n",
       "        [-0.0285],\n",
       "        [-0.5164],\n",
       "        [ 0.4151]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(t, dim=1, keepdim=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "appointed-october",
   "metadata": {},
   "source": [
    "对于高维Tensor，我们还可以同时对多个维度进行Reduce，求其均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "statistical-tonight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8714, -0.5348, -0.1684, -0.7420],\n",
       "         [ 1.6161,  0.9631,  1.7041, -0.0590],\n",
       "         [-0.9798,  0.8068,  0.3488, -0.2003]],\n",
       "\n",
       "        [[ 0.4771,  1.4805,  0.2393, -0.7670],\n",
       "         [ 1.2054, -0.4611, -0.3424, -0.6664],\n",
       "         [ 1.7911, -0.9301, -0.4897, -1.6210]]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(2, 3, 4)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "written-setup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1070,  0.4950, -0.1593])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 等价于reduce第0维，得到一个3x4的Tensor后，再reduce第1维，得到(3,)的Vector\n",
    "torch.mean(t, dim=(0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "million-drive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1070,  0.4950, -0.1593])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean(0).mean(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "secondary-explorer",
   "metadata": {},
   "source": [
    "#### torch.sum\n",
    "\n",
    "`torch.sum`是一个和`torch.mean`用法上很像的操作，只是`sum`的reduce op变成了求和，而不是求均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "separated-illustration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8562,  3.9598, -1.2741])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(t, dim=(0, 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31707904",
   "metadata": {},
   "source": [
    "#### torch.argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a96184a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[0.3633, 0.4314, 0.5305],\n",
      "        [0.6689, 0.1378, 0.3848]])\n",
      "Argmax: tensor([2, 0])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((2, 3))\n",
    "print(\"x:\", x)\n",
    "print(\"Argmax:\", x.argmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fbd95c",
   "metadata": {},
   "source": [
    "#### torch.maxmimu\n",
    "\n",
    "相同 Shape 的 Tensor 和 Tensor 按元素逐个比大小，保留最大的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fe8a0a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "\ttensor([[-0.1348, -0.6546, -0.5181],\n",
      "        [ 0.2910,  0.7669, -1.1600]]) \n",
      "relu(x):\n",
      "\ttensor([[0.0000, 0.0000, 0.0000],\n",
      "        [0.2910, 0.7669, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "def relu(x):\n",
    "    return torch.maximum(x, torch.tensor(0))\n",
    "\n",
    "\n",
    "x = torch.randn((2, 3))\n",
    "print(f\"x:\\n\\t{x} \\nrelu(x):\\n\\t{relu(x)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec47a468",
   "metadata": {},
   "source": [
    "### 排序"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "super-still",
   "metadata": {},
   "source": [
    "#### torch.sort\n",
    "\n",
    "```python\n",
    "sort(input, dim=-1, descending=False, *, out=None) -> (Tensor, LongTensor)\n",
    "```\n",
    "`sort`对input按给定义的dim进行升序排列，返回排列后的Tensor的同时，也返回一个对应的下标的重排后的Tensor\n",
    "\n",
    "dim的默认值是Tensor的最后一维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "potential-morrison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5611, 0.0464, 0.8813],\n",
      "        [0.9126, 0.0398, 0.2651]])\n",
      "tensor([[0.8813, 0.5611, 0.0464],\n",
      "        [0.9126, 0.2651, 0.0398]])\n",
      "tensor([[2, 0, 1],\n",
      "        [0, 2, 1]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "print(a)\n",
    "values, indices = torch.sort(a, dim=1, descending=True)\n",
    "print(values)\n",
    "print(indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "martial-petersburg",
   "metadata": {},
   "source": [
    "#### torch.topk\n",
    "\n",
    "```python\n",
    "topk(input, k, dim=None, largest=True, sorted=True, *, out=None) -> (Tensor, LongTensor)\n",
    "```\n",
    "`topk`返回input中指定维度上，最大的k个元素，以及对应的索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "pending-nickname",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3229, -0.3369, -0.2835,  0.0023, -1.6770])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "practical-review",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.3229,  0.0023, -0.2835])\n",
      "tensor([0, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "values, indices = torch.topk(a, 3)\n",
    "print(values)\n",
    "print(indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "sonic-groove",
   "metadata": {},
   "source": [
    "#### torch.kthvalue\n",
    "\n",
    "```python\n",
    "kthvalue(input, k, dim=None, keepdim=False, *, out=None) -> (Tensor, LongTensor)\n",
    "```\n",
    "`kthvalue`计算输出Tensor的指定维度上第`k`小的元素以及下标。如果dim没有指定，则默认为Tensor的最后一维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "august-chamber",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1169, -1.1037, -0.8417],\n",
       "        [ 0.0679, -1.0995,  0.8263],\n",
       "        [-0.0570,  1.6181, -0.2661],\n",
       "        [-0.9555, -0.3374, -0.2083]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cardiovascular-ticket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.kthvalue(\n",
       "values=tensor([-0.0570, -1.0995, -0.2661]),\n",
       "indices=tensor([2, 1, 2]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.kthvalue(a, 2, dim=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acknowledged-invitation",
   "metadata": {},
   "source": [
    "### 原地操作(in-place)\n",
    "\n",
    "pytorch的Tensor支持了很多原地操作，它们的特点就是在方法末尾以`_`结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "convinced-graphics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 = tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "after plus 2: t1 = tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.ones(2, 3)\n",
    "print(f\"t1 = {t1}\")\n",
    "t1.add_(2)\n",
    "print(f\"after plus 2: t1 = {t1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "electric-austria",
   "metadata": {},
   "source": [
    "### 转换为其他数据类型\n",
    "\n",
    "我们可以调用`numpy`接口,返回一个numpy.ndarray的对象，可以调用`tolist`接口，返回一个list的对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "clean-nashville",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "multiple-adapter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回的ndarray还是和t是共享存储的\n",
    "t.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "unlimited-patio",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 5, 6]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape(2, 3).tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "207b647a",
   "metadata": {},
   "source": [
    "### repeat和repeat_interleave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b70e577b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(6).reshape((2, 3))\n",
    "a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cefff4c1",
   "metadata": {},
   "source": [
    "`repeat(d0, d1, d2)` 将对应的维度复制多份，如果之前没有对应的维度，则可以当作原来维度为1，处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2432d308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 2, 0, 1, 2],\n",
       "         [3, 4, 5, 3, 4, 5]],\n",
       "\n",
       "        [[0, 1, 2, 0, 1, 2],\n",
       "         [3, 4, 5, 3, 4, 5]]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.repeat((2, 1, 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73f6fc4c",
   "metadata": {},
   "source": [
    "`repeat_interleave(n, dim)` 在对应的维度上进行复制，但复制的方式不是`[a b c a b c ]`这种，而是`[a a b b c c]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4a924af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.repeat_interleave(2, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a3a4f5",
   "metadata": {},
   "source": [
    "## 爱因斯坦标识"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "683f0574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, reduce, repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c088bd39",
   "metadata": {},
   "source": [
    "### 实现 Transpose 和 Permute 的功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f745fab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 8, 8)\n",
    "\n",
    "# Transose\n",
    "torch.allclose(rearrange(x, \"b c h w->b h w c\"), x.permute((0, 2, 3, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9dd32c",
   "metadata": {},
   "source": [
    "### 一步实现 Transpose + Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ca839bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一步完成 Transpose + Reshape\n",
    "torch.allclose(\n",
    "    rearrange(x, \"b c h w -> (b h w) c\"), x.permute(0, 2, 3, 1).reshape(-1, x.size(1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7690a5",
   "metadata": {},
   "source": [
    "### 实现维度拆分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "58e2386c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 64)\n",
    "\n",
    "torch.allclose(rearrange(x, \"b c (h w) -> b c h w\", h=8), x.reshape(2, 3, 8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e92d7f",
   "metadata": {},
   "source": [
    "### 实现 Image2Patch 的功能\n",
    "\n",
    "将 二维图像转换为 $B\\times N \\times D$ 的序列 Patches 的形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fde29237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1024, 192])\n"
     ]
    }
   ],
   "source": [
    "image = torch.randn(2, 3, 256, 256)\n",
    "\n",
    "patches = rearrange(image, \"b c (h1 ph) (w1 pw) -> b (h1 w1) (ph pw c)\", ph=8, pw=8)\n",
    "print(patches.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84091f14",
   "metadata": {},
   "source": [
    "### Reduce 操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c41a5946",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(8, 10)\n",
    "\n",
    "# mean\n",
    "x_mean = reduce(x, \"b d -> b\", reduction=\"mean\")\n",
    "# sum\n",
    "x_sum = reduce(x, \"b d -> 1 d\", reduction=\"sum\")\n",
    "\n",
    "x = torch.randn(2, 3, 4)\n",
    "x_max = reduce(x, \"b n d -> d\", reduction=\"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48792104",
   "metadata": {},
   "source": [
    "### 扩维与复制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d0a07612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 5])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(5, 5)\n",
    "rearrange(x, \"i j -> 1 i j\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e313f184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 25])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 5, 5)\n",
    "repeat(x, \"1 i j -> 3 i (5 j)\").shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c37a17f5960fe44e0e933f0a97f50106b021053c3b378a6f2025e9a4390c58c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
