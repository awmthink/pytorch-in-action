{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64b3b5af",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "\n",
    "Tensor 是 Pytorch 中最核心的数据结构，在 Pytorch 中几乎所有的形式的数据都在用 Tensor表示，模型的输入与输出格式都是 Tensor。它是一种多维数组结构，类似于 NumPy 的 `ndarray`，但具有更高的灵活性和 GPU 加速的能力。Tensor 的特点和操作在计算图（Computational Graph）中可以高效地支持梯度计算，适合自动微分（Autograd），因此在构建和训练神经网络时被广泛使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "placed-poison",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "serial-compromise",
   "metadata": {},
   "source": [
    "## Tensor的属性"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c14402d",
   "metadata": {},
   "source": [
    "### Tensor 的数据类型\n",
    "\n",
    "torch 在创建 Tensor 时，`dtype`的指定只支持使用`torch.[DataType]`这样的方式去指定，而不能像numpy一样，可以直接使用字符串。\n",
    "\n",
    "以下显示了从 `List` 创建一个新的 Tensor 时，同时指定了创建的 Tensor 的数值类型为 `int`，如果不显示的指定，那么默认是 `torch.int64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28fefdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype of t1: torch.int32\n",
      "dtype of t2: torch.int64\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([1, 2, 3], dtype=torch.int)\n",
    "t2 = torch.tensor([1, 2, 3])\n",
    "print(f\"dtype of t1: {t1.dtype}\")\n",
    "print(f\"dtype of t2: {t2.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2434a369",
   "metadata": {},
   "source": [
    "区别于 Torch，在 Numpy 中 \"int\" 默认代表的是 `int64`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d363752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype of t: int64\n"
     ]
    }
   ],
   "source": [
    "t = np.array([1, 2, 3], dtype=\"int\")\n",
    "print(f\"dtype of t: {t.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b55a947",
   "metadata": {},
   "source": [
    "其中`torch.dtype`表示Tensor的数据类型，常见的有下面几种不同的数据类型，和 c/c++ 中的表示是比较一致的。\n",
    "\n",
    "浮点类型：\n",
    "\n",
    "- `torch.float32`或`torch.float`\n",
    "- `torch.float64`或`torch.double`\n",
    "- `torch.float16`或`torch.half`\n",
    "- `torch.bfloat16`\n",
    "\n",
    "整数类型\n",
    "\n",
    "- `torch.uint8`\n",
    "- `torch.int8`\n",
    "- `torch.int16`\n",
    "- `torch.int32`\n",
    "- `torch.int64`\n",
    "\n",
    "布尔类型\n",
    "\n",
    "- `torch.bool`\n",
    "\n",
    "虚数类型\n",
    "\n",
    "\n",
    "- `torch.complex32`: complex32 目前只是实验性的支持，后续可能会取消支持\n",
    "- `torch.complex64`\n",
    "- `torch.complex128`或`torch.cdouble`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b7f425",
   "metadata": {},
   "source": [
    "虚数类型的 Tensor 在深度学习中使用的比较少，最近随机大语言模型的兴起，在相对旋转位置编码（RoPE）中有所使用。下面的代码演示了虚数 Tensor 的创建。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "888128cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6260+0.2726j,  1.2438+0.0740j,  0.4589+0.9147j],\n",
      "        [ 0.0079+0.6308j, -0.6461+0.2834j, -2.4735-0.7730j],\n",
      "        [ 0.5586-0.5617j, -1.3726+1.1622j,  0.4843+1.2107j]])\n",
      "torch.complex64\n",
      "torch.complex128\n"
     ]
    }
   ],
   "source": [
    "# 创建一个 Complex64 张量\n",
    "real_part = torch.randn(3, 3)  # 实部\n",
    "imag_part = torch.randn(3, 3)  # 虚部\n",
    "complex_tensor = torch.complex(\n",
    "    real_part, imag_part\n",
    ")  # 构建出来的 Tensor 默认是 complex64 类型\n",
    "\n",
    "print(complex_tensor)\n",
    "print(complex_tensor.dtype)  # 输出: torch.complex64\n",
    "complex_tensor = complex_tensor.to(dtype=torch.cdouble)\n",
    "print(complex_tensor.dtype)  # 输出: torch.complex128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "indian-vehicle",
   "metadata": {},
   "source": [
    "[`bfloat16`](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format)是一种和IEEE half-precision 16-bit float规定不一致的16Bit浮点数格式，它是直接对32位的IEEE 754规定的单精度float32的格式进行截取形成的，它是为机器学习系统特别定制的，它的组成是：\n",
    "- 1位符号位\n",
    "- 8个指数位\n",
    "- 7个小数位\n",
    "\n",
    "神经网络对指数的大小比对尾数的大小更敏感，也就是对数值的表示范围比较敏感。为了确保下溢、上溢和`NaN`的行为完全相同，`bfloat16`的指数大小与`float32`相同。`bfloat16`处理非规格化数的规则与`float32`不同，它会将它们截断为零。与通常需要特殊处理，如损失缩放的`float16`不同，`bfloat16`是训练和运行深度神经网络时`float32`的即插即用替换。\n",
    "\n",
    "简而言之，`bfloat16`表示的数值范围更大，但是精度不如`float16`。\n",
    "\n",
    "<div class=\"wy-nav-content-img\">\n",
    "    <img src=\"assets/Tensors_floating_point_formats.png\" width=\"800px\" alt=\"34 层的ResNet架构与 VGG19 以及线性连接的结构之间的对比\">\n",
    "    <p>图1: 三种浮点数格式的表示与区别</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6105f4",
   "metadata": {},
   "source": [
    "另外，在 Torch 2.x 的版本中也开始实验性的支持`FP8`格式，目前支持两种类型的`FP8`格式，一种是`E3M4`一种是`E5M2`，其中`E5M2`和 IEEE 的 `fp16`的指数部分长度一致，这使得 E5M2 和 IEEE FP16 格式之间可以直接转换。`E5M2`它表达的数值范围更大，可以处理一些特殊值。\n",
    "\n",
    "一般来说，我们会在模型推理以及训练过程的前向阶段为了保持较高的精度会使用`E3M4`的格式，而在训练的反向阶段，使用`E5M2`来有较好的数值动态范围，避免梯度消失与爆炸。\n",
    "\n",
    "Paper: [FP8 Formats for Deep Learning](https://arxiv.org/abs/2209.05433)\n",
    "\n",
    "<div class=\"wy-nav-content-img\">\n",
    "    <img src=\"assets/Tensors_FP8_format.png\" width=\"600px\" alt=\"二种半精度浮点数与二种 FP8 精度在存储格式上的区别\">\n",
    "    <p>图2: 二种半精度浮点数与二种 FP8 精度在存储格式上的区别</p>\n",
    "</div>\n",
    "\n",
    "上图展示了使用不同数值类型来表示“0.3952” 时，实际能够近似到的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57d45c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type        bits              max               min    smallest normal          eps\n",
      "-------------  ------  ---------------  ----------------  -----------------  -----------\n",
      "float32            32      3.40282e+38      -3.40282e+38        1.17549e-38  1.19209e-07\n",
      "float16            16  65504            -65504                  6.10352e-05  0.000976562\n",
      "bfloat16           16      3.38953e+38      -3.38953e+38        1.17549e-38  0.0078125\n",
      "float8_e4m3fn       8    448              -448                  0.015625     0.125\n",
      "float8_e5m2         8  57344            -57344                  6.10352e-05  0.25\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tabulate import tabulate\n",
    "\n",
    "f32_type = torch.float32\n",
    "f16_type = torch.float16\n",
    "bf16_type = torch.bfloat16\n",
    "e4m3_type = torch.float8_e4m3fn\n",
    "e5m2_type = torch.float8_e5m2\n",
    "\n",
    "# collect finfo for each type\n",
    "table = []\n",
    "for dtype in [f32_type, f16_type, bf16_type, e4m3_type, e5m2_type]:\n",
    "    numbits = (\n",
    "        32\n",
    "        if dtype == f32_type\n",
    "        else 16 if dtype == bf16_type or dtype == f16_type else 8\n",
    "    )\n",
    "    info = torch.finfo(dtype)\n",
    "    table.append(\n",
    "        [info.dtype, numbits, info.max, info.min, info.smallest_normal, info.eps]\n",
    "    )\n",
    "\n",
    "headers = [\"data type\", \"bits\", \"max\", \"min\", \"smallest normal\", \"eps\"]\n",
    "print(tabulate(table, headers=headers))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "working-commerce",
   "metadata": {},
   "source": [
    "### Tensor 的设备类型\n",
    "\n",
    "`torch.device`表示的是Tensor的数据存储的设备，其中分为`cpu`和`cuda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "advisory-diagnosis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "funny-acoustic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aggressive-inspection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(type=\"cuda\", index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cosmetic-baptist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1, 2, 3, 4, 5, 6], device=torch.device(\"cuda:0\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "sought-algorithm",
   "metadata": {},
   "source": [
    "### Tensor 的内存布局\n",
    "\n",
    "Tensor 的 `layout` 属性表示Tensor内部数据存储的内部布局，目前还是一个不成熟(beta)的特性，目前支持\n",
    "\n",
    "- torch.strided\n",
    "- torch.sparse_coo\n",
    "\n",
    "现在主要用的就是面向 dense Tensor的`torch.strided`，Tensor的 `strides` 是一个list，它代表每个dimension上两邻两个元素之间的跨度(元素个数)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abandoned-spanking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 5, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(60).reshape(3, 4, 5).stride()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643e5b03",
   "metadata": {},
   "source": [
    "我们可以理解为 Tensor 底层的存储的是一个一维的数组，我们对于 `Tensor`的索引，全部是是通过一个下标对应的 stride 来计算出最终在一维数组上的偏移量。 这样实现的好处时，对于 `Tensor`的很多操作，它并不需要实际对 `Tensor`的内存数据进行变动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cf2ce6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n",
      "(1, 4)\n"
     ]
    }
   ],
   "source": [
    "t = torch.arange(12).view(3, 4)\n",
    "# t_transposed 和 t 共享底层的数据\n",
    "t_transposed = t.transpose(0, 1)\n",
    "print(t.stride())\n",
    "print(t_transposed.stride())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fff0e6",
   "metadata": {},
   "source": [
    "`stride()`方法除了可以直接返回一个`List`之外，还可以通过维度的索引，返回对应维度上相邻两个元素之间在底层数组上的跨度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e9d63ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stride on dim 0: 4\n",
      "stride on dim 1: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"stride on dim 0: {t.stride(0)}\")\n",
    "print(f\"stride on dim 1: {t.stride(1)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "127fa16a",
   "metadata": {},
   "source": [
    "和 Numpy 不同的是，torch 中的 `stride` 以元素的个数来表示跨度，而 Numpy 则是用字节数量来表示相邻两个元素之间的跨度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "467f02b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 40, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(60).reshape(3, 4, 5).strides"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "painful-authorization",
   "metadata": {},
   "source": [
    "### Tensor属性转换\n",
    "\n",
    "我们可以使用`to`方法来进行 Tensor 的相关属性的转换，包括：数值类型、设备类型等，接口返回的是一个新的转换后的 Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "minute-cassette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64 cpu\n",
      "torch.float32 cuda:0\n"
     ]
    }
   ],
   "source": [
    "device_cuda = torch.device(\"cuda\")\n",
    "data = torch.tensor([1])\n",
    "print(data.dtype, data.device)\n",
    "data = data.to(dtype=torch.float32, device=device_cuda)\n",
    "print(data.dtype, data.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccddd5b",
   "metadata": {},
   "source": [
    "也可以通过 Tensor 的`dtype`方法来直接将返回新数据类型的 Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aba23467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.int()  # 将 data 转换为 int 类型\n",
    "data.float()  # 将 data 转换为 bool 类型\n",
    "data.bool()  # 将 data 转换为 bool 类型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "entertaining-aquatic",
   "metadata": {},
   "source": [
    "### Tensor的形状\n",
    "\n",
    "Tensor除了具有3个标准的属性外，一旦我们创建了一个Tensor，那么它就会具有一些形状相关的属性，我们可以通过下面这些接口获取 `Tensor` 不同的尺寸相关的信息。\n",
    "\n",
    "- `t.shape`: 返回的是一个 `torch.Size(tuple)` 类型的结果，表示每一维的维度值\n",
    "- `t.size()`: 和 `t.shape` 一致\n",
    "- `t.size(i)`: 返回第 `i` 个维度的值\n",
    "- `t.ndim`：返回 `Tensor` 有多少维\n",
    "- `t.numel()`：它是一个方法，返回 `Tensor` 内有多少个元素\n",
    "- `len(t)`：返回的是 `Tensor` 在第`0`维上的维度值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "headed-dylan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of t: torch.Size([2, 3, 4])\n",
      "size of t: torch.Size([2, 3, 4])\n",
      "size(1) of t: 3\n",
      "strides of t: (12, 4, 1)\n",
      "strides of axes1 of t: 4\n",
      "ndim of t: 3\n",
      "numel of t: 24\n",
      "len of t: 2\n"
     ]
    }
   ],
   "source": [
    "t = torch.empty(2, 3, 4)\n",
    "print(f\"shape of t: {t.shape}\")\n",
    "print(f\"size of t: {t.size()}\")\n",
    "print(f\"size(1) of t: {t.size(1)}\")\n",
    "print(f\"strides of t: {t.stride()}\")\n",
    "print(f\"strides of axes{1} of t: {t.stride(1)}\")\n",
    "print(f\"ndim of t: {t.ndim}\")\n",
    "print(f\"numel of t: {t.numel()}\")\n",
    "print(f\"len of t: {len(t)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "moderate-country",
   "metadata": {},
   "source": [
    "## Tensor的创建"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "neural-outline",
   "metadata": {},
   "source": [
    "在Pytorch中我们可以有多种方法来创建Tensor，常用的包括下面几种：\n",
    "\n",
    "- 从已有的 `scalar`、`list`、`tuple`、`numpy.array` 来创建\n",
    "- 用`arange`、`linspace`、`logspace`等创建一维数列 `Tensor`\n",
    "- 用`ones`、`zeros`、`eye`、`full`、`empty`等来创建特别填充值的多维 `Tensor`\n",
    "- 用随机数来创建指定形状的 `Tensor`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "personal-energy",
   "metadata": {},
   "source": [
    "### 从现有数据来创建\n",
    "\n",
    "我们可以使用`torch.tensor()`函数来从已有的一个 array_like 的 data 来创建一个 Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "awful-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从list创建\n",
    "t1 = torch.tensor([1, 2, 3, 4, 5])\n",
    "# 从tuple创建\n",
    "t2 = torch.tensor((1, 2, 3))\n",
    "# 从numpy.array创建，同时指定dtype和device\n",
    "t3 = torch.tensor(np.array([1, 2, 3, 4, 5]), dtype=torch.float32, device=\"cuda:0\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "stable-peeing",
   "metadata": {},
   "source": [
    "需要注意的是，无论是从 `python` 的内置序列创建，还是从 `numpy.array` 来创建，创建出来的 `Tensor` 都是复制了原数据的内容。如果我们希望，创建的 `Tensor` 不额外分配存储空间，而是和之前的 `numpy.array` 共享存储，那么可以使用`as_tensor`方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "secret-geneva",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始的array:  [1 2 3 4 5]\n",
      "修改后的arr:  [6 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([1, 2, 3, 4, 5])\n",
    "print(f\"原始的array: \", arr)\n",
    "t = torch.as_tensor(arr)\n",
    "# 对于Tensor的数据改动，也会影响在ndarray上\n",
    "t[0] = 6\n",
    "print(f\"修改后的arr: \", arr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "established-mystery",
   "metadata": {},
   "source": [
    "不过使用`as_tensor`后，能共享底层存储的，前提是，`as_tensor` 方法中指定的`dtype`和`device`和原 `ndarry` 是一致的。由于 `numpy` 不支持 CUDA，所以这样只能创建 cpu 上的 Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fresh-extreme",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([1, 2, 3, 4, 5])  # 原 arr 的数值类型是 int64\n",
    "t = torch.as_tensor(arr, dtype=torch.float32)  # 这种情况下，并不会共享底层存储\n",
    "t[0] = 6\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4160ee27",
   "metadata": {},
   "source": [
    "当我们使用 List 来创建 Tensor 时，需要注意对于数据类型的转换处理：对于 int 类型，NDArray 和 Tensor 都会转换为 `int64`，对于浮点数 float，Tensor 默认转换为`float32`，而 NDArray 则默认转换为 `float64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "circular-district",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndarray的默认整数类型为:int64\n",
      "tensor的默认整数类型为: torch.int64\n",
      "ndarray的默认整数类型为:float64\n",
      "tensor的默认整数类型为: torch.float32\n"
     ]
    }
   ],
   "source": [
    "il = [1, 2, 3, 4, 5]\n",
    "print(f\"ndarray的默认整数类型为:{np.array(il).dtype}\")\n",
    "print(f\"tensor的默认整数类型为: {torch.tensor(il).dtype}\")\n",
    "\n",
    "fl = [1.0, 2.0, 3.0, 4.0, 5.0]  # List[float]\n",
    "print(f\"ndarray的默认整数类型为:{np.array(fl).dtype}\")\n",
    "print(f\"tensor的默认整数类型为: {torch.tensor(fl).dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19507a0e",
   "metadata": {},
   "source": [
    "从另外一个 Tensor 来创建 Tensor，无论 b 是否指新新的 `dtype` 和 `device`，b 都不和 a 共享数据，根据 Warning 提示，我们知道这种用法目标在 Pytorch 中是不推荐的，如果要复制 Tensor，可以直接用 `clone` 方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fece622f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4144/1755634830.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  b = torch.tensor(a, dtype=torch.float, device=\"cuda:0\")\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor(a, dtype=torch.float, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b1990fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "b = a.clone()\n",
    "b = b.to(device=\"cuda:0\")\n",
    "print(b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "unauthorized-today",
   "metadata": {},
   "source": [
    "### torch.Tensor()\n",
    "\n",
    "请注意 `torch.Tensor`和 `torch.tensor`的不同。 `torch.Tensor` 实际上是`torch.FloatTensor`，用它来创建新的Tensor时，实际调用的是构造函数，它会默认以`torch.float32`来作为`dtype`。而`torch.tensor`会根据`data`的类型自动推断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "right-andrews",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "l = [1, 2, 3, 4, 5]\n",
    "print(torch.Tensor(l).dtype)\n",
    "print(torch.tensor(l).dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "stainless-garden",
   "metadata": {},
   "source": [
    "### 创建特别填充值的Tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dramatic-driver",
   "metadata": {},
   "source": [
    "#### `torch.arange`\n",
    "\n",
    "`torch.arange(start=0, end, step=1)` 用于创建一个区间范围的 Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "perfect-alpha",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([1, 2, 3, 4])\n",
      "tensor([ 1,  4,  7, 10, 13, 16, 19])\n"
     ]
    }
   ],
   "source": [
    "print(torch.arange(5))\n",
    "print(torch.arange(1, 5))\n",
    "print(torch.arange(1, 20, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03b62d7",
   "metadata": {},
   "source": [
    "如果 `start`、`end` 以及 `step` 中有浮点数，则创建出来的是 `FloatTensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "crucial-mission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.5000, 2.0000, 2.5000, 3.0000])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1, 3.5, 0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "indie-excellence",
   "metadata": {},
   "source": [
    "注意上面是没有包括 3.5 那个点的"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "robust-general",
   "metadata": {},
   "source": [
    "#### `torch.linspace`\n",
    "\n",
    "`torch.linspace`与`torch.arange`有点类似，都指定一个起点，一个终点，和一个步长。但`linspace`里步长最终指定了生成的一维 Tensor 中元素的个数\n",
    "\n",
    "```python\n",
    "def linspace(start:float ,end:float ,steps:int) -> Tensor:\n",
    "    pass\n",
    "```\n",
    "\n",
    "另外需要注意的是`torch.linspace`生成的一定是一个浮点数的Tensor，而且和`torch.arange`不同的是：`linspace`生成的Tensor是包括末点值的（inclusive）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cognitive-origin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.0000,  4.7500,  6.5000,  8.2500, 10.0000])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(3, 10, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "enhanced-breach",
   "metadata": {},
   "source": [
    "#### `torch.logspace`\n",
    "\n",
    "`torch.logspace`和`torch.linspace`行为类似，区别在于`logspace`生成的序列的范围的起始与终点是一个以`base`为底，`start`和`end`为指数的数字。\n",
    "\n",
    "```python\n",
    "def logspace(start:float ,end:float ,steps:int, base=10.0) -> Tensor:\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "innovative-graduation",
   "metadata": {},
   "source": [
    "#### `torch.ones`、`torch.zeros`、`torch.emtpy`\n",
    "\n",
    "它们三个都是用于创建一个指定 `size` 的Tensor，分别以`1`、`0`和未初始化的值来填充创建好的 `Tensor`。它们三个返回的都是 `FloatTensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "invisible-lesbian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[3.5373e-05, 4.5572e-41, 3.5373e-05],\n",
      "        [4.5572e-41, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.ones((2, 2)))\n",
    "print(torch.zeros((3, 4)))\n",
    "print(torch.empty((3, 3)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54e6d4db",
   "metadata": {},
   "source": [
    "`torch.ones/zeros/empty`支持`torch.ones(d1,d2,...)`这种调用方法，而 `numpy` 则不支持。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "statewide-brooklyn",
   "metadata": {},
   "source": [
    "#### `torch.eye`\n",
    "\n",
    "`torch.eye` 返回的是一个 2D 的对角线为 `1`，其他值都为`0` 的 `FloatTensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fatal-happiness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "interim-following",
   "metadata": {},
   "source": [
    "#### `torch.full`\n",
    "\n",
    "`torch.full`返回的是一个指定`size`和填充值的Tensor，Tensor的 `dtype` 是由填充值的类型来推导的。\n",
    "\n",
    "```python\n",
    "def full(size, fill_value) -> Tensor:\n",
    "  '''\n",
    "  Args:\n",
    "    size(int...): a list ,tuple or torch.Size\n",
    "    fill_vale(Scalar)\n",
    "  '''\n",
    "  pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "technological-highway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full((2, 3), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db37d49a",
   "metadata": {},
   "source": [
    "#### `torch.diag`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218209a5",
   "metadata": {},
   "source": [
    "如果输入的是一个 `1d` 的 Tensor，则返回的是一个 `2d` 的对角矩阵，其对角线上的元素为传入的 `Tensor`，也可以通过`diagonal`来指定对角线元素的轴偏值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2cb6d99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 3]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(torch.tensor([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3b9c9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [0, 2, 0, 0],\n",
       "        [0, 0, 3, 0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(torch.tensor([1, 2, 3]), diagonal=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb37e666",
   "metadata": {},
   "source": [
    "传入 `2d` 的 `Tensor`，则返回 `Tensor` 的对角线上的元素，返回的是一个 `1d` 的 `Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77507e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 5, 9])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4fd32bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 6])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), diagonal=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "chinese-ranch",
   "metadata": {},
   "source": [
    "### 使用随机数来创建 `Tensor`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "resident-physiology",
   "metadata": {},
   "source": [
    "#### `torch.normal`\n",
    "\n",
    "`torch.normal`返回一个正态分布产生在的随机数填充的Tensor，它一共有4种参数传递方式\n",
    "\n",
    "* 第一种是: `torch.norm(mean, std)` 其中 `mean` 和 `std` 都是一个 Tensor，生成的 `Tensor` 的形状和 `mean` 和`std` 的形状是一致的，其中每个元素都是通过对应位置的 `mean` 和 `std` 形成的正态分布来随机产生的。\n",
    "* 第二种是: `normal(mean=0.0, std, *, out=None)`。这种参数传递用法，与上面的区别就是 `mean` 变成一个 `scalar`，那么说明每个元素来共享一个 `mean` 值。在这种情况下。\n",
    "* 第三种是: `normal(mean, std=1.0, *, out=None)`。这种情况和第 2 种情况，恰恰相反了，`std` 变成了每个元素共享的。\n",
    "* 第四种是: `normal(mean, std, size, *, out=None)`。这种情况下，所有的元素都共享 `mean` 和 `std`，最终 `Tensor` 的形状是由 `size` 来决定的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "super-louisville",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0296, -0.4701,  0.1796, -0.5136],\n",
       "        [-1.6148,  0.7872, -0.9362,  1.4091],\n",
       "        [ 0.0777, -0.7316, -1.0285, -2.2252]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = torch.randn(3, 4)\n",
    "std = torch.rand((3, 4))\n",
    "torch.normal(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "olive-third",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1250,  0.4628,  2.7116,  0.6063],\n",
       "        [ 1.2810,  0.8124,  0.4107,  0.7474],\n",
       "        [ 0.7084,  1.1920, -0.7098,  1.2650]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(1.0, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ambient-shame",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2751, -0.2002,  0.5267, -0.8647],\n",
       "        [-1.4531,  0.8913, -0.8418,  1.7195],\n",
       "        [ 0.2077, -0.6990, -0.4085, -1.8245]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(mean, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "statewide-defeat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0147, -0.7058,  0.6063,  0.1756],\n",
       "        [ 0.5358, -0.5268, -0.1732, -1.9743],\n",
       "        [ 1.6481, -0.0582,  1.1962, -2.3791]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(0, 1, (3, 4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "expensive-ground",
   "metadata": {},
   "source": [
    "#### `torch.rand`、`torch.randn`\n",
    "\n",
    "* `rand`直接生成指定形状的 `Tensor`，其中每个元素都是由`[0,1)`均匀分布来随机产生。\n",
    "* `randn`直接生成指定形状的 `Tensor`，其中每个元素都是由标准正态分布来随机产生。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "difficult-exclusive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5694, 0.6862, 0.6448, 0.9219],\n",
       "        [0.7739, 0.2337, 0.6813, 0.9098],\n",
       "        [0.5170, 0.7443, 0.1146, 0.3266]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(3, 4)  # 或者 torch.randn((3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "resident-austin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2253, -0.2388, -1.1644,  1.8077],\n",
       "        [ 0.7187,  0.2216, -0.5639, -0.8072],\n",
       "        [-0.8558,  1.1578,  1.1449,  0.7575]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 4)  # 或者 torch.randn((3,4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "regional-afternoon",
   "metadata": {},
   "source": [
    "#### `torch.randint`\n",
    "\n",
    "```python\n",
    "randint(low=0, high, size, **kwargs)\n",
    "```\n",
    "产生一个由 `[low,high)` 区间均匀分布随机数填充的 `LongTensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "large-vaccine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9, 5, 1, 2],\n",
       "        [8, 1, 9, 7],\n",
       "        [2, 2, 8, 1]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(1, 10, (3, 4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "danish-prime",
   "metadata": {},
   "source": [
    "#### `torch.randperm`\n",
    "\n",
    "生成一个随机全排列的一维的 `LongTensor`，一般可以用于 RandomShuffle 的场景下，作为下标，比如下：`RandomSampler`里。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "strange-biography",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 10,  6,  1,  7,  9,  8, 11,  2,  3,  5,  4])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randperm(12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "directed-simon",
   "metadata": {},
   "source": [
    "### 使用`xx_like`系列创建相同形态的Tensor\n",
    "\n",
    "除了shape保持一致外，`dtype`、`layout`、`device`等，若无特别指定，则也与源 `Tensor` 保持一致。\n",
    "\n",
    "```python\n",
    "torch.zeros_like(input, **kwargs) # 返回与input相同size的零矩阵\n",
    "torch.ones_like(input, **kwargs) #返回与input相同size的单位矩阵\n",
    "torch.full_like(input, fill_value, **kwargs) #返回与input相同size，单位值为fill_value的矩阵\n",
    "torch.empty_like(input, **kwargs) # 返回与input相同size,并被未初始化的数值填充的tensor\n",
    "torch.rand_like(input, dtype=None, **kwargs) #返回与input相同size的tensor, 填充均匀分布的随机数值\n",
    "torch.randint_like(input, low=0, high, dtype=None, **kwargs) #返回与input相同size的tensor, 填充[low, high)均匀分布的随机数值\n",
    "torch.randn_like(input, dtype=None, **kwargs) #返回与input相同size的tensor, 填充标准正态分布的随机数值\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "spatial-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.randn(4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "forty-membrane",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros_like(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "overall-dragon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(src, dtype=torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "backed-budapest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty_like(src, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "pointed-williams",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42., 42., 42., 42., 42.],\n",
       "        [42., 42., 42., 42., 42.],\n",
       "        [42., 42., 42., 42., 42.],\n",
       "        [42., 42., 42., 42., 42.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里即使full_value是int类型，但生成的Tensor，依然是用的src的dtype\n",
    "torch.full_like(src, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "noticed-spanking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3649, 0.4973, 0.2954, 0.2283, 0.3783],\n",
       "        [0.4030, 0.3560, 0.2984, 0.0538, 0.6853],\n",
       "        [0.9367, 0.0751, 0.0192, 0.1870, 0.0224],\n",
       "        [0.4367, 0.9709, 0.9743, 0.0202, 0.0806]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand_like(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "norwegian-notice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2378,  1.3116,  1.0912,  0.0476, -0.9063],\n",
       "        [ 1.0352,  0.5998,  0.5077,  1.5463, -1.3423],\n",
       "        [-0.5876,  0.7427, -0.9596, -0.8097, -1.2763],\n",
       "        [ 0.4394,  0.4913, -0.6191, -1.1212, -1.3895]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn_like(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "committed-soundtrack",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 6., 6., 6., 4.],\n",
       "        [3., 2., 3., 2., 7.],\n",
       "        [3., 6., 3., 3., 2.],\n",
       "        [2., 2., 2., 5., 1.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint_like(src, 1, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "amateur-dream",
   "metadata": {},
   "source": [
    "## Tensor的操作\n",
    "\n",
    "Pytorch中的 `Tensor` 大约支持 100 种以上的操作，其中包括了数学运算、线性代数、矩阵操作（转置、索引、切片等），这些操作都可以跑在 CPU 或 GPU 上，这也是 Pytorch `Tensor`的强大之处。\n",
    "\n",
    "我们可以通过这个[页面](https://pytorch.org/docs/stable/torch.html)，来对Tensor支持的所有操作做个大概的了解。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "derived-holocaust",
   "metadata": {},
   "source": [
    "### 索引访值\n",
    "\n",
    "#### 基础索引\n",
    "\n",
    "我们可以像访问 `numpy.ndarray` 一样，对 `torch.Tensor` 进行各种下标索引与范围切片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "canadian-insulin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "取t的第2行的所有元素: tensor([4, 5, 6, 7])\n",
      "取t的最后一列的所有元素: tensor([ 3,  7, 11])\n",
      "取t的第2列到最后一列的所有元素: tensor([[ 2,  3],\n",
      "        [ 6,  7],\n",
      "        [10, 11]])\n",
      "取t的位置(2,3)上的元素: 11\n"
     ]
    }
   ],
   "source": [
    "t = torch.arange(12).reshape(3, 4)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"取t的第2行的所有元素: {t[1]}\")\n",
    "print(f\"取t的最后一列的所有元素: {t[:, -1]}\")\n",
    "print(f\"取t的第2列到最后一列的所有元素: {t[:, 2:]}\")\n",
    "print(f\"取t的位置(2,3)上的元素: {t[2, 3]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "necessary-adoption",
   "metadata": {},
   "source": [
    "当我们通过索引访问Tensor的单一元素时，得到的实际是一个`Tensor` 类型的对象，它并不是 python 中的内置数据类型，我们可以通过Tensor的`item()`方法来获取python对象的标量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "sound-pattern",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([])\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "single_element = t[2, 3]\n",
    "print(type(single_element))\n",
    "print(single_element.shape)\n",
    "print(single_element.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a3d026",
   "metadata": {},
   "source": [
    "注意对如果某个维度上我们只取一行/列数据，那么有两种方式，这两种方式得到的结果的 Shape 会不一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f9e4eddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2],\n",
      "        [ 6],\n",
      "        [10]])\n",
      "tensor([ 2,  6, 10])\n"
     ]
    }
   ],
   "source": [
    "t1 = t[:, 2:3]\n",
    "t2 = t[:, 2]\n",
    "print(t1)  # 还是一个二维的Tensor\n",
    "print(t2)  # 一维的 Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503e5459",
   "metadata": {},
   "source": [
    "#### 高级索引\n",
    "\n",
    "Tensor 的高级索引，支持我们直接用一个 `Long` 型的 `Tensor` 作为索引来取原 Tensor 中的元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0e2d8bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 10])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(8, 10)\n",
    "# indices 的所有元素都代表 t 的 dim=0 的下标\n",
    "indices = torch.randint(0, 8, (3, 2))\n",
    "t[indices].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0858ac95",
   "metadata": {},
   "source": [
    "#### torch.gather\n",
    "\n",
    "torch.gather 往往用于我们希望依次在输入 Tensor 的某个维度上取出其中一些索引的值。比如下面的 topK 的结果中，我们希望根据返回的 indices 取得对应的元素，也就是 values，这里可以用 gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1eac5e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0688,  0.6633,  1.1023, -0.5922, -0.2700],\n",
      "        [ 1.3701,  0.2983,  1.6838, -0.2791, -2.1550],\n",
      "        [-0.7148, -0.3601,  0.8379, -1.0062, -1.4216]])\n",
      "tensor([[ 1.1023,  0.6633],\n",
      "        [ 1.6838,  1.3701],\n",
      "        [ 0.8379, -0.3601]])\n",
      "tensor([[2, 1],\n",
      "        [2, 0],\n",
      "        [2, 1]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn(3, 5)\n",
    "values, indices = torch.topk(t, k=2, dim=1)\n",
    "print(t)\n",
    "print(values)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ebd0d2bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True],\n",
       "        [True, True],\n",
       "        [True, True]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values == torch.gather(t, dim=1, index=indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "through-marketplace",
   "metadata": {},
   "source": [
    "### 组合与分片\n",
    "\n",
    "#### torch.cat\n",
    "\n",
    "```python\n",
    "def cat(tensors, dim=0) -> Tensor:\n",
    "    pass    \n",
    "```\n",
    "\n",
    "`torch.cat`将给定义的tensor的序列(tensors)，按给定义的维度上合并起来，这就要求，这些tensor，除了合并的维度，其他的维度必须一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "every-parade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3146,  0.7856, -0.8999],\n",
       "        [ 0.1525,  0.3796,  0.4523],\n",
       "        [ 0.1411, -0.0736,  0.8204],\n",
       "        [-0.4679, -0.5389, -0.3914],\n",
       "        [ 0.3581,  0.1357, -0.6964]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.randn(2, 3)\n",
    "t2 = torch.randn(3, 3)\n",
    "torch.cat([t1, t2], dim=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cardiac-luther",
   "metadata": {},
   "source": [
    "#### torch.stack\n",
    "\n",
    "`torch.stack`和`torch.cat`接口用法一致，但它并不是在原有的维度上拼接，而是直接扩展一个新的维度。\n",
    "\n",
    "这就要求，序列中的tensor在维度上必须一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "peripheral-membrane",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1604, -0.5684,  0.3347],\n",
       "         [ 0.1768, -1.4267,  0.9843]],\n",
       "\n",
       "        [[-1.0624,  1.6439, -0.4874],\n",
       "         [-0.0111, -1.1370,  0.3593]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.randn(2, 3)\n",
    "t2 = torch.randn(2, 3)\n",
    "torch.stack([t1, t2], dim=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "complimentary-ordinance",
   "metadata": {},
   "source": [
    "#### torch.split\n",
    "\n",
    "```python\n",
    "def split(tensor, split_size_or_sections, dim=0):\n",
    "    pass\n",
    "```\n",
    "\n",
    "`split`将tensor按指定的维度，分拆为多个Tensor的元组，拆分的块chunk的大小是splite_size指定的。可能出现不能整分的情况，这时候最后一块大小一般小于 `splite_size`\n",
    "\n",
    "`split` 出来的 `Tensor` 是原 `Tensor` 的一个`view`视图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "identical-discipline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1],\n",
       "         [2, 3]]),\n",
       " tensor([[4, 5],\n",
       "         [6, 7]]),\n",
       " tensor([[8, 9]]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(10).view(5, 2)\n",
    "torch.split(a, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "narrative-hearts",
   "metadata": {},
   "source": [
    "`split_size_or_sections`也可能是一个list(int)，这时候，它的每个元素，代表每个chunk的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "starting-algorithm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1]]),\n",
       " tensor([[2, 3],\n",
       "         [4, 5],\n",
       "         [6, 7]]),\n",
       " tensor([[8, 9]]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1, a2, a3 = torch.split(a, (1, 3, 1))\n",
    "a1, a2, a3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "confidential-charger",
   "metadata": {},
   "source": [
    "#### torch.chunk\n",
    "\n",
    "```python\n",
    "def chunk(input, chunks, dim=0) -> List[Tensors]:\n",
    "    pass\n",
    "```\n",
    "`chunk`和`split`功能类似，不同在于，`chunk` 的第二的参数，直接指定的是 `chunk` 的数量，最后一个 `chunk` 的数量可能会少一些。也有可能 `axis[dim]<chunks`，那么就直接切分为`axis[dim]`个。\n",
    "\n",
    "切分出来的这些 `Tensor` 和原 `Tensor` 都是共享底层存储的，也就是说每个 `chunk` 都是原`Tensor`的一个`view`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ba0a82b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(a.shape)\n",
    "len(a.chunk(3, dim=1))  # 由于 shape[1] 小于 3，所以只能切分成 2 块"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "general-sitting",
   "metadata": {},
   "source": [
    "### 变换操作\n",
    "\n",
    "#### torch.reshape\n",
    "\n",
    "```python\n",
    "reshape(input, shape) -> Tensor\n",
    "```\n",
    "`reshape`返回一个和原Tensor具有相同数据，相同数量的Tensor，只是shape不一致。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "gross-botswana",
   "metadata": {},
   "source": [
    "#### torch.view\n",
    "\n",
    "`torch.view` vs. `torch.reshape`\n",
    "\n",
    "`view` 接口是随着第一个 Pytorch 版本就发布的，而`reshape`接口是 v0.4版本中才加入的。`reshape`可以用在`compact`或`non-compact`的 `Tensor` 上，而`view`只能用在`compact`的tensor上。`reshape`如果作用于`non-compact`的 `Tensor`上，则会产生一个拷贝。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "herbal-opening",
   "metadata": {},
   "source": [
    "#### `torch.transpose`\n",
    "\n",
    "```python\n",
    "def transpose(input, dim0, dim1) -> Tensor:\n",
    "    pass\n",
    "```\n",
    "转置input的指定的2个维度，返回的Tensor和原来的Tensor共享存储，内部实际上只是改变了 `stride`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "toxic-delay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 4, 1)\n",
      "(1, 4, 12)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3, 4)\n",
    "y = torch.transpose(x, 0, 2)\n",
    "print(x.stride())\n",
    "print(y.stride())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "built-beach",
   "metadata": {},
   "source": [
    "#### `torch.permute`\n",
    "\n",
    "`permute` 是一个更强大的进行不同维度进行变换的操作，它基本可以完全替代`transpose`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c2b403c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 3])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.permute(x, (2, 0, 1)).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93080d1e",
   "metadata": {},
   "source": [
    "#### `squeeze` 和 `unsqueeze`\n",
    "\n",
    "`squeeze` 在指定的维度上添加一维，而 `unsqueeze` 则在指定的维度上去掉 `size=1`的维度，如果对应维度上的`size`不等于 1，则不做任何操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7c8896e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([2, 3]), \n",
      "y (x.unsqueeze) shape: torch.Size([1, 2, 1, 3])\n",
      "unsqueeze shape torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "y = x.unsqueeze(dim=1).unsqueeze(dim=0)\n",
    "print(f\"x shape: {x.shape}, \\ny (x.unsqueeze) shape: {y.shape}\")\n",
    "print(\"unsqueeze shape\", y.squeeze(dim=0).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "clinical-phoenix",
   "metadata": {},
   "source": [
    "#### `contiguous`\n",
    "\n",
    "在 PyTorch 中，有一些对张量（Tensors）的操作不会改变张量的内容，但会改变数据的组织方式。这些操作包括：\n",
    "\n",
    "`narrow()`，`view()`，`expand()` 和 `transpose()`\n",
    "\n",
    "例如：当调用 `transpose()` 时，PyTorch 并不会生成一个新的张量来重新布局数据，它只是修改张量对象中的元信息，使 offset 和 stride 描述所需的新形状。在下面的例子中，转置后的张量和原始张量共享同一块内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "practical-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.contiguous: False\n",
      "tensor(42.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 2)\n",
    "y = torch.transpose(x, 0, 1)\n",
    "print(f\"y.contiguous: {y.is_contiguous()}\")\n",
    "x[0, 0] = 42\n",
    "print(y[0, 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75c7c0ae",
   "metadata": {},
   "source": [
    "这时就涉及到 “连续性”（contiguous）的概念了。在上面的例子中，`x` 是连续的，但 `y` 不是，因为它的内存布局不同于从头创建的同形状张量。需要注意的是，“连续性”这个词可能有些误导性，因为这并不是说张量的内容散布在不连接的内存块中。这里的字节仍然分配在一个内存块中，只是元素的顺序不同！\n",
    "\n",
    "当调用 `contiguous()` 时，它实际上会复制张量，使其内存中的元素顺序与从头用相同数据创建的张量的顺序一致。\n",
    "\n",
    "通常你不需要担心这一点。在大多数情况下，你可以假设一切都会正常运行，直到你遇到 `RuntimeError: input is not contiguous` 的错误提示，即 PyTorch 需要一个连续的张量时，再添加一次 `contiguous()` 调用即可。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "interior-morrison",
   "metadata": {},
   "source": [
    "### 降维操作"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "hundred-syria",
   "metadata": {},
   "source": [
    "#### `torch.mean`\n",
    "\n",
    "```python\n",
    "def mean(input, dim, keepdim=False, *, out=None) -> Tensor:\n",
    "  '''\n",
    "  Args:\n",
    "    input (Tensor): the input tensor.\n",
    "    dim (int or tuple of ints): the dimension or dimensions to reduce.\n",
    "    keepdim (bool): whether the output tensor has :attr:`dim` retained or not.\n",
    "  '''\n",
    "```\n",
    "\n",
    "对 input 沿着`dim`的维度求均值，这样的话，指定的那个维度就会被压缩掉，如果指定了`keepdim=True`的话，那个维度会保留，值为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "centered-murray",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.1964e-01, -1.8094e-01, -2.3633e-01, -8.9539e-01, -8.8298e-01,\n",
      "          2.5287e-01],\n",
      "        [-5.6449e-01,  1.1332e+00,  1.3148e+00, -8.3117e-01, -6.1372e-02,\n",
      "         -2.3306e-01],\n",
      "        [-9.9353e-01,  4.7723e-03, -7.6517e-01, -1.6716e-01, -2.2254e-01,\n",
      "          1.6273e+00],\n",
      "        [-1.1033e+00, -1.5613e+00,  3.0557e-01, -1.1018e-03,  9.1161e-01,\n",
      "          8.8587e-02],\n",
      "        [-1.3331e+00,  1.3878e-01, -1.8567e+00, -4.9110e-01, -4.6809e-01,\n",
      "         -6.4348e-01]])\n",
      "tensor([-0.7550, -0.0931, -0.2476, -0.4772, -0.1447,  0.2185])\n",
      "tensor([[-0.2872],\n",
      "        [ 0.1263],\n",
      "        [-0.0860],\n",
      "        [-0.2267],\n",
      "        [-0.7756]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn(5, 6)\n",
    "print(t)\n",
    "# 按列的方向(dim=0)将整个Tenoor压缩成为1维的\n",
    "print(torch.mean(t, dim=0))\n",
    "# 压缩掉 dim=1 变成 1 维，但同时保持这个维度还存在，它还是一个 2D 的 Tensor\n",
    "print(torch.mean(t, dim=1, keepdim=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "appointed-october",
   "metadata": {},
   "source": [
    "对于高维Tensor，我们还可以同时对多个维度进行 Reduce，求其均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "statistical-tonight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4651, -0.0868, -0.3484])\n",
      "tensor([ 0.4651, -0.0868, -0.3484])\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn(2, 3, 4)\n",
    "# 等价于reduce第0维，得到一个3x4的Tensor后，再reduce第1维，得到(3,)的Vector\n",
    "t1 = torch.mean(t, dim=(0, 2))\n",
    "t2 = t.mean(0).mean(1)\n",
    "print(t1)\n",
    "print(t2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "secondary-explorer",
   "metadata": {},
   "source": [
    "#### `torch.sum`\n",
    "\n",
    "`torch.sum`是一个和`torch.mean`用法上很像的操作，只是`sum`的reduce op变成了求和，而不是求均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "separated-illustration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.7211, -0.6946, -2.7875])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(t, dim=(0, 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31707904",
   "metadata": {},
   "source": [
    "#### `torch.argmax`\n",
    "\n",
    "`argmax` 返回对应维度上数值最大的元素的下标。这个操作在我们做多分类时时候尤其常见。网络输出的是每个类别的概率，我们求出概率最大的那个元素的下标，也就得到了目标的类别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a96184a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[0.7786, 0.9272, 0.2082],\n",
      "        [0.6738, 0.2240, 0.6439]])\n",
      "Argmax: tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((2, 3))\n",
    "print(\"x:\", x)\n",
    "print(\"Argmax:\", x.argmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fbd95c",
   "metadata": {},
   "source": [
    "#### `torch.maxmimu`\n",
    "\n",
    "相同 Shape 的 Tensor 和 Tensor 按元素逐个比大小，保留最大的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fe8a0a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "\ttensor([[-2.4755, -1.9741,  0.5477],\n",
      "        [-1.7354,  2.1774, -0.0848]]) \n",
      "relu(x):\n",
      "\ttensor([[0.0000, 0.0000, 0.5477],\n",
      "        [0.0000, 2.1774, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "def relu(x):\n",
    "    return torch.maximum(x, torch.tensor(0))\n",
    "\n",
    "\n",
    "x = torch.randn((2, 3))\n",
    "print(f\"x:\\n\\t{x} \\nrelu(x):\\n\\t{relu(x)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec47a468",
   "metadata": {},
   "source": [
    "### 排序操作"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "super-still",
   "metadata": {},
   "source": [
    "#### `torch.sort`\n",
    "\n",
    "```python\n",
    "sort(input, dim=-1, descending=False, *, out=None) -> (Tensor, LongTensor)\n",
    "```\n",
    "`sort`对 input 按给定义的 `dim` 进行升序排列，返回排列后的 `Tensor` 的同时，也返回一个对应的下标的重排后的Tensor\n",
    "\n",
    "`dim`的默认值是`Tensor`的最后一维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "potential-morrison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6712, 0.3191, 0.3396],\n",
      "        [0.2225, 0.5578, 0.1161]])\n",
      "sorted values: tensor([[0.6712, 0.3396, 0.3191],\n",
      "        [0.5578, 0.2225, 0.1161]])\n",
      "sroted index: tensor([[0, 2, 1],\n",
      "        [1, 0, 2]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "print(a)\n",
    "values, indices = torch.sort(a, dim=1, descending=True)\n",
    "print(\"sorted values:\", values)\n",
    "print(\"sroted index:\", indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "martial-petersburg",
   "metadata": {},
   "source": [
    "#### `torch.topk`\n",
    "\n",
    "```python\n",
    "topk(input, k, dim=None, largest=True, sorted=True, *, out=None) -> (Tensor, LongTensor)\n",
    "```\n",
    "`topk`返回input中指定维度上，最大的 `k` 个元素，以及对应的索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "pending-nickname",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.2982, -2.3320,  0.5917,  0.2135,  0.4425])\n",
      "top3 values: tensor([1.2982, 0.5917, 0.4425])\n",
      "top3 index: tensor([0, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(5)\n",
    "print(a)\n",
    "values, indices = torch.topk(a, 3)\n",
    "print(\"top3 values:\", values)\n",
    "print(\"top3 index:\", indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "sonic-groove",
   "metadata": {},
   "source": [
    "#### `torch.kthvalue`\n",
    "\n",
    "```python\n",
    "kthvalue(input, k, dim=None, keepdim=False, *, out=None) -> (Tensor, LongTensor)\n",
    "```\n",
    "`kthvalue`计算输出Tensor的指定维度上第`k`小的元素以及下标。也就是从小到大排序的第 `k` 个（从 1 计数）。如果dim没有指定，则默认为Tensor的最后一维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "august-chamber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6328,  0.8376, -0.0609],\n",
      "        [ 1.1792, -1.2289,  1.5499],\n",
      "        [-0.5145, -0.4100, -2.0221],\n",
      "        [ 1.0787, -0.2838,  0.0410]])\n",
      "2th values: tensor([ 0.6328, -0.4100, -0.0609])\n",
      "2th index: tensor([0, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(4, 3)\n",
    "print(a)\n",
    "kth_values, kth_indices = torch.kthvalue(a, 2, dim=0)\n",
    "print(\"2th values:\", kth_values)\n",
    "print(\"2th index:\", kth_indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "207b647a",
   "metadata": {},
   "source": [
    "### `repeat` 和 `repeat_interleave`\n",
    "\n",
    "* `repeat(d0, d1, d2)` 将对应的维度复制多份，如果之前没有对应的维度，则可以当作原来维度为1，处理。\n",
    "* `repeat_interleave(n, dim)` 在对应的维度上进行复制，但复制的方式不是`[a b c a b c ]`这种，而是`[a a b b c c]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b70e577b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[[0, 1, 2, 0, 1, 2],\n",
      "         [3, 4, 5, 3, 4, 5]],\n",
      "\n",
      "        [[0, 1, 2, 0, 1, 2],\n",
      "         [3, 4, 5, 3, 4, 5]]])\n",
      "tensor([[0, 1, 2],\n",
      "        [0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(6).reshape((2, 3))\n",
    "print(a)\n",
    "print(a.repeat((2, 1, 2)))\n",
    "print(a.repeat_interleave(2, dim=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acknowledged-invitation",
   "metadata": {},
   "source": [
    "### 原地操作(in-place)\n",
    "\n",
    "pytorch的Tensor支持了很多原地操作，它们的特点就是在方法末尾以`_`结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "convinced-graphics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 = tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "after plus 2: t1 = tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.ones(2, 3)\n",
    "print(f\"t1 = {t1}\")\n",
    "t1.add_(2)\n",
    "print(f\"after plus 2: t1 = {t1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "electric-austria",
   "metadata": {},
   "source": [
    "### 转换为其他数据类型\n",
    "\n",
    "我们可以调用`numpy`接口,返回一个 `numpy.ndarray` 的对象，可以调用 `tolist` 接口，返回一个 `list` 的对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "clean-nashville",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "# 返回的ndarray还是和t是共享存储的\n",
    "nparray = t.numpy()\n",
    "li = t.reshape(2, 3).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a3a4f5",
   "metadata": {},
   "source": [
    "## 爱因斯坦标识"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "683f0574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, reduce, repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c088bd39",
   "metadata": {},
   "source": [
    "### 实现 `transpose` 和 `permute` 的功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f745fab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 8, 8)\n",
    "\n",
    "# Transose\n",
    "torch.allclose(rearrange(x, \"b c h w->b h w c\"), x.permute((0, 2, 3, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9dd32c",
   "metadata": {},
   "source": [
    "### 一步实现 Transpose + Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ca839bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一步完成 Transpose + Reshape\n",
    "torch.allclose(\n",
    "    rearrange(x, \"b c h w -> (b h w) c\"), x.permute(0, 2, 3, 1).reshape(-1, x.size(1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7690a5",
   "metadata": {},
   "source": [
    "### 实现维度拆分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "58e2386c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 64)\n",
    "\n",
    "torch.allclose(rearrange(x, \"b c (h w) -> b c h w\", h=8), x.reshape(2, 3, 8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e92d7f",
   "metadata": {},
   "source": [
    "### 实现 Image2Patch 的功能\n",
    "\n",
    "将 二维图像转换为 $B\\times N \\times D$ 的序列 Patches 的形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fde29237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1024, 192])\n"
     ]
    }
   ],
   "source": [
    "image = torch.randn(2, 3, 256, 256)\n",
    "\n",
    "patches = rearrange(image, \"b c (h1 ph) (w1 pw) -> b (h1 w1) (ph pw c)\", ph=8, pw=8)\n",
    "print(patches.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84091f14",
   "metadata": {},
   "source": [
    "### Reduce 操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c41a5946",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(8, 10)\n",
    "\n",
    "# mean\n",
    "x_mean = reduce(x, \"b d -> b\", reduction=\"mean\")\n",
    "# sum\n",
    "x_sum = reduce(x, \"b d -> 1 d\", reduction=\"sum\")\n",
    "\n",
    "x = torch.randn(2, 3, 4)\n",
    "x_max = reduce(x, \"b n d -> d\", reduction=\"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48792104",
   "metadata": {},
   "source": [
    "### 扩维与复制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d0a07612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 5])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(5, 5)\n",
    "rearrange(x, \"i j -> 1 i j\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e313f184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.6514,  0.5428],\n",
      "         [-1.0820, -1.2077]]])\n",
      "tensor([[[-1.6514,  0.5428, -1.6514,  0.5428],\n",
      "         [-1.0820, -1.2077, -1.0820, -1.2077]],\n",
      "\n",
      "        [[-1.6514,  0.5428, -1.6514,  0.5428],\n",
      "         [-1.0820, -1.2077, -1.0820, -1.2077]],\n",
      "\n",
      "        [[-1.6514,  0.5428, -1.6514,  0.5428],\n",
      "         [-1.0820, -1.2077, -1.0820, -1.2077]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 2, 2)\n",
    "print(x)\n",
    "# 在最后一维上进行复制 2 次\n",
    "print(repeat(x, \"1 i j -> 3 i (2 j)\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
