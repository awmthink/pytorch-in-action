{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shifted Windows Transformer\n",
    "\n",
    "https://amaarora.github.io/posts/2022-07-04-swintransformerv1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, SwinForImageClassification\n",
    "import torch\n",
    "from torch import nn\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/yangyansheng/.cache/huggingface/modules/datasets_modules/datasets/huggingface--cats-image/68fbc793fb10cd165e490867f5d61fa366086ea40c73e549a020103dcb4f597e (last modified on Mon Oct 14 11:03:54 2024) since it couldn't be found locally at huggingface/cats-image, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tabby, tabby cat\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"huggingface/cats-image\", trust_remote_code=True)\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    \"microsoft/swin-tiny-patch4-window7-224\"\n",
    ")\n",
    "model = SwinForImageClassification.from_pretrained(\n",
    "    \"microsoft/swin-tiny-patch4-window7-224\"\n",
    ")\n",
    "\n",
    "inputs = image_processor(image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_label = logits.argmax(-1).item()\n",
    "print(model.config.id2label[predicted_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 4\n",
    "window_size = 7\n",
    "embed_dim = 96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SwinPatchEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 96, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "_, num_channels, height, weight = inputs.pixel_values.shape\n",
    "patch_projection = nn.Conv2d(num_channels, embed_dim, patch_size, patch_size)\n",
    "\n",
    "# [1, 3, 224, 224] -> [1, 96, 56, 56]\n",
    "embeddings = patch_projection(inputs.pixel_values)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, height, width = embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 96, 3136])\n",
      "torch.Size([1, 3136, 96])\n"
     ]
    }
   ],
   "source": [
    "embeddings = embeddings.flatten(2)\n",
    "print(embeddings.shape)\n",
    "embeddings = embeddings.transpose(1, 2)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window SelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 56, 56, 96])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states = embeddings.view(-1, height, width, embed_dim)\n",
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 7, 8, 7, 96])\n",
      "torch.Size([1, 8, 8, 7, 7, 96])\n",
      "torch.Size([64, 49, 96])\n"
     ]
    }
   ],
   "source": [
    "hidden_states_windows = hidden_states.view(\n",
    "    -1, height // window_size, window_size, width // window_size, window_size, embed_dim\n",
    ")\n",
    "print(hidden_states_windows.shape)\n",
    "hidden_states_windows = hidden_states_windows.permute(0, 1, 3, 2, 4, 5)\n",
    "print(hidden_states_windows.shape)\n",
    "\n",
    "# 将窗口个数的维度合并到 batchsize 中，将每个7x7 的窗口合成一个 序列长度维度\n",
    "hidden_states_windows = hidden_states_windows.contiguous().view(\n",
    "    -1, window_size * window_size, embed_dim\n",
    ")\n",
    "print(hidden_states_windows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shift Windows SelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 56, 56, 96])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states = embeddings.view(-1, height, width, embed_dim)\n",
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14],\n",
       "        [15, 16, 17, 18, 19],\n",
       "        [20, 21, 22, 23, 24]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(25).view((5, 5))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[18, 19, 15, 16, 17],\n",
       "        [23, 24, 20, 21, 22],\n",
       "        [ 3,  4,  0,  1,  2],\n",
       "        [ 8,  9,  5,  6,  7],\n",
       "        [13, 14, 10, 11, 12]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.roll(t, shifts=(2, 2), dims=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Mergeings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
