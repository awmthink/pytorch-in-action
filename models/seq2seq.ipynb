{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于 Seq2Seq 模型的机器翻译实践"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1: 加载数据集，并进行数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 加载本地数据集\n",
    "\n",
    "使用 huggingface 的 datasets 库来加载一个 huggingface 上的数据集（本数据集已经下载到本地）。并划分训练集与数据集。\n",
    "\n",
    "使用的是HuggingFace dataset 中的一个中英翻译的一个数据集：[Garsa3112/ChineseEnglishTranslationDataset](https://huggingface.co/datasets/Garsa3112/ChineseEnglishTranslationDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>zh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>They decide to go to Kashmir for their honeymo...</td>\n",
       "      <td>随后他们前往日本度蜜月,而迪马乔正好也有出差。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kester Echenim.</td>\n",
       "      <td>Calender 砑光。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A luxury car, taxi, or passenger bus would be ...</td>\n",
       "      <td>高级房车、计程车或客运巴士通常可以说是具备较软的弹簧。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>At the Third Congress of Austrian Socialists h...</td>\n",
       "      <td>在奥地利社会主义者第三次代表大会上,达申斯基坚持要将波兰社会民主党与奥地利组织分离,并强调了...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Promoting Content in Africa Keynote.</td>\n",
       "      <td>单曲封面主题为非洲。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0  They decide to go to Kashmir for their honeymo...   \n",
       "1                                    Kester Echenim.   \n",
       "2  A luxury car, taxi, or passenger bus would be ...   \n",
       "3  At the Third Congress of Austrian Socialists h...   \n",
       "4               Promoting Content in Africa Keynote.   \n",
       "\n",
       "                                                  zh  \n",
       "0                            随后他们前往日本度蜜月,而迪马乔正好也有出差。  \n",
       "1                                       Calender 砑光。  \n",
       "2                        高级房车、计程车或客运巴士通常可以说是具备较软的弹簧。  \n",
       "3  在奥地利社会主义者第三次代表大会上,达申斯基坚持要将波兰社会民主党与奥地利组织分离,并强调了...  \n",
       "4                                         单曲封面主题为非洲。  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_data = load_dataset(\n",
    "    \"arrow\",\n",
    "    data_files={\n",
    "        \"train\": \"/data/datasets/Garsa3112/ChineseEnglishTranslationDataset/train.arrow\"\n",
    "    },\n",
    "    split=\"train\",\n",
    ")\n",
    "\n",
    "num_samples = len(translation_data)\n",
    "\n",
    "# 划分训练和测试数据集，测试集占20%\n",
    "train_test_data = translation_data.train_test_split(test_size=0.1)\n",
    "\n",
    "# 将数据集的随机10条记录转换为DataFrame并展示\n",
    "sample_idx = np.random.randint(0, num_samples, 10)\n",
    "sample_df = pd.DataFrame(translation_data.select(sample_idx))\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_length = [(len(sample[\"en\"]), len(sample[\"zh\"])) for sample in translation_data]\n",
    "src_seq_length, tgt_seq_length = zip(*text_length)\n",
    "\n",
    "bins = np.arange(0, 512, 32)\n",
    "_, _, patches = plt.hist(\n",
    "    [src_seq_length, tgt_seq_length],\n",
    "    bins=bins,\n",
    ")\n",
    "plt.xlabel(\"# tokens per sequence\")\n",
    "plt.ylabel(\"count\")\n",
    "for patch in patches[1].patches:\n",
    "    patch.set_hatch(\"/\")\n",
    "plt.legend([\"source\", \"target\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 创建分词器，并创建 Dataloader\n",
    "\n",
    "通过自定义的 collate function 来对翻译的源文本与目标文本分别进行分词并转换为 token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def collate_fn(batch_example, tokenizer, src_max_length, tgt_max_length):\n",
    "    src_texts = []\n",
    "    tgt_texts = []\n",
    "    for example in batch_example:\n",
    "        src_texts.append(example[\"en\"])\n",
    "        tgt_texts.append(tokenizer.bos_token + example[\"zh\"] + tokenizer.eos_token)\n",
    "    src_tokenized = tokenizer(\n",
    "        src_texts,\n",
    "        padding=\"longest\",\n",
    "        max_length=src_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    tgt_tokenized = tokenizer(\n",
    "        tgt_texts,\n",
    "        padding=\"longest\",\n",
    "        max_length=tgt_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    return {\n",
    "        \"src_input_ids\": src_tokenized.input_ids,\n",
    "        \"src_attention_mask\": src_tokenized.attention_mask,\n",
    "        \"tgt_input_ids\": tgt_tokenized.input_ids[:, :-1],\n",
    "        \"tgt_attention_mask\": tgt_tokenized.attention_mask[:, :-1],\n",
    "        \"labels\": tgt_tokenized.input_ids[:, 1:],\n",
    "    }\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")\n",
    "tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \"bos_token\": \"[BOS]\",\n",
    "        \"eos_token\": \"[EOS]\",\n",
    "    }\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "src_max_length = 384\n",
    "tgt_max_length = 256\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_test_data[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=partial(\n",
    "        collate_fn,\n",
    "        tokenizer=tokenizer,\n",
    "        src_max_length=src_max_length,\n",
    "        tgt_max_length=tgt_max_length,\n",
    "    ),\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    train_test_data[\"test\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=partial(\n",
    "        collate_fn,\n",
    "        tokenizer=tokenizer,\n",
    "        src_max_length=src_max_length,\n",
    "        tgt_max_length=tgt_max_length,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_positional_embedding(max_len=256, embed_dim=512):\n",
    "    \"\"\"\n",
    "    生成位置嵌入矩阵，用于为输入序列添加位置信息。\n",
    "\n",
    "    参数:\n",
    "    - max_len: 序列的最大长度\n",
    "    - embed_dim: 嵌入向量的维度\n",
    "\n",
    "    返回:\n",
    "    - position_embeddings: 位置嵌入矩阵\n",
    "    \"\"\"\n",
    "    position_indices = torch.arange(max_len)[:, None]\n",
    "    div_term = torch.pow(10000, torch.arange(0, embed_dim, 2) / embed_dim)[None, :]\n",
    "    position_embeddings = torch.zeros(max_len, embed_dim)\n",
    "    position_embeddings[:, 0::2] = torch.sin(position_indices / div_term)\n",
    "    position_embeddings[:, 1::2] = torch.cos(position_indices / div_term)\n",
    "\n",
    "    return position_embeddings\n",
    "\n",
    "\n",
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    生成自回归掩码，用于避免在解码过程中看到未来的时间步。\n",
    "\n",
    "    参数:\n",
    "    - seq_len: 序列的长度\n",
    "\n",
    "    返回:\n",
    "    - causal_mask: 自回归掩码\n",
    "    \"\"\"\n",
    "    return torch.triu(torch.ones(seq_len, seq_len), diagonal=1) == 1\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        max_seq_len,\n",
    "        d_model,\n",
    "        nhead,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dim_feedforward,\n",
    "        device,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        初始化Transformer模型，用于序列到序列的翻译任务。\n",
    "\n",
    "        参数:\n",
    "        - vocab_size: 词汇表大小\n",
    "        - max_seq_len: 最大序列长度\n",
    "        - d_model: 嵌入维度\n",
    "        - nhead: 多头注意力机制的头数\n",
    "        - num_encoder_layers: 编码器层数\n",
    "        - num_decoder_layers: 解码器层数\n",
    "        - dim_feedforward: 前馈网络的维度\n",
    "        - device: 运行设备\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # 词嵌入层\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model, device=device)\n",
    "\n",
    "        # 位置嵌入矩阵\n",
    "        self.positional_embedding = create_positional_embedding(\n",
    "            max_seq_len, d_model\n",
    "        ).to(device=device)\n",
    "\n",
    "        # 编码器层和编码器\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model,\n",
    "            nhead,\n",
    "            dim_feedforward,\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "            device=device,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, num_encoder_layers, enable_nested_tensor=False\n",
    "        )\n",
    "\n",
    "        # 解码器层和解码器\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model,\n",
    "            nhead,\n",
    "            dim_feedforward,\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "            device=device,\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers)\n",
    "\n",
    "        # 线性投影层，用于生成词汇表中的概率分布\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size, device=device)\n",
    "\n",
    "    def forward(self, src, tgt, src_attention_mask, tgt_attention_mask):\n",
    "        \"\"\"\n",
    "        执行前向传播，计算模型输出。\n",
    "\n",
    "        参数:\n",
    "        - src: 源序列\n",
    "        - tgt: 目标序列\n",
    "        - src_attention_mask: 源序列的掩码\n",
    "        - tgt_attention_mask: 目标序列的掩码\n",
    "\n",
    "        返回:\n",
    "        - output_logits: 预测的词汇分布\n",
    "        \"\"\"\n",
    "        src_embed = self.token_embedding(src)\n",
    "        batch_size, src_len = src.shape\n",
    "\n",
    "        # 为源序列添加位置嵌入\n",
    "        src_positions = torch.arange(0, src_len)[None, :].repeat(batch_size, 1)\n",
    "        src_embed += self.positional_embedding[src_positions]\n",
    "\n",
    "        # 源序列的掩码\n",
    "        src_key_padding_mask = src_attention_mask == 0\n",
    "        memory = self.encoder(src_embed, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        # 生成目标序列的嵌入\n",
    "        tgt_len = tgt.size(1)\n",
    "        tgt_embed = self.token_embedding(tgt)\n",
    "        tgt_positions = torch.arange(0, tgt_len)[None, :].repeat(batch_size, 1)\n",
    "        tgt_embed += self.positional_embedding[tgt_positions]\n",
    "\n",
    "        tgt_key_padding_mask = tgt_attention_mask == 0\n",
    "        causal_mask = create_causal_mask(tgt_len).to(device=tgt.device)\n",
    "\n",
    "        # 解码器的输出\n",
    "        decoder_output = self.decoder(\n",
    "            tgt_embed,\n",
    "            memory,\n",
    "            memory_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            tgt_is_causal=True,\n",
    "            tgt_mask=causal_mask,\n",
    "        )\n",
    "\n",
    "        # 通过线性层得到词汇分布\n",
    "        output_logits = self.output_projection(decoder_output)\n",
    "\n",
    "        return output_logits\n",
    "\n",
    "    def predict(self, src_text, tokenizer):\n",
    "        with torch.inference_mode():\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "nhead = 8\n",
    "dim_feedforward = 2048\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "device = torch.device(\"cuda\", 0)\n",
    "vocab_size = len(tokenizer)\n",
    "\n",
    "model = TransformerModel(\n",
    "    vocab_size,\n",
    "    max(src_max_length, tgt_max_length),\n",
    "    d_model,\n",
    "    nhead,\n",
    "    num_encoder_layers,\n",
    "    num_decoder_layers,\n",
    "    dim_feedforward,\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # 关闭梯度计算\n",
    "        for batch in dataloader:\n",
    "            # 获取输入和目标数据\n",
    "            src_input = batch[\"src_input_ids\"].to(device)\n",
    "            src_mask = batch[\"src_attention_mask\"].to(device)\n",
    "            tgt_input = batch[\"tgt_input_ids\"].to(device)\n",
    "            tgt_mask = batch[\"tgt_attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            logits = model(src_input, tgt_input, src_mask, tgt_mask).transpose(1, 2)\n",
    "\n",
    "            # 计算损失\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 计算准确率\n",
    "            predicted_labels = torch.argmax(logits, dim=1)\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 23:25:38,196 - Epoch: 1/3, Step: 100, Loss: 1.3511\n",
      "2024-09-16 23:26:24,996 - Epoch: 1/3, Step: 200, Loss: 3.1644\n",
      "2024-09-16 23:27:15,324 - Epoch: 1/3, Step: 300, Loss: 1.9691\n",
      "2024-09-16 23:28:03,420 - Epoch: 1/3, Step: 400, Loss: 2.3189\n",
      "2024-09-16 23:28:53,813 - Epoch: 1/3, Step: 500, Loss: 2.6493\n",
      "2024-09-16 23:29:44,081 - Epoch: 1/3, Step: 600, Loss: 2.2011\n",
      "2024-09-16 23:30:33,847 - Epoch: 1/3, Step: 700, Loss: 2.3287\n",
      "2024-09-16 23:31:23,609 - Epoch: 1/3, Step: 800, Loss: 2.1225\n",
      "2024-09-16 23:32:12,119 - Epoch: 1/3, Step: 900, Loss: 2.2073\n",
      "2024-09-16 23:32:59,787 - Epoch: 1/3, Step: 1000, Loss: 2.1682\n",
      "2024-09-16 23:33:52,522 - Epoch: 1/3, Step: 1100, Loss: 1.9106\n",
      "2024-09-16 23:34:41,512 - Epoch: 1/3, Step: 1200, Loss: 2.4593\n",
      "2024-09-16 23:35:31,002 - Epoch: 1/3, Step: 1300, Loss: 1.8154\n",
      "2024-09-16 23:36:20,439 - Epoch: 1/3, Step: 1400, Loss: 2.0338\n",
      "2024-09-16 23:37:10,769 - Epoch: 1/3, Step: 1500, Loss: 2.2787\n",
      "2024-09-16 23:38:03,683 - Epoch: 1/3, Step: 1600, Loss: 1.5839\n",
      "2024-09-16 23:38:54,642 - Epoch: 1/3, Step: 1700, Loss: 2.1701\n",
      "2024-09-16 23:39:43,529 - Epoch: 1/3, Step: 1800, Loss: 1.8672\n",
      "2024-09-16 23:40:33,050 - Epoch: 1/3, Step: 1900, Loss: 1.4888\n",
      "2024-09-16 23:41:21,354 - Epoch: 1/3, Step: 2000, Loss: 1.9866\n",
      "2024-09-16 23:47:16,651 - Validation - Epoch: 1, Loss: 1.8896, Accuracy: 68.8257\n",
      "2024-09-16 23:48:05,770 - Epoch: 1/3, Step: 2100, Loss: 2.1092\n",
      "2024-09-16 23:48:52,646 - Epoch: 1/3, Step: 2200, Loss: 2.0719\n",
      "2024-09-16 23:49:41,652 - Epoch: 1/3, Step: 2300, Loss: 1.4656\n",
      "2024-09-16 23:50:29,838 - Epoch: 1/3, Step: 2400, Loss: 1.7015\n",
      "2024-09-16 23:51:19,220 - Epoch: 1/3, Step: 2500, Loss: 2.2367\n",
      "2024-09-16 23:52:10,145 - Epoch: 1/3, Step: 2600, Loss: 2.1066\n",
      "2024-09-16 23:52:59,668 - Epoch: 1/3, Step: 2700, Loss: 1.5865\n",
      "2024-09-16 23:53:47,823 - Epoch: 1/3, Step: 2800, Loss: 1.9728\n",
      "2024-09-16 23:54:36,219 - Epoch: 1/3, Step: 2900, Loss: 1.3314\n",
      "2024-09-16 23:55:23,199 - Epoch: 1/3, Step: 3000, Loss: 2.0832\n",
      "2024-09-16 23:56:11,634 - Epoch: 1/3, Step: 3100, Loss: 1.2316\n",
      "2024-09-16 23:57:01,903 - Epoch: 1/3, Step: 3200, Loss: 1.0161\n",
      "2024-09-16 23:57:50,578 - Epoch: 1/3, Step: 3300, Loss: 1.2026\n",
      "2024-09-16 23:58:36,854 - Epoch: 1/3, Step: 3400, Loss: 1.8730\n",
      "2024-09-16 23:59:25,085 - Epoch: 1/3, Step: 3500, Loss: 1.6295\n",
      "2024-09-17 00:00:15,180 - Epoch: 1/3, Step: 3600, Loss: 1.2671\n",
      "2024-09-17 00:01:03,727 - Epoch: 1/3, Step: 3700, Loss: 1.2492\n",
      "2024-09-17 00:01:50,094 - Epoch: 1/3, Step: 3800, Loss: 1.3097\n",
      "2024-09-17 00:02:37,876 - Epoch: 1/3, Step: 3900, Loss: 2.0912\n",
      "2024-09-17 00:03:25,737 - Epoch: 1/3, Step: 4000, Loss: 1.6146\n",
      "2024-09-17 00:09:21,096 - Validation - Epoch: 1, Loss: 1.8166, Accuracy: 69.5830\n",
      "2024-09-17 00:10:09,344 - Epoch: 1/3, Step: 4100, Loss: 2.0227\n",
      "2024-09-17 00:10:58,529 - Epoch: 1/3, Step: 4200, Loss: 1.0485\n",
      "2024-09-17 00:11:44,497 - Epoch: 1/3, Step: 4300, Loss: 1.7057\n",
      "2024-09-17 00:12:34,703 - Epoch: 1/3, Step: 4400, Loss: 1.6289\n",
      "2024-09-17 00:13:23,844 - Epoch: 1/3, Step: 4500, Loss: 2.1036\n",
      "2024-09-17 00:14:12,285 - Epoch: 1/3, Step: 4600, Loss: 1.6966\n",
      "2024-09-17 00:15:00,137 - Epoch: 1/3, Step: 4700, Loss: 1.5137\n",
      "2024-09-17 00:15:48,988 - Epoch: 1/3, Step: 4800, Loss: 1.9301\n",
      "2024-09-17 00:16:37,052 - Epoch: 1/3, Step: 4900, Loss: 2.1612\n",
      "2024-09-17 00:17:25,943 - Epoch: 1/3, Step: 5000, Loss: 1.5753\n",
      "2024-09-17 00:18:13,885 - Epoch: 1/3, Step: 5100, Loss: 2.1412\n",
      "2024-09-17 00:19:05,020 - Epoch: 1/3, Step: 5200, Loss: 1.7136\n",
      "2024-09-17 00:19:52,126 - Epoch: 1/3, Step: 5300, Loss: 1.6105\n",
      "2024-09-17 00:20:39,695 - Epoch: 1/3, Step: 5400, Loss: 1.9150\n",
      "2024-09-17 00:21:29,570 - Epoch: 1/3, Step: 5500, Loss: 1.8900\n",
      "2024-09-17 00:22:19,421 - Epoch: 1/3, Step: 5600, Loss: 1.7448\n",
      "2024-09-17 00:23:07,603 - Epoch: 1/3, Step: 5700, Loss: 1.9267\n",
      "2024-09-17 00:23:55,219 - Epoch: 1/3, Step: 5800, Loss: 1.0904\n",
      "2024-09-17 00:24:43,659 - Epoch: 1/3, Step: 5900, Loss: 1.6394\n",
      "2024-09-17 00:25:31,805 - Epoch: 1/3, Step: 6000, Loss: 1.9595\n",
      "2024-09-17 00:31:27,187 - Validation - Epoch: 1, Loss: 1.8099, Accuracy: 69.6511\n",
      "2024-09-17 00:32:13,804 - Epoch: 1/3, Step: 6100, Loss: 1.4186\n",
      "2024-09-17 00:33:00,777 - Epoch: 1/3, Step: 6200, Loss: 1.3356\n",
      "2024-09-17 00:33:49,665 - Epoch: 1/3, Step: 6300, Loss: 1.8533\n",
      "2024-09-17 00:34:38,907 - Epoch: 1/3, Step: 6400, Loss: 1.5589\n",
      "2024-09-17 00:35:28,941 - Epoch: 1/3, Step: 6500, Loss: 1.3925\n",
      "2024-09-17 00:36:18,419 - Epoch: 1/3, Step: 6600, Loss: 2.0488\n",
      "2024-09-17 00:37:08,772 - Epoch: 1/3, Step: 6700, Loss: 1.8406\n",
      "2024-09-17 00:37:57,779 - Epoch: 1/3, Step: 6800, Loss: 1.3326\n",
      "2024-09-17 00:38:47,128 - Epoch: 1/3, Step: 6900, Loss: 1.9780\n",
      "2024-09-17 00:39:34,642 - Epoch: 1/3, Step: 7000, Loss: 2.0840\n",
      "2024-09-17 00:40:23,375 - Epoch: 1/3, Step: 7100, Loss: 2.2119\n",
      "2024-09-17 00:41:12,150 - Epoch: 1/3, Step: 7200, Loss: 1.8542\n",
      "2024-09-17 00:42:00,415 - Epoch: 1/3, Step: 7300, Loss: 1.6707\n",
      "2024-09-17 00:42:47,936 - Epoch: 1/3, Step: 7400, Loss: 1.8965\n",
      "2024-09-17 00:43:36,852 - Epoch: 1/3, Step: 7500, Loss: 2.0231\n",
      "2024-09-17 00:44:24,615 - Epoch: 1/3, Step: 7600, Loss: 1.9704\n",
      "2024-09-17 00:45:12,894 - Epoch: 1/3, Step: 7700, Loss: 2.3557\n",
      "2024-09-17 00:46:00,925 - Epoch: 1/3, Step: 7800, Loss: 1.7362\n",
      "2024-09-17 00:46:49,135 - Epoch: 1/3, Step: 7900, Loss: 2.0877\n",
      "2024-09-17 00:47:38,657 - Epoch: 1/3, Step: 8000, Loss: 2.0963\n",
      "2024-09-17 00:53:34,966 - Validation - Epoch: 1, Loss: 1.8093, Accuracy: 69.6575\n",
      "2024-09-17 00:54:23,294 - Epoch: 1/3, Step: 8100, Loss: 1.5753\n",
      "2024-09-17 00:55:12,305 - Epoch: 1/3, Step: 8200, Loss: 2.3734\n",
      "2024-09-17 00:56:02,806 - Epoch: 1/3, Step: 8300, Loss: 1.2634\n",
      "2024-09-17 00:56:49,455 - Epoch: 1/3, Step: 8400, Loss: 2.0689\n",
      "2024-09-17 00:57:38,318 - Epoch: 1/3, Step: 8500, Loss: 1.8041\n",
      "2024-09-17 00:58:26,365 - Epoch: 1/3, Step: 8600, Loss: 1.9852\n",
      "2024-09-17 00:59:15,859 - Epoch: 1/3, Step: 8700, Loss: 1.8668\n",
      "2024-09-17 01:00:04,790 - Epoch: 1/3, Step: 8800, Loss: 1.5458\n",
      "2024-09-17 01:00:52,386 - Epoch: 1/3, Step: 8900, Loss: 2.1569\n",
      "2024-09-17 01:01:40,822 - Epoch: 1/3, Step: 9000, Loss: 1.6588\n",
      "2024-09-17 01:02:27,117 - Epoch: 1/3, Step: 9100, Loss: 1.4489\n",
      "2024-09-17 01:03:14,520 - Epoch: 1/3, Step: 9200, Loss: 1.7787\n",
      "2024-09-17 01:04:01,348 - Epoch: 1/3, Step: 9300, Loss: 1.8130\n",
      "2024-09-17 01:04:49,710 - Epoch: 1/3, Step: 9400, Loss: 2.1993\n",
      "2024-09-17 01:05:38,165 - Epoch: 1/3, Step: 9500, Loss: 1.7837\n",
      "2024-09-17 01:06:26,570 - Epoch: 1/3, Step: 9600, Loss: 1.9535\n",
      "2024-09-17 01:07:13,870 - Epoch: 1/3, Step: 9700, Loss: 2.1991\n",
      "2024-09-17 01:08:02,292 - Epoch: 1/3, Step: 9800, Loss: 1.6015\n",
      "2024-09-17 01:08:50,070 - Epoch: 1/3, Step: 9900, Loss: 2.0649\n",
      "2024-09-17 01:09:40,345 - Epoch: 1/3, Step: 10000, Loss: 1.6936\n",
      "2024-09-17 01:15:36,549 - Validation - Epoch: 1, Loss: 1.8093, Accuracy: 69.6573\n",
      "2024-09-17 01:16:27,368 - Epoch: 1/3, Step: 10100, Loss: 1.8848\n",
      "2024-09-17 01:17:15,382 - Epoch: 1/3, Step: 10200, Loss: 1.9257\n",
      "2024-09-17 01:18:02,920 - Epoch: 1/3, Step: 10300, Loss: 1.1902\n",
      "2024-09-17 01:18:53,471 - Epoch: 1/3, Step: 10400, Loss: 2.0324\n",
      "2024-09-17 01:19:43,385 - Epoch: 1/3, Step: 10500, Loss: 1.7700\n",
      "2024-09-17 01:20:32,475 - Epoch: 1/3, Step: 10600, Loss: 1.6202\n",
      "2024-09-17 01:21:21,567 - Epoch: 1/3, Step: 10700, Loss: 1.3008\n",
      "2024-09-17 01:22:10,636 - Epoch: 1/3, Step: 10800, Loss: 1.5562\n",
      "2024-09-17 01:22:57,099 - Epoch: 1/3, Step: 10900, Loss: 2.1269\n",
      "2024-09-17 01:23:45,562 - Epoch: 1/3, Step: 11000, Loss: 1.6011\n",
      "2024-09-17 01:24:35,578 - Epoch: 1/3, Step: 11100, Loss: 1.4051\n",
      "2024-09-17 01:25:21,366 - Epoch: 1/3, Step: 11200, Loss: 1.5985\n",
      "2024-09-17 01:26:07,529 - Epoch: 1/3, Step: 11300, Loss: 1.8235\n",
      "2024-09-17 01:26:55,450 - Epoch: 1/3, Step: 11400, Loss: 1.6254\n",
      "2024-09-17 01:27:43,351 - Epoch: 1/3, Step: 11500, Loss: 2.0175\n",
      "2024-09-17 01:28:31,408 - Epoch: 1/3, Step: 11600, Loss: 1.8406\n",
      "2024-09-17 01:29:18,704 - Epoch: 1/3, Step: 11700, Loss: 2.1181\n",
      "2024-09-17 01:30:07,932 - Epoch: 1/3, Step: 11800, Loss: 1.5916\n",
      "2024-09-17 01:30:55,135 - Epoch: 1/3, Step: 11900, Loss: 1.6418\n",
      "2024-09-17 01:31:45,066 - Epoch: 1/3, Step: 12000, Loss: 2.5468\n",
      "2024-09-17 01:37:40,361 - Validation - Epoch: 1, Loss: 1.8093, Accuracy: 69.6572\n",
      "2024-09-17 01:38:28,593 - Epoch: 1/3, Step: 12100, Loss: 1.9836\n",
      "2024-09-17 01:39:17,401 - Epoch: 1/3, Step: 12200, Loss: 1.3241\n",
      "2024-09-17 01:40:07,512 - Epoch: 1/3, Step: 12300, Loss: 1.5349\n",
      "2024-09-17 01:40:54,767 - Epoch: 1/3, Step: 12400, Loss: 1.7733\n",
      "2024-09-17 01:41:43,390 - Epoch: 1/3, Step: 12500, Loss: 1.9201\n",
      "2024-09-17 01:42:32,619 - Epoch: 1/3, Step: 12600, Loss: 1.3389\n",
      "2024-09-17 01:43:21,827 - Epoch: 1/3, Step: 12700, Loss: 1.8164\n",
      "2024-09-17 01:44:11,069 - Epoch: 1/3, Step: 12800, Loss: 1.0630\n",
      "2024-09-17 01:44:58,574 - Epoch: 1/3, Step: 12900, Loss: 2.2516\n",
      "2024-09-17 01:45:47,430 - Epoch: 1/3, Step: 13000, Loss: 1.9616\n",
      "2024-09-17 01:46:35,863 - Epoch: 1/3, Step: 13100, Loss: 1.9339\n",
      "2024-09-17 01:47:24,379 - Epoch: 1/3, Step: 13200, Loss: 1.6197\n",
      "2024-09-17 01:48:15,063 - Epoch: 1/3, Step: 13300, Loss: 1.9436\n",
      "2024-09-17 01:49:04,292 - Epoch: 1/3, Step: 13400, Loss: 1.3865\n",
      "2024-09-17 01:49:54,904 - Epoch: 1/3, Step: 13500, Loss: 2.3296\n",
      "2024-09-17 01:50:43,121 - Epoch: 1/3, Step: 13600, Loss: 2.0084\n",
      "2024-09-17 01:51:32,527 - Epoch: 1/3, Step: 13700, Loss: 1.1619\n",
      "2024-09-17 01:52:20,517 - Epoch: 1/3, Step: 13800, Loss: 1.8854\n",
      "2024-09-17 01:53:08,584 - Epoch: 1/3, Step: 13900, Loss: 1.9461\n",
      "2024-09-17 01:53:56,031 - Epoch: 1/3, Step: 14000, Loss: 1.9763\n",
      "2024-09-17 01:59:51,267 - Validation - Epoch: 1, Loss: 1.8093, Accuracy: 69.6572\n",
      "2024-09-17 02:00:38,958 - Epoch: 1/3, Step: 14100, Loss: 2.1476\n",
      "2024-09-17 02:01:29,109 - Epoch: 1/3, Step: 14200, Loss: 1.9813\n",
      "2024-09-17 02:02:16,091 - Epoch: 1/3, Step: 14300, Loss: 1.2709\n",
      "2024-09-17 02:03:03,300 - Epoch: 1/3, Step: 14400, Loss: 1.7307\n",
      "2024-09-17 02:03:52,493 - Epoch: 1/3, Step: 14500, Loss: 1.6121\n",
      "2024-09-17 02:04:42,050 - Epoch: 1/3, Step: 14600, Loss: 1.8338\n",
      "2024-09-17 02:05:30,265 - Epoch: 1/3, Step: 14700, Loss: 1.7827\n",
      "2024-09-17 02:06:17,306 - Epoch: 1/3, Step: 14800, Loss: 1.7010\n",
      "2024-09-17 02:07:06,988 - Epoch: 1/3, Step: 14900, Loss: 1.9768\n",
      "2024-09-17 02:07:56,559 - Epoch: 1/3, Step: 15000, Loss: 2.1219\n",
      "2024-09-17 02:08:45,147 - Epoch: 1/3, Step: 15100, Loss: 1.5182\n",
      "2024-09-17 02:09:32,886 - Epoch: 1/3, Step: 15200, Loss: 2.0675\n",
      "2024-09-17 02:10:23,387 - Epoch: 1/3, Step: 15300, Loss: 2.0669\n",
      "2024-09-17 02:11:11,621 - Epoch: 1/3, Step: 15400, Loss: 1.7518\n",
      "2024-09-17 02:11:58,676 - Epoch: 1/3, Step: 15500, Loss: 1.8791\n",
      "2024-09-17 02:12:47,250 - Epoch: 1/3, Step: 15600, Loss: 2.1440\n",
      "2024-09-17 02:13:35,986 - Epoch: 1/3, Step: 15700, Loss: 2.3379\n",
      "2024-09-17 02:14:27,188 - Epoch: 1/3, Step: 15800, Loss: 1.6519\n",
      "2024-09-17 02:15:16,017 - Epoch: 1/3, Step: 15900, Loss: 2.4412\n",
      "2024-09-17 02:16:03,804 - Epoch: 1/3, Step: 16000, Loss: 2.2656\n",
      "2024-09-17 02:21:58,951 - Validation - Epoch: 1, Loss: 1.8093, Accuracy: 69.6572\n",
      "2024-09-17 02:22:47,792 - Epoch: 1/3, Step: 16100, Loss: 1.8515\n",
      "2024-09-17 02:23:35,277 - Epoch: 1/3, Step: 16200, Loss: 1.8646\n",
      "2024-09-17 02:24:25,112 - Epoch: 1/3, Step: 16300, Loss: 1.9523\n",
      "2024-09-17 02:25:12,771 - Epoch: 1/3, Step: 16400, Loss: 1.8290\n",
      "2024-09-17 02:25:59,694 - Epoch: 1/3, Step: 16500, Loss: 2.0092\n",
      "2024-09-17 02:26:49,990 - Epoch: 1/3, Step: 16600, Loss: 1.7375\n",
      "2024-09-17 02:27:38,156 - Epoch: 1/3, Step: 16700, Loss: 1.7064\n",
      "2024-09-17 02:28:26,449 - Epoch: 1/3, Step: 16800, Loss: 1.2377\n",
      "2024-09-17 02:29:17,676 - Epoch: 1/3, Step: 16900, Loss: 1.6689\n",
      "2024-09-17 02:30:07,946 - Epoch: 1/3, Step: 17000, Loss: 1.9326\n",
      "2024-09-17 02:30:56,954 - Epoch: 1/3, Step: 17100, Loss: 1.7093\n",
      "2024-09-17 02:31:44,210 - Epoch: 1/3, Step: 17200, Loss: 1.9935\n",
      "2024-09-17 02:32:33,464 - Epoch: 1/3, Step: 17300, Loss: 1.5364\n",
      "2024-09-17 02:33:21,720 - Epoch: 1/3, Step: 17400, Loss: 1.7390\n",
      "2024-09-17 02:34:08,482 - Epoch: 1/3, Step: 17500, Loss: 1.6440\n",
      "2024-09-17 02:34:57,600 - Epoch: 1/3, Step: 17600, Loss: 2.4103\n",
      "2024-09-17 02:35:46,721 - Epoch: 1/3, Step: 17700, Loss: 1.3362\n",
      "2024-09-17 02:36:37,337 - Epoch: 1/3, Step: 17800, Loss: 1.6515\n",
      "2024-09-17 02:37:27,202 - Epoch: 1/3, Step: 17900, Loss: 1.7742\n",
      "2024-09-17 02:38:14,541 - Epoch: 1/3, Step: 18000, Loss: 2.0448\n",
      "2024-09-17 02:44:10,162 - Validation - Epoch: 1, Loss: 1.8093, Accuracy: 69.6572\n",
      "2024-09-17 02:44:58,408 - Epoch: 1/3, Step: 18100, Loss: 1.7980\n",
      "2024-09-17 02:45:45,588 - Epoch: 1/3, Step: 18200, Loss: 2.1983\n",
      "2024-09-17 02:46:34,497 - Epoch: 1/3, Step: 18300, Loss: 1.4562\n",
      "2024-09-17 02:47:24,776 - Epoch: 1/3, Step: 18400, Loss: 2.0014\n",
      "2024-09-17 02:48:13,474 - Epoch: 1/3, Step: 18500, Loss: 1.7572\n",
      "2024-09-17 02:49:01,144 - Epoch: 1/3, Step: 18600, Loss: 2.3137\n",
      "2024-09-17 02:49:49,750 - Epoch: 1/3, Step: 18700, Loss: 2.1228\n",
      "2024-09-17 02:50:38,736 - Epoch: 1/3, Step: 18800, Loss: 1.8301\n",
      "2024-09-17 02:51:26,933 - Epoch: 1/3, Step: 18900, Loss: 1.7024\n",
      "2024-09-17 02:52:15,203 - Epoch: 1/3, Step: 19000, Loss: 1.4734\n",
      "2024-09-17 02:53:04,981 - Epoch: 2/3, Step: 19100, Loss: 1.4439\n",
      "2024-09-17 02:53:53,962 - Epoch: 2/3, Step: 19200, Loss: 1.8529\n",
      "2024-09-17 02:54:43,188 - Epoch: 2/3, Step: 19300, Loss: 2.3256\n",
      "2024-09-17 02:55:32,225 - Epoch: 2/3, Step: 19400, Loss: 1.6562\n",
      "2024-09-17 02:56:22,625 - Epoch: 2/3, Step: 19500, Loss: 2.2487\n",
      "2024-09-17 02:57:13,499 - Epoch: 2/3, Step: 19600, Loss: 1.5841\n",
      "2024-09-17 02:58:03,100 - Epoch: 2/3, Step: 19700, Loss: 1.8489\n",
      "2024-09-17 02:58:51,762 - Epoch: 2/3, Step: 19800, Loss: 2.2853\n",
      "2024-09-17 02:59:43,572 - Epoch: 2/3, Step: 19900, Loss: 1.3151\n",
      "2024-09-17 03:00:31,588 - Epoch: 2/3, Step: 20000, Loss: 1.9801\n",
      "2024-09-17 03:06:27,681 - Validation - Epoch: 2, Loss: 1.8093, Accuracy: 69.6572\n",
      "2024-09-17 03:07:13,877 - Epoch: 2/3, Step: 20100, Loss: 1.5440\n",
      "2024-09-17 03:08:01,808 - Epoch: 2/3, Step: 20200, Loss: 1.7787\n",
      "2024-09-17 03:08:51,101 - Epoch: 2/3, Step: 20300, Loss: 2.1404\n",
      "2024-09-17 03:09:38,136 - Epoch: 2/3, Step: 20400, Loss: 2.0671\n",
      "2024-09-17 03:10:27,511 - Epoch: 2/3, Step: 20500, Loss: 2.1334\n",
      "2024-09-17 03:11:15,133 - Epoch: 2/3, Step: 20600, Loss: 1.7911\n",
      "2024-09-17 03:12:03,443 - Epoch: 2/3, Step: 20700, Loss: 1.7123\n",
      "2024-09-17 03:12:53,212 - Epoch: 2/3, Step: 20800, Loss: 1.7028\n",
      "2024-09-17 03:13:40,373 - Epoch: 2/3, Step: 20900, Loss: 1.0473\n",
      "2024-09-17 03:14:27,647 - Epoch: 2/3, Step: 21000, Loss: 1.9329\n",
      "2024-09-17 03:15:13,573 - Epoch: 2/3, Step: 21100, Loss: 2.4206\n",
      "2024-09-17 03:16:02,014 - Epoch: 2/3, Step: 21200, Loss: 1.4230\n",
      "2024-09-17 03:16:49,830 - Epoch: 2/3, Step: 21300, Loss: 1.6690\n",
      "2024-09-17 03:17:39,851 - Epoch: 2/3, Step: 21400, Loss: 1.4822\n",
      "2024-09-17 03:18:28,052 - Epoch: 2/3, Step: 21500, Loss: 2.1020\n",
      "2024-09-17 03:19:16,481 - Epoch: 2/3, Step: 21600, Loss: 1.7228\n",
      "2024-09-17 03:20:02,204 - Epoch: 2/3, Step: 21700, Loss: 1.9213\n",
      "2024-09-17 03:20:52,045 - Epoch: 2/3, Step: 21800, Loss: 2.0534\n",
      "2024-09-17 03:21:41,671 - Epoch: 2/3, Step: 21900, Loss: 2.0486\n",
      "2024-09-17 03:22:32,706 - Epoch: 2/3, Step: 22000, Loss: 1.9103\n",
      "2024-09-17 03:28:27,959 - Validation - Epoch: 2, Loss: 1.8093, Accuracy: 69.6572\n",
      "2024-09-17 03:29:14,658 - Epoch: 2/3, Step: 22100, Loss: 1.8719\n",
      "2024-09-17 03:30:02,987 - Epoch: 2/3, Step: 22200, Loss: 1.3934\n",
      "2024-09-17 03:30:54,093 - Epoch: 2/3, Step: 22300, Loss: 2.1830\n",
      "2024-09-17 03:31:43,534 - Epoch: 2/3, Step: 22400, Loss: 1.3984\n",
      "2024-09-17 03:32:32,089 - Epoch: 2/3, Step: 22500, Loss: 1.2576\n",
      "2024-09-17 03:33:22,790 - Epoch: 2/3, Step: 22600, Loss: 1.5686\n",
      "2024-09-17 03:34:12,655 - Epoch: 2/3, Step: 22700, Loss: 1.1436\n",
      "2024-09-17 03:35:01,798 - Epoch: 2/3, Step: 22800, Loss: 1.3348\n",
      "2024-09-17 03:35:49,702 - Epoch: 2/3, Step: 22900, Loss: 2.0833\n",
      "2024-09-17 03:36:37,976 - Epoch: 2/3, Step: 23000, Loss: 2.0131\n",
      "2024-09-17 03:37:28,076 - Epoch: 2/3, Step: 23100, Loss: 1.6824\n",
      "2024-09-17 03:38:15,731 - Epoch: 2/3, Step: 23200, Loss: 2.1312\n",
      "2024-09-17 03:39:03,958 - Epoch: 2/3, Step: 23300, Loss: 1.8549\n",
      "2024-09-17 03:39:52,537 - Epoch: 2/3, Step: 23400, Loss: 1.2374\n",
      "2024-09-17 03:40:40,468 - Epoch: 2/3, Step: 23500, Loss: 2.1746\n",
      "2024-09-17 03:41:29,418 - Epoch: 2/3, Step: 23600, Loss: 2.4743\n",
      "2024-09-17 03:42:16,746 - Epoch: 2/3, Step: 23700, Loss: 1.9510\n",
      "2024-09-17 03:43:05,149 - Epoch: 2/3, Step: 23800, Loss: 2.1170\n",
      "2024-09-17 03:43:53,007 - Epoch: 2/3, Step: 23900, Loss: 1.3384\n",
      "2024-09-17 03:44:39,899 - Epoch: 2/3, Step: 24000, Loss: 1.9488\n",
      "2024-09-17 03:50:35,568 - Validation - Epoch: 2, Loss: 1.8093, Accuracy: 69.6572\n",
      "2024-09-17 03:51:25,588 - Epoch: 2/3, Step: 24100, Loss: 2.0166\n",
      "2024-09-17 03:52:14,682 - Epoch: 2/3, Step: 24200, Loss: 2.5728\n",
      "2024-09-17 03:53:04,136 - Epoch: 2/3, Step: 24300, Loss: 2.2091\n",
      "2024-09-17 03:53:51,661 - Epoch: 2/3, Step: 24400, Loss: 2.1709\n",
      "2024-09-17 03:54:38,903 - Epoch: 2/3, Step: 24500, Loss: 1.6820\n",
      "2024-09-17 03:55:27,432 - Epoch: 2/3, Step: 24600, Loss: 1.6585\n",
      "2024-09-17 03:56:16,385 - Epoch: 2/3, Step: 24700, Loss: 1.6002\n",
      "2024-09-17 03:57:06,851 - Epoch: 2/3, Step: 24800, Loss: 2.0370\n",
      "2024-09-17 03:57:53,858 - Epoch: 2/3, Step: 24900, Loss: 2.0451\n",
      "2024-09-17 03:58:43,091 - Epoch: 2/3, Step: 25000, Loss: 2.1532\n",
      "2024-09-17 03:59:32,034 - Epoch: 2/3, Step: 25100, Loss: 2.2803\n",
      "2024-09-17 04:00:22,150 - Epoch: 2/3, Step: 25200, Loss: 1.5769\n",
      "2024-09-17 04:01:12,270 - Epoch: 2/3, Step: 25300, Loss: 1.7341\n",
      "2024-09-17 04:02:00,292 - Epoch: 2/3, Step: 25400, Loss: 1.7931\n",
      "2024-09-17 04:02:48,252 - Epoch: 2/3, Step: 25500, Loss: 2.0950\n",
      "2024-09-17 04:03:35,672 - Epoch: 2/3, Step: 25600, Loss: 1.5706\n",
      "2024-09-17 04:04:23,115 - Epoch: 2/3, Step: 25700, Loss: 2.1726\n",
      "2024-09-17 04:05:12,942 - Epoch: 2/3, Step: 25800, Loss: 1.5401\n",
      "2024-09-17 04:06:02,875 - Epoch: 2/3, Step: 25900, Loss: 1.6026\n",
      "2024-09-17 04:06:50,481 - Epoch: 2/3, Step: 26000, Loss: 1.9882\n",
      "2024-09-17 04:12:45,121 - Validation - Epoch: 2, Loss: 1.8093, Accuracy: 69.6572\n",
      "2024-09-17 04:13:32,874 - Epoch: 2/3, Step: 26100, Loss: 1.4391\n",
      "2024-09-17 04:14:22,627 - Epoch: 2/3, Step: 26200, Loss: 1.2870\n",
      "2024-09-17 04:15:08,847 - Epoch: 2/3, Step: 26300, Loss: 2.0549\n",
      "2024-09-17 04:15:58,936 - Epoch: 2/3, Step: 26400, Loss: 2.2038\n",
      "2024-09-17 04:16:47,713 - Epoch: 2/3, Step: 26500, Loss: 1.3947\n",
      "2024-09-17 04:17:35,072 - Epoch: 2/3, Step: 26600, Loss: 1.6786\n",
      "2024-09-17 04:18:25,228 - Epoch: 2/3, Step: 26700, Loss: 1.6421\n",
      "2024-09-17 04:19:14,297 - Epoch: 2/3, Step: 26800, Loss: 1.9289\n",
      "2024-09-17 04:20:02,629 - Epoch: 2/3, Step: 26900, Loss: 1.7903\n",
      "2024-09-17 04:20:50,396 - Epoch: 2/3, Step: 27000, Loss: 1.8016\n",
      "2024-09-17 04:21:37,695 - Epoch: 2/3, Step: 27100, Loss: 1.5469\n",
      "2024-09-17 04:22:26,078 - Epoch: 2/3, Step: 27200, Loss: 2.0910\n",
      "2024-09-17 04:23:15,068 - Epoch: 2/3, Step: 27300, Loss: 1.9405\n",
      "2024-09-17 04:24:03,885 - Epoch: 2/3, Step: 27400, Loss: 1.7132\n",
      "2024-09-17 04:24:54,716 - Epoch: 2/3, Step: 27500, Loss: 1.7078\n",
      "2024-09-17 04:25:44,028 - Epoch: 2/3, Step: 27600, Loss: 1.7353\n",
      "2024-09-17 04:26:31,644 - Epoch: 2/3, Step: 27700, Loss: 2.1002\n",
      "2024-09-17 04:27:18,676 - Epoch: 2/3, Step: 27800, Loss: 2.2097\n",
      "2024-09-17 04:28:05,956 - Epoch: 2/3, Step: 27900, Loss: 1.7507\n",
      "2024-09-17 04:28:55,068 - Epoch: 2/3, Step: 28000, Loss: 2.2401\n",
      "2024-09-17 04:34:50,328 - Validation - Epoch: 2, Loss: 1.8093, Accuracy: 69.6572\n",
      "2024-09-17 04:35:37,187 - Epoch: 2/3, Step: 28100, Loss: 1.7454\n",
      "2024-09-17 04:36:25,253 - Epoch: 2/3, Step: 28200, Loss: 2.0436\n",
      "2024-09-17 04:37:10,416 - Epoch: 2/3, Step: 28300, Loss: 1.8545\n",
      "2024-09-17 04:37:59,779 - Epoch: 2/3, Step: 28400, Loss: 2.1952\n",
      "2024-09-17 04:38:48,701 - Epoch: 2/3, Step: 28500, Loss: 1.4525\n",
      "2024-09-17 04:39:37,387 - Epoch: 2/3, Step: 28600, Loss: 2.2343\n",
      "2024-09-17 04:40:24,511 - Epoch: 2/3, Step: 28700, Loss: 1.8614\n",
      "2024-09-17 04:41:12,348 - Epoch: 2/3, Step: 28800, Loss: 1.7839\n",
      "2024-09-17 04:42:00,413 - Epoch: 2/3, Step: 28900, Loss: 2.4088\n",
      "2024-09-17 04:42:47,994 - Epoch: 2/3, Step: 29000, Loss: 1.9235\n",
      "2024-09-17 04:43:36,077 - Epoch: 2/3, Step: 29100, Loss: 2.1206\n",
      "2024-09-17 04:44:24,060 - Epoch: 2/3, Step: 29200, Loss: 1.7640\n",
      "2024-09-17 04:45:12,418 - Epoch: 2/3, Step: 29300, Loss: 1.2488\n",
      "2024-09-17 04:45:57,978 - Epoch: 2/3, Step: 29400, Loss: 1.5906\n",
      "2024-09-17 04:46:46,719 - Epoch: 2/3, Step: 29500, Loss: 1.8897\n",
      "2024-09-17 04:47:37,731 - Epoch: 2/3, Step: 29600, Loss: 1.3758\n",
      "2024-09-17 04:48:26,209 - Epoch: 2/3, Step: 29700, Loss: 2.1988\n",
      "2024-09-17 04:49:15,029 - Epoch: 2/3, Step: 29800, Loss: 1.4258\n",
      "2024-09-17 04:50:02,652 - Epoch: 2/3, Step: 29900, Loss: 2.5105\n",
      "2024-09-17 04:50:51,057 - Epoch: 2/3, Step: 30000, Loss: 1.3277\n",
      "2024-09-17 04:56:46,041 - Validation - Epoch: 2, Loss: 1.8093, Accuracy: 69.6572\n",
      "2024-09-17 04:57:37,997 - Epoch: 2/3, Step: 30100, Loss: 1.7992\n",
      "2024-09-17 04:58:27,133 - Epoch: 2/3, Step: 30200, Loss: 1.8384\n",
      "2024-09-17 04:59:16,385 - Epoch: 2/3, Step: 30300, Loss: 1.6136\n",
      "2024-09-17 05:00:04,365 - Epoch: 2/3, Step: 30400, Loss: 1.6633\n",
      "2024-09-17 05:00:52,841 - Epoch: 2/3, Step: 30500, Loss: 2.4421\n",
      "2024-09-17 05:01:40,901 - Epoch: 2/3, Step: 30600, Loss: 1.6592\n",
      "2024-09-17 05:02:27,735 - Epoch: 2/3, Step: 30700, Loss: 2.2209\n",
      "2024-09-17 05:03:17,173 - Epoch: 2/3, Step: 30800, Loss: 1.1485\n",
      "2024-09-17 05:04:05,952 - Epoch: 2/3, Step: 30900, Loss: 1.7452\n",
      "2024-09-17 05:04:54,230 - Epoch: 2/3, Step: 31000, Loss: 1.8147\n",
      "2024-09-17 05:05:42,573 - Epoch: 2/3, Step: 31100, Loss: 1.5482\n",
      "2024-09-17 05:06:30,582 - Epoch: 2/3, Step: 31200, Loss: 1.6043\n",
      "2024-09-17 05:07:21,083 - Epoch: 2/3, Step: 31300, Loss: 2.0565\n",
      "2024-09-17 05:08:08,599 - Epoch: 2/3, Step: 31400, Loss: 2.4152\n",
      "2024-09-17 05:08:56,018 - Epoch: 2/3, Step: 31500, Loss: 1.8291\n",
      "2024-09-17 05:09:43,690 - Epoch: 2/3, Step: 31600, Loss: 1.9375\n",
      "2024-09-17 05:10:31,297 - Epoch: 2/3, Step: 31700, Loss: 1.9538\n",
      "2024-09-17 05:11:21,462 - Epoch: 2/3, Step: 31800, Loss: 1.6119\n",
      "2024-09-17 05:12:08,625 - Epoch: 2/3, Step: 31900, Loss: 1.8115\n",
      "2024-09-17 05:12:55,876 - Epoch: 2/3, Step: 32000, Loss: 1.3778\n",
      "2024-09-17 05:18:50,089 - Validation - Epoch: 2, Loss: 1.8093, Accuracy: 69.6572\n",
      "2024-09-17 05:19:38,843 - Epoch: 2/3, Step: 32100, Loss: 1.9073\n",
      "2024-09-17 05:20:27,881 - Epoch: 2/3, Step: 32200, Loss: 1.9861\n",
      "2024-09-17 05:21:16,664 - Epoch: 2/3, Step: 32300, Loss: 1.7321\n",
      "2024-09-17 05:22:07,418 - Epoch: 2/3, Step: 32400, Loss: 1.3399\n",
      "2024-09-17 05:22:54,731 - Epoch: 2/3, Step: 32500, Loss: 1.6503\n",
      "2024-09-17 05:23:43,515 - Epoch: 2/3, Step: 32600, Loss: 2.2947\n",
      "2024-09-17 05:24:33,000 - Epoch: 2/3, Step: 32700, Loss: 0.8337\n",
      "2024-09-17 05:25:20,904 - Epoch: 2/3, Step: 32800, Loss: 1.8938\n",
      "2024-09-17 05:26:10,694 - Epoch: 2/3, Step: 32900, Loss: 1.4346\n",
      "2024-09-17 05:26:59,160 - Epoch: 2/3, Step: 33000, Loss: 2.3888\n",
      "2024-09-17 05:27:47,743 - Epoch: 2/3, Step: 33100, Loss: 2.0166\n",
      "2024-09-17 05:28:36,303 - Epoch: 2/3, Step: 33200, Loss: 1.9012\n",
      "2024-09-17 05:29:24,680 - Epoch: 2/3, Step: 33300, Loss: 1.7641\n",
      "2024-09-17 05:30:12,673 - Epoch: 2/3, Step: 33400, Loss: 1.5284\n",
      "2024-09-17 05:30:59,857 - Epoch: 2/3, Step: 33500, Loss: 1.4980\n",
      "2024-09-17 05:31:47,040 - Epoch: 2/3, Step: 33600, Loss: 2.0896\n",
      "2024-09-17 05:32:37,388 - Epoch: 2/3, Step: 33700, Loss: 1.7685\n",
      "2024-09-17 05:33:23,245 - Epoch: 2/3, Step: 33800, Loss: 1.5162\n",
      "2024-09-17 05:34:12,823 - Epoch: 2/3, Step: 33900, Loss: 1.8092\n",
      "2024-09-17 05:35:02,451 - Epoch: 2/3, Step: 34000, Loss: 1.8204\n",
      "2024-09-17 05:40:57,394 - Validation - Epoch: 2, Loss: 1.8093, Accuracy: 69.6572\n",
      "2024-09-17 05:41:46,758 - Epoch: 2/3, Step: 34100, Loss: 1.7608\n",
      "2024-09-17 05:42:35,074 - Epoch: 2/3, Step: 34200, Loss: 1.7522\n",
      "2024-09-17 05:43:22,258 - Epoch: 2/3, Step: 34300, Loss: 2.1651\n",
      "2024-09-17 05:44:10,599 - Epoch: 2/3, Step: 34400, Loss: 2.0429\n",
      "2024-09-17 05:44:59,272 - Epoch: 2/3, Step: 34500, Loss: 2.0034\n",
      "2024-09-17 05:45:44,348 - Epoch: 2/3, Step: 34600, Loss: 1.4596\n",
      "2024-09-17 05:46:32,640 - Epoch: 2/3, Step: 34700, Loss: 1.4883\n",
      "2024-09-17 05:47:20,920 - Epoch: 2/3, Step: 34800, Loss: 1.2809\n",
      "2024-09-17 05:48:08,912 - Epoch: 2/3, Step: 34900, Loss: 1.5312\n",
      "2024-09-17 05:48:56,531 - Epoch: 2/3, Step: 35000, Loss: 1.9022\n",
      "2024-09-17 05:49:46,361 - Epoch: 2/3, Step: 35100, Loss: 1.7901\n",
      "2024-09-17 05:50:35,037 - Epoch: 2/3, Step: 35200, Loss: 1.5661\n",
      "2024-09-17 05:51:22,652 - Epoch: 2/3, Step: 35300, Loss: 0.9977\n",
      "2024-09-17 05:52:11,184 - Epoch: 2/3, Step: 35400, Loss: 1.4948\n",
      "2024-09-17 05:53:01,409 - Epoch: 2/3, Step: 35500, Loss: 2.2916\n",
      "2024-09-17 05:53:49,459 - Epoch: 2/3, Step: 35600, Loss: 2.1990\n",
      "2024-09-17 05:54:37,826 - Epoch: 2/3, Step: 35700, Loss: 2.0362\n",
      "2024-09-17 05:55:28,490 - Epoch: 2/3, Step: 35800, Loss: 2.0914\n",
      "2024-09-17 05:56:15,553 - Epoch: 2/3, Step: 35900, Loss: 1.8937\n",
      "2024-09-17 05:57:04,875 - Epoch: 2/3, Step: 36000, Loss: 2.0005\n",
      "2024-09-17 06:02:59,291 - Validation - Epoch: 2, Loss: 1.8093, Accuracy: 69.6572\n",
      "2024-09-17 06:03:46,476 - Epoch: 2/3, Step: 36100, Loss: 2.3793\n",
      "2024-09-17 06:04:35,833 - Epoch: 2/3, Step: 36200, Loss: 1.9514\n",
      "2024-09-17 06:05:24,500 - Epoch: 2/3, Step: 36300, Loss: 1.5126\n",
      "2024-09-17 06:06:12,612 - Epoch: 2/3, Step: 36400, Loss: 1.6025\n",
      "2024-09-17 06:07:00,869 - Epoch: 2/3, Step: 36500, Loss: 2.3931\n",
      "2024-09-17 06:07:49,657 - Epoch: 2/3, Step: 36600, Loss: 1.9517\n",
      "2024-09-17 06:08:38,306 - Epoch: 2/3, Step: 36700, Loss: 1.7162\n",
      "2024-09-17 06:09:27,269 - Epoch: 2/3, Step: 36800, Loss: 1.3748\n",
      "2024-09-17 06:10:14,865 - Epoch: 2/3, Step: 36900, Loss: 2.0102\n",
      "2024-09-17 06:11:02,381 - Epoch: 2/3, Step: 37000, Loss: 2.0706\n",
      "2024-09-17 06:11:51,966 - Epoch: 2/3, Step: 37100, Loss: 1.3071\n",
      "2024-09-17 06:12:40,917 - Epoch: 2/3, Step: 37200, Loss: 1.1953\n",
      "2024-09-17 06:13:30,010 - Epoch: 2/3, Step: 37300, Loss: 2.2115\n",
      "2024-09-17 06:14:19,751 - Epoch: 2/3, Step: 37400, Loss: 2.1052\n",
      "2024-09-17 06:15:06,761 - Epoch: 2/3, Step: 37500, Loss: 1.9169\n",
      "2024-09-17 06:15:53,466 - Epoch: 2/3, Step: 37600, Loss: 2.2200\n",
      "2024-09-17 06:16:42,232 - Epoch: 2/3, Step: 37700, Loss: 1.2635\n",
      "2024-09-17 06:17:31,661 - Epoch: 2/3, Step: 37800, Loss: 1.8698\n",
      "2024-09-17 06:18:22,442 - Epoch: 2/3, Step: 37900, Loss: 1.2520\n",
      "2024-09-17 06:19:09,523 - Epoch: 2/3, Step: 38000, Loss: 2.0163\n",
      "2024-09-17 06:25:03,503 - Validation - Epoch: 2, Loss: 1.8093, Accuracy: 69.6572\n",
      "2024-09-17 06:25:52,146 - Epoch: 2/3, Step: 38100, Loss: 1.5340\n",
      "2024-09-17 06:26:39,137 - Epoch: 3/3, Step: 38200, Loss: 2.3651\n",
      "2024-09-17 06:27:27,768 - Epoch: 3/3, Step: 38300, Loss: 1.5800\n",
      "2024-09-17 06:28:17,768 - Epoch: 3/3, Step: 38400, Loss: 2.4482\n",
      "2024-09-17 06:29:06,261 - Epoch: 3/3, Step: 38500, Loss: 1.9749\n",
      "2024-09-17 06:29:53,818 - Epoch: 3/3, Step: 38600, Loss: 1.6171\n",
      "2024-09-17 06:30:42,170 - Epoch: 3/3, Step: 38700, Loss: 2.0049\n",
      "2024-09-17 06:31:30,018 - Epoch: 3/3, Step: 38800, Loss: 1.7303\n",
      "2024-09-17 06:32:18,767 - Epoch: 3/3, Step: 38900, Loss: 1.8025\n",
      "2024-09-17 06:33:10,898 - Epoch: 3/3, Step: 39000, Loss: 1.5983\n",
      "2024-09-17 06:33:58,075 - Epoch: 3/3, Step: 39100, Loss: 1.6554\n",
      "2024-09-17 06:34:45,854 - Epoch: 3/3, Step: 39200, Loss: 1.0613\n",
      "2024-09-17 06:35:33,392 - Epoch: 3/3, Step: 39300, Loss: 1.9489\n",
      "2024-09-17 06:36:21,170 - Epoch: 3/3, Step: 39400, Loss: 1.6892\n",
      "2024-09-17 06:37:11,868 - Epoch: 3/3, Step: 39500, Loss: 1.9140\n",
      "2024-09-17 06:38:03,341 - Epoch: 3/3, Step: 39600, Loss: 1.6021\n",
      "2024-09-17 06:38:53,035 - Epoch: 3/3, Step: 39700, Loss: 1.9060\n",
      "2024-09-17 06:39:42,440 - Epoch: 3/3, Step: 39800, Loss: 1.2195\n",
      "2024-09-17 06:40:30,196 - Epoch: 3/3, Step: 39900, Loss: 2.2969\n",
      "2024-09-17 06:41:17,443 - Epoch: 3/3, Step: 40000, Loss: 2.0045\n",
      "2024-09-17 06:47:11,012 - Validation - Epoch: 3, Loss: 1.8093, Accuracy: 69.6572\n",
      "2024-09-17 06:47:59,709 - Epoch: 3/3, Step: 40100, Loss: 1.6311\n",
      "2024-09-17 06:48:48,051 - Epoch: 3/3, Step: 40200, Loss: 2.1058\n",
      "2024-09-17 06:49:36,757 - Epoch: 3/3, Step: 40300, Loss: 1.6784\n",
      "2024-09-17 06:50:26,189 - Epoch: 3/3, Step: 40400, Loss: 1.5250\n",
      "2024-09-17 06:51:14,604 - Epoch: 3/3, Step: 40500, Loss: 1.7508\n",
      "2024-09-17 06:52:03,864 - Epoch: 3/3, Step: 40600, Loss: 1.1890\n",
      "2024-09-17 06:52:53,820 - Epoch: 3/3, Step: 40700, Loss: 2.1013\n",
      "2024-09-17 06:53:40,784 - Epoch: 3/3, Step: 40800, Loss: 1.4855\n",
      "2024-09-17 06:54:30,448 - Epoch: 3/3, Step: 40900, Loss: 1.8151\n",
      "2024-09-17 06:55:18,516 - Epoch: 3/3, Step: 41000, Loss: 1.0988\n",
      "2024-09-17 06:56:06,797 - Epoch: 3/3, Step: 41100, Loss: 1.7196\n",
      "2024-09-17 06:56:54,392 - Epoch: 3/3, Step: 41200, Loss: 1.6018\n",
      "2024-09-17 06:57:43,651 - Epoch: 3/3, Step: 41300, Loss: 1.7060\n",
      "2024-09-17 06:58:32,813 - Epoch: 3/3, Step: 41400, Loss: 1.8859\n",
      "2024-09-17 06:59:23,042 - Epoch: 3/3, Step: 41500, Loss: 2.0311\n",
      "2024-09-17 07:00:12,904 - Epoch: 3/3, Step: 41600, Loss: 1.0017\n",
      "2024-09-17 07:01:02,883 - Epoch: 3/3, Step: 41700, Loss: 1.8026\n",
      "2024-09-17 07:01:50,350 - Epoch: 3/3, Step: 41800, Loss: 1.8788\n",
      "2024-09-17 07:02:39,494 - Epoch: 3/3, Step: 41900, Loss: 1.4150\n",
      "2024-09-17 07:03:27,616 - Epoch: 3/3, Step: 42000, Loss: 1.9221\n",
      "2024-09-17 07:09:22,255 - Validation - Epoch: 3, Loss: 1.8093, Accuracy: 69.6572\n",
      "2024-09-17 07:10:10,973 - Epoch: 3/3, Step: 42100, Loss: 1.9951\n",
      "2024-09-17 07:10:59,571 - Epoch: 3/3, Step: 42200, Loss: 2.2032\n",
      "2024-09-17 07:11:46,994 - Epoch: 3/3, Step: 42300, Loss: 1.7861\n",
      "2024-09-17 07:12:33,418 - Epoch: 3/3, Step: 42400, Loss: 1.9584\n",
      "2024-09-17 07:13:24,201 - Epoch: 3/3, Step: 42500, Loss: 1.8645\n",
      "2024-09-17 07:14:11,577 - Epoch: 3/3, Step: 42600, Loss: 1.8570\n",
      "2024-09-17 07:15:01,589 - Epoch: 3/3, Step: 42700, Loss: 1.4416\n",
      "2024-09-17 07:15:50,006 - Epoch: 3/3, Step: 42800, Loss: 1.4540\n",
      "2024-09-17 07:16:37,598 - Epoch: 3/3, Step: 42900, Loss: 2.1129\n",
      "2024-09-17 07:17:26,627 - Epoch: 3/3, Step: 43000, Loss: 1.4543\n",
      "2024-09-17 07:18:16,967 - Epoch: 3/3, Step: 43100, Loss: 1.7460\n",
      "2024-09-17 07:19:05,021 - Epoch: 3/3, Step: 43200, Loss: 1.7605\n",
      "2024-09-17 07:19:52,655 - Epoch: 3/3, Step: 43300, Loss: 1.6006\n",
      "2024-09-17 07:20:41,243 - Epoch: 3/3, Step: 43400, Loss: 2.0512\n",
      "2024-09-17 07:21:32,076 - Epoch: 3/3, Step: 43500, Loss: 1.9748\n",
      "2024-09-17 07:22:21,692 - Epoch: 3/3, Step: 43600, Loss: 1.8704\n",
      "2024-09-17 07:23:07,353 - Epoch: 3/3, Step: 43700, Loss: 1.5728\n",
      "2024-09-17 07:23:54,384 - Epoch: 3/3, Step: 43800, Loss: 1.6554\n",
      "2024-09-17 07:24:42,322 - Epoch: 3/3, Step: 43900, Loss: 1.8164\n",
      "2024-09-17 07:25:33,350 - Epoch: 3/3, Step: 44000, Loss: 1.7591\n",
      "2024-09-17 07:31:27,617 - Validation - Epoch: 3, Loss: 1.8093, Accuracy: 69.6572\n",
      "2024-09-17 07:32:16,034 - Epoch: 3/3, Step: 44100, Loss: 1.5884\n",
      "2024-09-17 07:33:03,243 - Epoch: 3/3, Step: 44200, Loss: 1.8704\n",
      "2024-09-17 07:33:54,888 - Epoch: 3/3, Step: 44300, Loss: 1.6784\n",
      "2024-09-17 07:34:42,687 - Epoch: 3/3, Step: 44400, Loss: 1.6049\n",
      "2024-09-17 07:35:29,368 - Epoch: 3/3, Step: 44500, Loss: 1.5891\n",
      "2024-09-17 07:36:18,022 - Epoch: 3/3, Step: 44600, Loss: 1.3050\n",
      "2024-09-17 07:37:08,649 - Epoch: 3/3, Step: 44700, Loss: 2.1977\n",
      "2024-09-17 07:37:57,548 - Epoch: 3/3, Step: 44800, Loss: 1.7835\n",
      "2024-09-17 07:38:45,393 - Epoch: 3/3, Step: 44900, Loss: 2.1028\n",
      "2024-09-17 07:39:33,601 - Epoch: 3/3, Step: 45000, Loss: 2.4268\n",
      "2024-09-17 07:40:22,503 - Epoch: 3/3, Step: 45100, Loss: 1.4525\n",
      "2024-09-17 07:41:11,070 - Epoch: 3/3, Step: 45200, Loss: 1.6845\n",
      "2024-09-17 07:42:01,778 - Epoch: 3/3, Step: 45300, Loss: 1.4880\n",
      "2024-09-17 07:42:48,992 - Epoch: 3/3, Step: 45400, Loss: 1.7682\n",
      "2024-09-17 07:43:36,602 - Epoch: 3/3, Step: 45500, Loss: 2.0219\n",
      "2024-09-17 07:44:25,718 - Epoch: 3/3, Step: 45600, Loss: 1.7905\n",
      "2024-09-17 07:45:15,651 - Epoch: 3/3, Step: 45700, Loss: 0.7102\n",
      "2024-09-17 07:46:04,006 - Epoch: 3/3, Step: 45800, Loss: 2.1341\n",
      "2024-09-17 07:46:53,677 - Epoch: 3/3, Step: 45900, Loss: 1.5190\n",
      "2024-09-17 07:47:41,894 - Epoch: 3/3, Step: 46000, Loss: 2.1090\n",
      "2024-09-17 07:53:35,993 - Validation - Epoch: 3, Loss: 1.8093, Accuracy: 69.6572\n",
      "2024-09-17 07:54:25,255 - Epoch: 3/3, Step: 46100, Loss: 1.7241\n",
      "2024-09-17 07:55:12,837 - Epoch: 3/3, Step: 46200, Loss: 2.0733\n",
      "2024-09-17 07:56:00,647 - Epoch: 3/3, Step: 46300, Loss: 2.0972\n",
      "2024-09-17 07:56:51,992 - Epoch: 3/3, Step: 46400, Loss: 1.4371\n",
      "2024-09-17 07:57:41,206 - Epoch: 3/3, Step: 46500, Loss: 1.9441\n",
      "2024-09-17 07:58:31,042 - Epoch: 3/3, Step: 46600, Loss: 2.1079\n",
      "2024-09-17 07:59:17,636 - Epoch: 3/3, Step: 46700, Loss: 2.0714\n",
      "2024-09-17 08:00:06,510 - Epoch: 3/3, Step: 46800, Loss: 1.8863\n",
      "2024-09-17 08:00:52,181 - Epoch: 3/3, Step: 46900, Loss: 1.7574\n",
      "2024-09-17 08:01:40,334 - Epoch: 3/3, Step: 47000, Loss: 2.0126\n",
      "2024-09-17 08:02:29,546 - Epoch: 3/3, Step: 47100, Loss: 2.1019\n",
      "2024-09-17 08:03:17,036 - Epoch: 3/3, Step: 47200, Loss: 1.8869\n",
      "2024-09-17 08:04:06,782 - Epoch: 3/3, Step: 47300, Loss: 1.8952\n",
      "2024-09-17 08:04:54,661 - Epoch: 3/3, Step: 47400, Loss: 1.2060\n",
      "2024-09-17 08:05:43,344 - Epoch: 3/3, Step: 47500, Loss: 1.4412\n",
      "2024-09-17 08:06:33,969 - Epoch: 3/3, Step: 47600, Loss: 1.8285\n",
      "2024-09-17 08:07:21,747 - Epoch: 3/3, Step: 47700, Loss: 1.8398\n",
      "2024-09-17 08:08:11,333 - Epoch: 3/3, Step: 47800, Loss: 1.6856\n",
      "2024-09-17 08:08:58,620 - Epoch: 3/3, Step: 47900, Loss: 2.2877\n",
      "2024-09-17 08:09:47,802 - Epoch: 3/3, Step: 48000, Loss: 1.5494\n",
      "2024-09-17 08:15:42,436 - Validation - Epoch: 3, Loss: 1.8093, Accuracy: 69.6572\n",
      "2024-09-17 08:16:30,784 - Epoch: 3/3, Step: 48100, Loss: 1.7292\n",
      "2024-09-17 08:17:18,089 - Epoch: 3/3, Step: 48200, Loss: 2.3045\n",
      "2024-09-17 08:18:04,790 - Epoch: 3/3, Step: 48300, Loss: 1.4841\n",
      "2024-09-17 08:18:52,366 - Epoch: 3/3, Step: 48400, Loss: 1.7326\n",
      "2024-09-17 08:19:40,415 - Epoch: 3/3, Step: 48500, Loss: 1.6439\n",
      "2024-09-17 08:20:29,790 - Epoch: 3/3, Step: 48600, Loss: 1.5020\n",
      "2024-09-17 08:21:17,184 - Epoch: 3/3, Step: 48700, Loss: 1.5024\n",
      "2024-09-17 08:22:05,722 - Epoch: 3/3, Step: 48800, Loss: 1.6635\n",
      "2024-09-17 08:22:54,220 - Epoch: 3/3, Step: 48900, Loss: 1.6992\n",
      "2024-09-17 08:23:42,416 - Epoch: 3/3, Step: 49000, Loss: 1.5219\n",
      "2024-09-17 08:24:30,491 - Epoch: 3/3, Step: 49100, Loss: 1.3014\n",
      "2024-09-17 08:25:17,678 - Epoch: 3/3, Step: 49200, Loss: 1.6949\n",
      "2024-09-17 08:26:07,157 - Epoch: 3/3, Step: 49300, Loss: 1.7916\n",
      "2024-09-17 08:26:53,581 - Epoch: 3/3, Step: 49400, Loss: 2.1026\n",
      "2024-09-17 08:27:42,030 - Epoch: 3/3, Step: 49500, Loss: 1.8616\n",
      "2024-09-17 08:28:30,055 - Epoch: 3/3, Step: 49600, Loss: 1.9954\n",
      "2024-09-17 08:29:16,729 - Epoch: 3/3, Step: 49700, Loss: 2.0741\n",
      "2024-09-17 08:30:03,187 - Epoch: 3/3, Step: 49800, Loss: 1.4735\n",
      "2024-09-17 08:30:51,402 - Epoch: 3/3, Step: 49900, Loss: 1.6005\n",
      "2024-09-17 08:31:40,038 - Epoch: 3/3, Step: 50000, Loss: 1.4019\n",
      "2024-09-17 08:37:33,922 - Validation - Epoch: 3, Loss: 1.8093, Accuracy: 69.6572\n",
      "2024-09-17 08:38:23,662 - Epoch: 3/3, Step: 50100, Loss: 2.1499\n",
      "2024-09-17 08:39:12,418 - Epoch: 3/3, Step: 50200, Loss: 1.6082\n",
      "2024-09-17 08:40:00,605 - Epoch: 3/3, Step: 50300, Loss: 2.3926\n",
      "2024-09-17 08:40:48,668 - Epoch: 3/3, Step: 50400, Loss: 1.9443\n",
      "2024-09-17 08:41:38,674 - Epoch: 3/3, Step: 50500, Loss: 1.2889\n",
      "2024-09-17 08:42:28,482 - Epoch: 3/3, Step: 50600, Loss: 1.6229\n",
      "2024-09-17 08:43:18,798 - Epoch: 3/3, Step: 50700, Loss: 2.1609\n",
      "2024-09-17 08:44:07,405 - Epoch: 3/3, Step: 50800, Loss: 2.0275\n",
      "2024-09-17 08:44:55,554 - Epoch: 3/3, Step: 50900, Loss: 1.8498\n",
      "2024-09-17 08:45:40,941 - Epoch: 3/3, Step: 51000, Loss: 1.7548\n",
      "2024-09-17 08:46:31,499 - Epoch: 3/3, Step: 51100, Loss: 1.6705\n",
      "2024-09-17 08:47:18,236 - Epoch: 3/3, Step: 51200, Loss: 1.8790\n",
      "2024-09-17 08:48:05,904 - Epoch: 3/3, Step: 51300, Loss: 2.0589\n",
      "2024-09-17 08:48:52,691 - Epoch: 3/3, Step: 51400, Loss: 2.5009\n",
      "2024-09-17 08:49:39,149 - Epoch: 3/3, Step: 51500, Loss: 1.9980\n",
      "2024-09-17 08:50:26,766 - Epoch: 3/3, Step: 51600, Loss: 1.1724\n",
      "2024-09-17 08:51:14,488 - Epoch: 3/3, Step: 51700, Loss: 1.7645\n",
      "2024-09-17 08:52:02,492 - Epoch: 3/3, Step: 51800, Loss: 1.7079\n",
      "2024-09-17 08:52:51,076 - Epoch: 3/3, Step: 51900, Loss: 2.0481\n",
      "2024-09-17 08:53:39,719 - Epoch: 3/3, Step: 52000, Loss: 1.0208\n",
      "2024-09-17 08:59:33,687 - Validation - Epoch: 3, Loss: 1.8093, Accuracy: 69.6572\n",
      "2024-09-17 09:00:22,669 - Epoch: 3/3, Step: 52100, Loss: 1.7594\n",
      "2024-09-17 09:01:11,594 - Epoch: 3/3, Step: 52200, Loss: 1.8968\n",
      "2024-09-17 09:01:59,854 - Epoch: 3/3, Step: 52300, Loss: 1.7559\n",
      "2024-09-17 09:02:48,351 - Epoch: 3/3, Step: 52400, Loss: 1.1254\n",
      "2024-09-17 09:03:37,372 - Epoch: 3/3, Step: 52500, Loss: 1.9148\n",
      "2024-09-17 09:04:23,700 - Epoch: 3/3, Step: 52600, Loss: 2.2428\n",
      "2024-09-17 09:05:10,750 - Epoch: 3/3, Step: 52700, Loss: 1.9692\n",
      "2024-09-17 09:05:57,970 - Epoch: 3/3, Step: 52800, Loss: 2.5476\n",
      "2024-09-17 09:06:46,795 - Epoch: 3/3, Step: 52900, Loss: 2.0720\n",
      "2024-09-17 09:07:36,882 - Epoch: 3/3, Step: 53000, Loss: 1.2800\n",
      "2024-09-17 09:08:24,150 - Epoch: 3/3, Step: 53100, Loss: 2.2038\n",
      "2024-09-17 09:09:13,666 - Epoch: 3/3, Step: 53200, Loss: 1.0539\n",
      "2024-09-17 09:10:03,097 - Epoch: 3/3, Step: 53300, Loss: 1.7518\n",
      "2024-09-17 09:10:51,436 - Epoch: 3/3, Step: 53400, Loss: 1.8052\n",
      "2024-09-17 09:11:40,876 - Epoch: 3/3, Step: 53500, Loss: 1.9691\n",
      "2024-09-17 09:12:29,594 - Epoch: 3/3, Step: 53600, Loss: 1.3696\n",
      "2024-09-17 09:13:17,653 - Epoch: 3/3, Step: 53700, Loss: 1.6184\n",
      "2024-09-17 09:14:06,740 - Epoch: 3/3, Step: 53800, Loss: 2.0239\n",
      "2024-09-17 09:14:53,427 - Epoch: 3/3, Step: 53900, Loss: 1.9797\n",
      "2024-09-17 09:15:41,500 - Epoch: 3/3, Step: 54000, Loss: 2.1840\n",
      "2024-09-17 09:21:34,970 - Validation - Epoch: 3, Loss: 1.8093, Accuracy: 69.6572\n",
      "2024-09-17 09:22:23,957 - Epoch: 3/3, Step: 54100, Loss: 1.7077\n",
      "2024-09-17 09:23:13,493 - Epoch: 3/3, Step: 54200, Loss: 0.9553\n",
      "2024-09-17 09:24:03,023 - Epoch: 3/3, Step: 54300, Loss: 1.6192\n",
      "2024-09-17 09:24:51,328 - Epoch: 3/3, Step: 54400, Loss: 1.7918\n",
      "2024-09-17 09:25:37,665 - Epoch: 3/3, Step: 54500, Loss: 1.7783\n",
      "2024-09-17 09:26:26,639 - Epoch: 3/3, Step: 54600, Loss: 2.0494\n",
      "2024-09-17 09:27:15,335 - Epoch: 3/3, Step: 54700, Loss: 2.2822\n",
      "2024-09-17 09:28:05,437 - Epoch: 3/3, Step: 54800, Loss: 1.8586\n",
      "2024-09-17 09:28:52,619 - Epoch: 3/3, Step: 54900, Loss: 1.7282\n",
      "2024-09-17 09:29:43,366 - Epoch: 3/3, Step: 55000, Loss: 1.7347\n",
      "2024-09-17 09:30:31,452 - Epoch: 3/3, Step: 55100, Loss: 1.8076\n",
      "2024-09-17 09:31:18,499 - Epoch: 3/3, Step: 55200, Loss: 2.2405\n",
      "2024-09-17 09:32:07,784 - Epoch: 3/3, Step: 55300, Loss: 1.6976\n",
      "2024-09-17 09:32:56,886 - Epoch: 3/3, Step: 55400, Loss: 2.0506\n",
      "2024-09-17 09:33:44,700 - Epoch: 3/3, Step: 55500, Loss: 1.2733\n",
      "2024-09-17 09:34:32,067 - Epoch: 3/3, Step: 55600, Loss: 1.8889\n",
      "2024-09-17 09:35:20,551 - Epoch: 3/3, Step: 55700, Loss: 2.1878\n",
      "2024-09-17 09:36:10,377 - Epoch: 3/3, Step: 55800, Loss: 1.6893\n",
      "2024-09-17 09:36:58,755 - Epoch: 3/3, Step: 55900, Loss: 2.2322\n",
      "2024-09-17 09:37:45,316 - Epoch: 3/3, Step: 56000, Loss: 1.2596\n",
      "2024-09-17 09:43:37,344 - Validation - Epoch: 3, Loss: 1.8093, Accuracy: 69.6572\n",
      "2024-09-17 09:44:24,893 - Epoch: 3/3, Step: 56100, Loss: 1.2381\n",
      "2024-09-17 09:45:11,296 - Epoch: 3/3, Step: 56200, Loss: 1.6474\n",
      "2024-09-17 09:45:59,053 - Epoch: 3/3, Step: 56300, Loss: 2.1132\n",
      "2024-09-17 09:46:47,079 - Epoch: 3/3, Step: 56400, Loss: 1.4998\n",
      "2024-09-17 09:47:33,904 - Epoch: 3/3, Step: 56500, Loss: 1.7350\n",
      "2024-09-17 09:48:23,038 - Epoch: 3/3, Step: 56600, Loss: 1.4918\n",
      "2024-09-17 09:49:08,187 - Epoch: 3/3, Step: 56700, Loss: 1.8206\n",
      "2024-09-17 09:49:56,097 - Epoch: 3/3, Step: 56800, Loss: 2.1047\n",
      "2024-09-17 09:50:45,438 - Epoch: 3/3, Step: 56900, Loss: 1.7124\n",
      "2024-09-17 09:51:32,092 - Epoch: 3/3, Step: 57000, Loss: 1.4004\n",
      "2024-09-17 09:52:22,207 - Epoch: 3/3, Step: 57100, Loss: 1.4986\n",
      "2024-09-17 09:53:09,985 - Epoch: 3/3, Step: 57200, Loss: 1.9200\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# 设置日志记录器\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "\n",
    "# 优化器和调度器\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=2000, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 训练参数\n",
    "num_epochs = 3\n",
    "n_steps = 0\n",
    "log_interval = 100\n",
    "eval_interval = 2000\n",
    "grad_clip_value = 1.0  # 梯度裁剪阈值\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    for batch in train_dataloader:\n",
    "        # 将输入和目标数据迁移到设备\n",
    "        src = batch[\"src_input_ids\"].to(device)\n",
    "        src_attention_mask = batch[\"src_attention_mask\"].to(device)\n",
    "        tgt = batch[\"tgt_input_ids\"].to(device)\n",
    "        tgt_attention_mask = batch[\"tgt_attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # 向前传播\n",
    "        logits = model(src, tgt, src_attention_mask, tgt_attention_mask).transpose(1, 2)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # 反向传播和梯度更新\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_value)  # 梯度裁剪\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        n_steps += 1\n",
    "\n",
    "        if n_steps % log_interval == 0:\n",
    "            logging.info(\n",
    "                f\"Epoch: {epoch + 1}/{num_epochs}, Step: {n_steps}, Loss: {loss.item():.4f}\"\n",
    "            )\n",
    "        if n_steps % eval_interval == 0:\n",
    "            val_loss, val_accuracy = evaluate_model(\n",
    "                model, validation_dataloader, criterion, device\n",
    "            )\n",
    "            logging.info(\n",
    "                f\"Validation - Epoch: {epoch + 1}, Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\"\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
