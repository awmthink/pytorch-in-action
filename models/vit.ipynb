{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT)\n",
    "\n",
    "ViT æ¨¡å‹æ˜¯ç”± Google Brain å›¢é˜Ÿåœ¨ 2021 ICLRä¸Šå‘è¡¨çš„ [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) ä¸­æå‡ºã€‚è¿™æ˜¯ç¬¬ä¸€ç¯‡æˆåŠŸåœ¨ ImageNet ä¸Šè®­ç»ƒ Transformer ç¼–ç å™¨çš„è®ºæ–‡ï¼Œä¸å¸¸è§çš„å·ç§¯æ¶æ„ç›¸æ¯”ï¼Œå–å¾—äº†éå¸¸å¥½çš„æ•ˆæœã€‚\n",
    "\n",
    "**è®ºæ–‡æ‘˜è¦**ï¼šå°½ç®¡ Transformer æ¶æ„å·²æˆä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„é»˜è®¤æ ‡å‡†ï¼Œä½†å…¶åº”ç”¨äºè®¡ç®—æœºè§†è§‰çš„é¢†åŸŸä»ç„¶æœ‰é™ã€‚åœ¨è§†è§‰é¢†åŸŸï¼Œæ³¨æ„åŠ›æœºåˆ¶è¦ä¹ˆä¸å·ç§¯ç¥ç»ç½‘ç»œç»“åˆä½¿ç”¨ï¼Œè¦ä¹ˆç”¨äºæ›¿æ¢å·ç§¯ç½‘ç»œä¸­çš„æŸäº›ç»„ä»¶ï¼ŒåŒæ—¶ä¿æŒå…¶æ•´ä½“ç»“æ„ä¸å˜ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œè¿™ç§å¯¹ CNN çš„ä¾èµ–å¹¶éå¿…è¦ï¼Œç›´æ¥åº”ç”¨äºå›¾åƒå—åºåˆ—çš„çº¯ Transformer åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šå¯ä»¥è¡¨ç°å‡ºè‰²ã€‚å½“åœ¨å¤§é‡æ•°æ®ä¸Šé¢„è®­ç»ƒå¹¶åœ¨å¤šä¸ªä¸­ç­‰è§„æ¨¡æˆ–å°å‹å›¾åƒè¯†åˆ«åŸºå‡†ï¼ˆImageNetã€CIFAR-100ã€VTAB ç­‰ï¼‰ä¸Šè¿›è¡Œè¿ç§»å­¦ä¹ æ—¶ï¼Œä¸æœ€å…ˆè¿›çš„å·ç§¯ç½‘ç»œç›¸æ¯”ï¼Œè§†è§‰ Transformerï¼ˆViTï¼‰å–å¾—äº†ä¼˜å¼‚çš„æˆç»©ï¼ŒåŒæ—¶è®­ç»ƒæ‰€éœ€çš„è®¡ç®—èµ„æºå¤§å¹…å‡å°‘ã€‚\n",
    "\n",
    "![](../images/vit_2024-10-27-11-58-39.png)\n",
    "\n",
    "ä»ä¸Šå›¾ä¸­å¯ä»¥çœ‹å‡ºæ¥ï¼Œæ•´ä¸ª ViT çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œå°†æ•´ä¸ªå›¾åƒåˆ’åˆ†æˆç½‘æ ¼ï¼Œæ¯ä¸ªç½‘ç»œæˆ‘ä»¬ç§°ä¸ºä¸€ä¸ªå›¾åƒ Patchï¼Œå°†è¿™äº› Patch é“ºå¹³ä¸ºä¸€ç»´çš„ï¼Œé‚£ä¹ˆæ•´ä¸ª2D çš„å›¾åƒå°±å¯ä»¥çœ‹æˆæ˜¯ä¸€ä¸ªç”± Patches ç»„æˆçš„ä¸€ä¸ªåºåˆ—è¾“å‡ºã€‚ä¸€æ—¦å›¾åƒè½¬æ¢ä¸ºäº†åºåˆ—å‘é‡çš„å½¢å¼ï¼Œé‚£ä¹ˆæ•´ä¸ªæ¨¡å‹çš„ä¸»å¹²å°±å¯ä»¥ä½¿ç”¨ä¸€ä¸ª TransformerBlock ç»„æˆçš„å¤šå±‚ Transformer Encoder ç»“æ„ã€‚\n",
    "\n",
    "è®ºæ–‡ä¸­å®ç°äº†ä¸‰ç§ä¸åŒå¤§å°çš„æ¨¡å‹ç»“æ„ï¼š\n",
    "\n",
    "|Model|Layers|Hidden size $D$| MLP size | Heads | Params |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|ViT-Base|12|768|3072|12|86M|\n",
    "|ViT-Large|24|1024|4096|16|307M|\n",
    "|ViT-Huge|32|1280|5120|16|632M|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å…³é”®æ¨¡å—çš„ä»é›¶å®ç°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PatchEmbedding\n",
    "\n",
    "ViT çš„æ ¸å¿ƒå°±æ˜¯æŠŠä¸€å¹…å›¾åƒçœ‹æˆæ˜¯ä¸€ä¸ªè‹¥å¹²ä¸ª 16x16 çš„å›¾åƒ Patchesï¼Œæ¯ä¸ªå›¾åƒ patch å°±çœ‹ä½œæ˜¯ NLP ä¸­çš„ä¸€ä¸ª wordã€‚æ‰€ä»¥ ViT æ¨¡å‹çš„ç¬¬ä¸€æ­¥å°±æ˜¯æŠŠå›¾åƒåˆ’åˆ†ä¸º Patchï¼Œæœ‰ä¸¤ç§ä¸»æµçš„å®ç°æ–¹å¼ï¼š\n",
    "\n",
    "1. ä¸€ç§æ˜¯é€šè¿‡ Reshape + Permute å°†å›¾åƒè½¬æ¢ä¸º Patchï¼Œç„¶åç»è¿‡ä¸€ä¸ª MLP è¿›è¡Œç»´åº¦å˜æ¢ã€‚\n",
    "2. å¦ä¸€ç§æ˜¯ç›´æ¥é‡‡ç”¨ä¸€ä¸ªå·ç§¯çš„æ»‘åŠ¨çª—å£ï¼Œæ»‘åŠ¨çš„æ­¥é•¿ç­‰äºçª—å£çš„å¤§å°ã€‚\n",
    "\n",
    "åœ¨ Transformers åº“ä¸­ï¼Œå®ƒæ˜¯ç»è¿‡ transformers.models.vit ä¸­çš„`ViTEmbeddings` å®ç°ï¼Œå†…éƒ¨å®é™…æ˜¯å°±æ˜¯é€šè¿‡ `Conv2d`æ¥å®ç°çš„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "batch_size = 1\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "in_chans = 3\n",
    "embed_dim = 1024\n",
    "\n",
    "img = torch.randn(batch_size, in_chans, img_size, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196, 1024])\n"
     ]
    }
   ],
   "source": [
    "# å®ç°æ–¹æ¡ˆä¸€ï¼š\n",
    "proj = nn.Conv2d(in_chans, embed_dim, patch_size, patch_size)\n",
    "x = proj(img)  # batch_size, embed_dim, h_patches, w_patches\n",
    "x = x.flatten(2).transpose(1, 2)  # NCHW -> NLC\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196, 1024])\n"
     ]
    }
   ],
   "source": [
    "# å®ç°æ–¹æ¡ˆäºŒï¼š\n",
    "patch_h = img.shape[2] // patch_size\n",
    "patch_w = img.shape[3] // patch_size\n",
    "x = img.reshape(batch_size, in_chans, patch_h, patch_size, patch_w, patch_size)\n",
    "x = x.permute(0, 2, 4, 3, 5, 1).reshape(\n",
    "    batch_size, -1, patch_size * patch_size * in_chans\n",
    ")\n",
    "\n",
    "proj = nn.Linear(patch_size * patch_size * in_chans, embed_dim)\n",
    "x = proj(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position Embedding\n",
    "\n",
    "ViT ä¸­ä½¿ç”¨çš„æ˜¯ä¸€ç»´çš„å¯è®­ç»ƒçš„ç»å¯¹ä½ç½®ç¼–ç ï¼Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_patches = x.shape[1]\n",
    "# è€ƒè™‘ä¸Š CLS Tokenï¼Œæ‰€ä»¥é•¿åº¦ä¸º num_pathces + 1\n",
    "position_embeddings = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ­£å¸¸æƒ…å†µä¸‹ï¼Œå¦‚æœè¾“å…¥å›¾åƒçš„å°ºå¯¸æ­£å¥½å’Œé¢„å®šä¹‰çš„å›¾åƒå°ºå¯¸æ˜¯ä¸€è‡´çš„ï¼Œé‚£ä¹ˆå°±ç›´æ¥å°† position_embeddings åŠ åˆ° patch embedding ä¸­å»ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "\n",
    "embeddings = torch.concat([cls_tokens, x], dim=1)  # æ·»åŠ  CLS Token\n",
    "embeddings = embeddings + position_embeddings  # æ·»åŠ ä½ç½®ç¼–ç ä¿¡æ¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœè¾“å…¥çš„å›¾åƒçš„å°ºå¯¸å’Œé¢„å®šä¹‰çš„å°ºå¯¸ä¸ä¸€è‡´ï¼Œè¿™æ—¶å€™ï¼Œæˆ‘ä»¬çš„ä½ç½®ç¼–ç çŸ©é˜µå°±éœ€è¦è¿›è¡Œæ’å€¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.randn(batch_size, in_chans, 384, 384)\n",
    "\n",
    "proj = nn.Conv2d(in_chans, embed_dim, patch_size, patch_size)\n",
    "x = proj(img)\n",
    "x = x.flatten(2).transpose(1, 2)\n",
    "embeddings = torch.concat([cls_tokens, x], dim=1)  # æ·»åŠ  CLS Token\n",
    "\n",
    "height = img.shape[2]\n",
    "width = img.shape[3]\n",
    "new_height = height // patch_size\n",
    "new_width = width // patch_size\n",
    "\n",
    "cls_pos_embedding = position_embeddings[:, :1, :]\n",
    "patch_pos_embedding = position_embeddings[:, 1:, :]\n",
    "\n",
    "patch_pos_embedding = patch_pos_embedding.reshape(1, patch_h, patch_w, embed_dim)\n",
    "patch_pos_embedding = patch_pos_embedding.permute(0, 3, 1, 2)  # NHWC -> NCHW\n",
    "\n",
    "patch_pos_embedding = nn.functional.interpolate(\n",
    "    patch_pos_embedding,\n",
    "    size=(new_height, new_width),\n",
    "    mode=\"bicubic\",\n",
    "    align_corners=False,\n",
    ")\n",
    "\n",
    "patch_pos_embedding = patch_pos_embedding.permute(0, 2, 3, 1).reshape(1, -1, embed_dim)\n",
    "position_embeddings = torch.concat([cls_pos_embedding, patch_pos_embedding], dim=1)\n",
    "\n",
    "embeddings = embeddings + position_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨ ğŸ¤— Transformers åº“\n",
    "\n",
    "Transformers ä¸­å®ç°äº† ViT æ¶æ„çš„æ¨¡å‹ï¼Œç±»åä¸º `ViTModel`ï¼Œæˆ‘ä»¬é€šè¿‡ä¸‹é¢æ„é€ å‡½æ•°çš„ä»£ç ï¼Œå¯ä»¥çœ‹å‡ºï¼Œæ•´ä¸ª ViTModel ç”±ä¸‰ä¸ªéƒ¨åˆ†ç»„æˆï¼š\n",
    "\n",
    "1. ViTEmbeddings: å›¾åƒ Patch åŒ–çš„é¢„å¤„ç†\n",
    "2. ViTEncoder: Transformer Encoder\n",
    "3. ViTPooler: åˆ†ç±»å¤´\n",
    "\n",
    "```python\n",
    "\n",
    "class ViTModel(nn.Moduel):\n",
    "    def __init__(self, config: ViTConfig, add_pooling_layer: bool = True, use_mask_token: bool = False):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = ViTEmbeddings(config, use_mask_token=use_mask_token)\n",
    "        self.encoder = ViTEncoder(config)\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.pooler = ViTPooler(config) if add_pooling_layer else None\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 577, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "from timm.models import vision_transformer\n",
    "from transformers import ViTModel, ViTConfig\n",
    "import torch\n",
    "\n",
    "config = ViTConfig()\n",
    "\n",
    "vit_model = ViTModel(config)\n",
    "\n",
    "img = torch.randn(1, 3, 384, 384)\n",
    "vit_output = vit_model(pixel_values=img, interpolate_pos_encoding=True)\n",
    "print(vit_output.last_hidden_state.shape)\n",
    "print(vit_output.pooler_output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
