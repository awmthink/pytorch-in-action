{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP\n",
    "\n",
    "CLIP æ¨¡å‹æ˜¯åœ¨ 2021 å¹´ OpenAI åœ¨ [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) æå‡ºã€‚CLIPï¼ˆContrastive Language-Image Pre-Trainingï¼‰æ˜¯ä¸€ç§åœ¨å¤§é‡çš„å›¾åƒï¼Œæ–‡æœ¬æ•°æ®å¯¹ä¸Šè®­ç»ƒçš„ç¥ç»ç½‘ç»œã€‚å¯¹äºç»™å®šçš„å›¾ç‰‡ï¼ŒCLIP å¯ä»¥ç›´æ¥é¢„æµ‹å’Œè¯¥å›¾ç‰‡æœ€ç›¸å…³çš„æ–‡æœ¬æè¿°ï¼Œè¿™æ ·çš„èƒ½åŠ›èµ‹äºˆ CLIP é›¶æ ·æœ¬çš„èƒ½åŠ›ã€‚\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**æ‘˜è¦ï¼š**\n",
    "\n",
    "å½“å‰æœ€å…ˆè¿›çš„è®¡ç®—æœºè§†è§‰ç³»ç»Ÿé€šå¸¸è¢«è®­ç»ƒç”¨äºé¢„æµ‹ä¸€ç»„å›ºå®šçš„é¢„è®¾ç‰©ä½“ç±»åˆ«ã€‚è¿™ç§å—é™çš„ç›‘ç£å½¢å¼é™åˆ¶äº†æ¨¡å‹çš„é€šç”¨æ€§å’Œå¯ç”¨æ€§ï¼Œå› ä¸ºéœ€è¦é¢å¤–æ ‡æ³¨çš„æ•°æ®æ¥å®šä¹‰å…¶ä»–è§†è§‰æ¦‚å¿µã€‚è€Œç›´æ¥ä»ä¸å›¾åƒç›¸å…³çš„åŸå§‹æ–‡æœ¬ä¸­å­¦ä¹ æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ³•ï¼Œèƒ½å¤Ÿåˆ©ç”¨æ›´å¹¿æ³›çš„ç›‘ç£æ¥æºã€‚æˆ‘ä»¬å±•ç¤ºäº†ä¸€ç§ç®€å•ä½†æœ‰æ•ˆçš„é¢„è®­ç»ƒä»»åŠ¡ï¼šé¢„æµ‹å“ªæ®µæ–‡å­—ä¸å“ªå¼ å›¾åƒåŒ¹é…ã€‚é€šè¿‡åœ¨äº’è”ç½‘ä¸Šæ”¶é›†çš„4äº¿ï¼ˆå›¾åƒï¼Œæ–‡æœ¬ï¼‰å¯¹æ•°æ®é›†ä¸Šä»é›¶å¼€å§‹è¿›è¡Œè®­ç»ƒï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿé«˜æ•ˆã€å¯æ‰©å±•åœ°å­¦ä¹ æœ€å…ˆè¿›çš„å›¾åƒè¡¨ç¤ºã€‚é¢„è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€å¼•ç”¨å­¦ä¹ åˆ°çš„è§†è§‰æ¦‚å¿µï¼ˆæˆ–æè¿°æ–°çš„æ¦‚å¿µï¼‰ï¼Œä»è€Œå®ç°æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„é›¶æ ·æœ¬è¿ç§»ã€‚\n",
    "\n",
    "æˆ‘ä»¬é€šè¿‡åœ¨è¶…è¿‡ 30 ä¸ªç°æœ‰è®¡ç®—æœºè§†è§‰æ•°æ®é›†ä¸Šçš„åŸºå‡†æµ‹è¯•ï¼Œç ”ç©¶äº†è¿™ç§æ–¹æ³•çš„æ€§èƒ½ï¼Œæ¶µç›–ä»»åŠ¡åŒ…æ‹¬OCRï¼ˆå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼‰ã€è§†é¢‘ä¸­çš„åŠ¨ä½œè¯†åˆ«ã€åœ°ç†å®šä½ä»¥åŠå¤šç§ç»†ç²’åº¦ç‰©ä½“åˆ†ç±»ã€‚æ¨¡å‹åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸­çš„è¿ç§»æ€§èƒ½æ˜¾è‘—ï¼Œä¸”é€šå¸¸èƒ½å¤Ÿåœ¨æ— éœ€ä»»ä½•ç‰¹å®šæ•°æ®é›†è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œä¸å…¨ç›‘ç£çš„åŸºçº¿æ¨¡å‹ç«äº‰ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬åœ¨ImageNetä¸Šçš„é›¶æ ·æœ¬åˆ†ç±»å‡†ç¡®ç‡ä¸åŸå§‹ResNet-50ç›¸å½“ï¼Œè€Œæ— éœ€ä½¿ç”¨ä»»ä½•åŸæœ¬ç”¨äºè®­ç»ƒçš„128ä¸‡è®­ç»ƒæ ·æœ¬ã€‚\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åœ¨ ğŸ¤— Transformers ä¸­ä½¿ç”¨ CLIP æ¨¡å‹\n",
    "\n",
    "`CLIPTokenizer` ç”¨äºç¼–ç æ–‡æœ¬ã€‚`CLIPProcessor` å°† `CLIPImageProcessor` å’Œ `CLIPTokenizer` å°è£…æˆä¸€ä¸ªå®ä¾‹ï¼Œä»¥åŒæ—¶ç¼–ç æ–‡æœ¬å’Œå‡†å¤‡å›¾åƒã€‚ä»¥ä¸‹ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ `CLIPProcessor` å’Œ `CLIPModel` è·å–å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦åˆ†æ•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9961, 0.0046]], device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = CLIPModel.from_pretrained(\n",
    "    \"openai/clip-vit-base-patch32\", torch_dtype=torch.bfloat16, device_map=device\n",
    ")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(\n",
    "    text=[\"a photo of a cat\", \"a photo of a dog\"],\n",
    "    images=image,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ")\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(\n",
    "    dim=1\n",
    ")  # we can take the softmax to get the label probabilities\n",
    "print(probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
