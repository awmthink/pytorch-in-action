{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutaion vision Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型概述\n",
    "\n",
    "在进入 CvT 之前，我们先简要回顾一下之前章节中讨论的 ViT 架构，以便更好地理解 CvT 架构。ViT 将每幅图像分解为具有固定长度的序列标记（即不重叠的图像块），然后应用多个标准的 Transformer 层，其中包括多头自注意力和位置前馈模块 (FFN)，以建模全局关系进行分类。\n",
    "\n",
    "卷积视觉 Transformer (CvT) 模型是微软 Cloud+AI 团队 在其论文 [CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2103.15808) 中提出的。CvT 结合了 CNN 的所有优点：局部感受野、共享权重、空间下采样，以及 平移、缩放、畸变不变性，同时保留 Transformer 的优点：动态注意力、全局上下文融合、更好的泛化能力。与 ViT 相比，CvT 在保持计算效率的同时实现了更优的性能。此外，由于卷积引入了内建的局部上下文结构，CvT 不再需要位置嵌入，这使其在适应需要可变输入分辨率的广泛视觉任务方面具有潜在优势。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型架构\n",
    "\n",
    "<div class=\"wy-nav-content-img\">\n",
    "    <img src=\"assets/CvT_model_arch.png\" width=960px alt=\"CvT 的模型架构图\">\n",
    "    <p>图1：(a) 整体架构，展示了通过卷积标记嵌入层实现的分层多阶段结构。 (b) 卷积 Transformer 块的详细信息，卷积投影作为第一层。</p>\n",
    "</div>\n",
    "\n",
    "上图展示了 CvT 架构的 3 阶段流水线的主要步骤。CvT 的核心在于将两种基于卷积的操作融合到视觉 Transformer 架构中：\n",
    "\n",
    "* 卷积标记嵌入：将输入图像分割为重叠的图像块，重组为标记，然后输入卷积层。这减少了标记数量（类似于下采样图像中的像素），同时增强其特征丰富度，类似于传统的 CNN。不像其他 Transformer，我们跳过为标记添加预定义的位置信息，而完全依赖卷积操作来捕获空间关系。\n",
    "* 卷积 Transformer 块：CvT 的每个阶段包含多个此类块。在此，我们使用深度可分离卷积（卷积投影）来处理自注意力模块的“查询”、“键”和“值”组件，而不是 ViT 中的线性投影，如上图所示。这保留了 Transformer 的优点，同时提高了效率。请注意，“分类标记”（用于最终预测）仅在最后一个阶段添加。最后，一个标准的全连接层对最终的分类标记进行分析，以预测图像类别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CvT 架构与其他视觉 Transformer 的比较\n",
    "\n",
    "下表显示了上述代表性并行工作与 CvT 之间在位置编码的必要性、标记嵌入类型、投影类型和主干中的 Transformer 结构方面的关键差异。\n",
    "\n",
    "\n",
    "| 模型      | 需要位置编码 (PE) | 标记嵌入类型            | 注意力投影类型 | 分层 Transformer |\n",
    "| --------- | ----------------- | ----------------------- | -------------- | ---------------- |\n",
    "| ViT, DeiT | 是                | 非重叠                  | 线性           | 否               |\n",
    "| CPVT      | 否 (带 PE 生成器) | 非重叠                  | 线性           | 否               |\n",
    "| TNT       | 是                | 非重叠（图像块 + 像素） | 线性           | 否               |\n",
    "| T2T       | 是                | 重叠（拼接）            | 线性           | 部分 (标记化)    |\n",
    "| PVT       | 是                | 非重叠                  | 空间缩减       | 是               |\n",
    "| _CvT_     | _否_              | _重叠（卷积）_          | _卷积_         | _是_             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主要亮点\n",
    "\n",
    "CvT 实现卓越性能和计算效率的四个主要亮点如下：\n",
    "\n",
    "* 包含新的 卷积标记嵌入 的 分层 Transformer。\n",
    "* 利用 卷积投影 的卷积 Transformer 块。\n",
    "* 由于卷积引入了内建的局部上下文结构， 不需要位置编码。\n",
    "* 相较于其他视觉 Transformer 架构，参数更少且 FLOPs（每秒浮点运算次数）更低。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch 动手实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积 Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEmbed(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        patch_size=7,\n",
    "        in_chans=3,\n",
    "        embed_dim=64,\n",
    "        stride=4,\n",
    "        padding=2,\n",
    "        norm_layer=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans, embed_dim, kernel_size=patch_size, stride=stride, padding=padding\n",
    "        )\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "\n",
    "        B, C, H, W = x.shape\n",
    "        x = rearrange(x, \"b c h w -> b (h w) c\")\n",
    "        if self.norm:\n",
    "            x = self.norm(x)\n",
    "        x = rearrange(x, \"b (h w) c -> b c h w\", h=H, w=W)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CvtAttention 中的卷积投射层的实现\n",
    "\n",
    "<div class=\"wy-nav-content-img\">\n",
    "    <img src=\"assets/CvT_conv_projection.png\" width=1280px alt=\"CvT 中的卷积投射层\">\n",
    "    <p>图2：(a) ViT 中的线性投影。 (b) 卷积投影。 (c) 压缩卷积投影（CvT 中的默认设置）</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CvtSelfAttentionConvProjection(nn.Module):\n",
    "    def __init__(self, embed_dim, kernel_size, padding, stride):\n",
    "        super().__init__()\n",
    "        self.convolution = nn.Conv2d(\n",
    "            embed_dim,\n",
    "            embed_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            stride=stride,\n",
    "            bias=False,\n",
    "            groups=embed_dim,\n",
    "        )\n",
    "        self.normalization = nn.BatchNorm2d(embed_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        hidden_state = self.convolution(hidden_state)\n",
    "        hidden_state = self.normalization(hidden_state)\n",
    "        return hidden_state\n",
    "\n",
    "\n",
    "class CvtSelfAttentionLinearProjection(nn.Module):\n",
    "    def forward(self, hidden_state):\n",
    "        batch_size, num_channels, height, width = hidden_state.shape\n",
    "        hidden_size = height * width\n",
    "        # rearrange \" b c h w -> b (h w) c\"\n",
    "        hidden_state = hidden_state.view(batch_size, num_channels, hidden_size).permute(\n",
    "            0, 2, 1\n",
    "        )\n",
    "        return hidden_state\n",
    "\n",
    "\n",
    "class CvtSelfAttentionProjection(nn.Module):\n",
    "    def __init__(\n",
    "        self, embed_dim, kernel_size, padding, stride, projection_method=\"dw_bn\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if projection_method == \"dw_bn\":\n",
    "            self.convolution_projection = CvtSelfAttentionConvProjection(\n",
    "                embed_dim, kernel_size, padding, stride\n",
    "            )\n",
    "        self.linear_projection = CvtSelfAttentionLinearProjection()\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        hidden_state = self.convolution_projection(hidden_state)\n",
    "        hidden_state = self.linear_projection(hidden_state)\n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 带卷积的 CvtSelfAttention 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CvtSelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads,\n",
    "        embed_dim,\n",
    "        kernel_size,\n",
    "        padding_q,\n",
    "        padding_kv,\n",
    "        stride_q,\n",
    "        stride_kv,\n",
    "        qkv_projection_method,\n",
    "        qkv_bias,\n",
    "        attention_drop_rate,\n",
    "        with_cls_token=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = embed_dim**-0.5\n",
    "        self.with_cls_token = with_cls_token\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.convolution_projection_query = CvtSelfAttentionProjection(\n",
    "            embed_dim,\n",
    "            kernel_size,\n",
    "            padding_q,\n",
    "            stride_q,\n",
    "            projection_method=(\n",
    "                \"linear\" if qkv_projection_method == \"avg\" else qkv_projection_method\n",
    "            ),\n",
    "        )\n",
    "        self.convolution_projection_key = CvtSelfAttentionProjection(\n",
    "            embed_dim,\n",
    "            kernel_size,\n",
    "            padding_kv,\n",
    "            stride_kv,\n",
    "            projection_method=qkv_projection_method,\n",
    "        )\n",
    "        self.convolution_projection_value = CvtSelfAttentionProjection(\n",
    "            embed_dim,\n",
    "            kernel_size,\n",
    "            padding_kv,\n",
    "            stride_kv,\n",
    "            projection_method=qkv_projection_method,\n",
    "        )\n",
    "\n",
    "        self.projection_query = nn.Linear(embed_dim, embed_dim, bias=qkv_bias)\n",
    "        self.projection_key = nn.Linear(embed_dim, embed_dim, bias=qkv_bias)\n",
    "        self.projection_value = nn.Linear(embed_dim, embed_dim, bias=qkv_bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(attention_drop_rate)\n",
    "\n",
    "    def rearrange_for_multi_head_attention(self, hidden_state):\n",
    "        batch_size, hidden_size, _ = hidden_state.shape\n",
    "        head_dim = self.embed_dim // self.num_heads\n",
    "        # rearrange 'b t (h d) -> b h t d'\n",
    "        return hidden_state.view(\n",
    "            batch_size, hidden_size, self.num_heads, head_dim\n",
    "        ).permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_state, height, width):\n",
    "        if self.with_cls_token:\n",
    "            cls_token, hidden_state = torch.split(hidden_state, [1, height * width], 1)\n",
    "        batch_size, hidden_size, num_channels = hidden_state.shape\n",
    "        # rearrange \"b (h w) c -> b c h w\"\n",
    "        hidden_state = hidden_state.permute(0, 2, 1).view(\n",
    "            batch_size, num_channels, height, width\n",
    "        )\n",
    "\n",
    "        key = self.convolution_projection_key(hidden_state)\n",
    "        query = self.convolution_projection_query(hidden_state)\n",
    "        value = self.convolution_projection_value(hidden_state)\n",
    "\n",
    "        if self.with_cls_token:\n",
    "            query = torch.cat((cls_token, query), dim=1)\n",
    "            key = torch.cat((cls_token, key), dim=1)\n",
    "            value = torch.cat((cls_token, value), dim=1)\n",
    "\n",
    "        head_dim = self.embed_dim // self.num_heads\n",
    "\n",
    "        query = self.rearrange_for_multi_head_attention(self.projection_query(query))\n",
    "        key = self.rearrange_for_multi_head_attention(self.projection_key(key))\n",
    "        value = self.rearrange_for_multi_head_attention(self.projection_value(value))\n",
    "\n",
    "        attention_score = torch.einsum(\"bhlk,bhtk->bhlt\", [query, key]) * self.scale\n",
    "        attention_probs = torch.nn.functional.softmax(attention_score, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context = torch.einsum(\"bhlt,bhtv->bhlv\", [attention_probs, value])\n",
    "        # rearrange\"b h t d -> b t (h d)\"\n",
    "        _, _, hidden_size, _ = context.shape\n",
    "        context = (\n",
    "            context.permute(0, 2, 1, 3)\n",
    "            .contiguous()\n",
    "            .view(batch_size, hidden_size, self.num_heads * head_dim)\n",
    "        )\n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers 中使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tabby, tabby cat\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor, CvtForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/cvt-13\")\n",
    "model = CvtForImageClassification.from_pretrained(\"microsoft/cvt-13\")\n",
    "\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "# 模型预测 1,000 个 ImageNet 类别中的一个\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
