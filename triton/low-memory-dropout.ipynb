{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low Memory Dropout\n",
    "\n",
    "在 `dropout` 的一般实现中，我们在 `Forward` 时需要生成一个和输入 Tensor 相同形状的 Mask Tensor，我们一般称为 `x_keep`，其中值为 0 的位置表示要丢弃，值为 1 的位置的输入原样输出。同时我们要将 `x_keep`保存起来，用于 Backward 时计算梯度。\n",
    "\n",
    "<div class=\"wy-nav-content-img\">\n",
    "    <img src=\"assets/low-memory-dropout_mask.drawio.svg\" width=\"600px\" alt=\"Dropout 的执行示意图\">\n",
    "    <p>Dropout 的执行示意图</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "这就相当于多占用了一份内存，我们来考虑，能否在 Backward 的时候，将 `x_keep` 重新计算出来呢？实际上就是和 Forward 的时候生成相同的随机数就可以了，那么我们只需要保存好随机种子就行了。\n",
    "\n",
    "`triton.language.rand(seed, offsets)` 接口用于根据 `offsets` 生成一组随机数，这样我们就可以在运行时，计算出 `x_keep`：\n",
    "\n",
    "```python\n",
    "x_keep = tl.rand(seed, offsets) > p\n",
    "```\n",
    "\n",
    "`tl.rand` 的接口除了 seed 外还要求传入 offsets，而不是一个 shape，这是因为随机数的生成需要考虑到所有的线程块，避免不同的线程块生成相同的随机数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------  --------  -------  ----------  --------  -------  --------  ---------  --------  ---------  -------\n",
      "input      0.799002  1.09926  -0.0294367  0.408484  1.90967  -2.65934  -0.446925  0.318453  -0.166543  1.86979\n",
      "keep mask  1         0         1          0         1         0         1         0          1         1\n",
      "output     1.598     0        -0.0588734  0         3.81934   0        -0.89385   0         -0.333087  3.73957\n",
      "---------  --------  -------  ----------  --------  -------  --------  ---------  --------  ---------  -------\n",
      "-----------------  --------  --------  ----------  -------  -------  -------  --------  ---------  -------  ---------\n",
      "input              0.932829  0.556717  -0.0195228  1.48022  -0.9475  -0.9432  -1.13308  -0.733073  1.15927  -0.355865\n",
      "output (seed 42)   0         0          0          0        -1.895    0        0        -1.46615   0        -0.71173\n",
      "output (seed 123)  0         1.11343    0          0         0       -1.8864   0         0         2.31854  -0.71173\n",
      "output (seed 42)   0         0          0          0        -1.895    0        0        -1.46615   0        -0.71173\n",
      "-----------------  --------  --------  ----------  -------  -------  -------  --------  ---------  -------  ---------\n"
     ]
    }
   ],
   "source": [
    "import tabulate\n",
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def dropout_kernel(\n",
    "    x_ptr, x_keep_ptr, output_ptr, n_elements, p, BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "\n",
    "    # load data\n",
    "    x = tl.load(x_ptr + offsets, mask)\n",
    "    x_keep = tl.load(x_keep_ptr + offsets, mask)\n",
    "    output = tl.where(x_keep, x / (1 - p), 0)\n",
    "    # write back to output\n",
    "    tl.store(output_ptr + offsets, output, mask)\n",
    "\n",
    "\n",
    "def dropout(x: torch.Tensor, x_keep: torch.Tensor, p):\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.is_contiguous()\n",
    "    n_elements = x.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n",
    "    dropout_kernel[grid](x, x_keep, output, n_elements, p, BLOCK_SIZE=1024)\n",
    "    return output\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def seeded_dropout_kernel(\n",
    "    x_ptr, output_ptr, n_elements, p, seed, BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "    x_keep = tl.rand(seed, offsets) > p\n",
    "    # load data\n",
    "    x = tl.load(x_ptr + offsets, mask)\n",
    "    output = tl.where(x_keep, x / (1 - p), 0)\n",
    "    # write back to output\n",
    "    tl.store(output_ptr + offsets, output, mask)\n",
    "\n",
    "\n",
    "def seeded_dropout(x: torch.Tensor, p, seed=123):\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.is_contiguous()\n",
    "    n_elements = x.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n",
    "    seeded_dropout_kernel[grid](x, output, n_elements, p, seed, BLOCK_SIZE=1024)\n",
    "    return output\n",
    "\n",
    "\n",
    "def dropout_test():\n",
    "    x = torch.randn((10,), device=\"cuda\")\n",
    "    p = 0.5\n",
    "    x_keep = torch.rand(x.shape) > p\n",
    "    x_keep = x_keep.to(dtype=torch.int32, device=\"cuda\")\n",
    "    output = dropout(x, x_keep, p)\n",
    "    print(\n",
    "        tabulate.tabulate(\n",
    "            [\n",
    "                [\"input\"] + x.tolist(),\n",
    "                [\"keep mask\"] + x_keep.tolist(),\n",
    "                [\"output\"] + output.tolist(),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def seeded_dropout_test():\n",
    "    x = torch.randn((10,), device=\"cuda\")\n",
    "    p = 0.5\n",
    "    output1 = seeded_dropout(x, p, 423)\n",
    "    output2 = seeded_dropout(x, p, 123)\n",
    "    output3 = seeded_dropout(x, p, 423)\n",
    "    print(\n",
    "        tabulate.tabulate(\n",
    "            [\n",
    "                [\"input\"] + x.tolist(),\n",
    "                [\"output (seed 42)\"] + output1.tolist(),\n",
    "                [\"output (seed 123)\"] + output2.tolist(),\n",
    "                [\"output (seed 42)\"] + output3.tolist(),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "dropout_test()\n",
    "seeded_dropout_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
