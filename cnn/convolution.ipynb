{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv2D 的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积相关的输入参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "input_w, input_h = 10, 10\n",
    "kernel_size = 3\n",
    "in_channel, out_channel = 2, 4\n",
    "padding = 1\n",
    "stride = 2\n",
    "dilation = 2\n",
    "groups = 1\n",
    "\n",
    "input_tensor = torch.randn(batch_size, in_channel, input_w, input_h)\n",
    "kernel = torch.randn(out_channel, in_channel // groups, kernel_size, kernel_size)\n",
    "bias = torch.randn(out_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch 算子调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor_torch = F.conv2d(\n",
    "    input_tensor,\n",
    "    kernel,\n",
    "    bias,\n",
    "    stride=stride,\n",
    "    padding=padding,\n",
    "    dilation=dilation,\n",
    "    groups=groups,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.5165,  1.5675,  4.4946,  3.6341],\n",
       "          [-1.3439,  2.2366,  9.1844,  4.2583],\n",
       "          [ 1.1333,  1.0637, 11.7791,  8.8648],\n",
       "          [ 3.2420,  3.1308,  0.2205,  4.3939]],\n",
       "\n",
       "         [[ 0.7311,  2.7015,  1.6318,  1.2317],\n",
       "          [-0.4422,  3.3261, -6.8021,  1.3115],\n",
       "          [-1.0562, -3.0663, -1.3042, -0.7498],\n",
       "          [ 0.7304, -1.6261,  1.4370, -0.0572]],\n",
       "\n",
       "         [[ 1.9504,  4.7555,  1.8208,  0.0887],\n",
       "          [-1.7191, -3.8353,  4.9241,  3.5999],\n",
       "          [-0.2118,  4.1998,  3.5847,  5.6590],\n",
       "          [ 4.1347,  3.7212,  3.9359, -2.5025]],\n",
       "\n",
       "         [[ 6.0964,  9.5853,  5.6209,  0.0882],\n",
       "          [-3.4786, -2.2103,  4.6434,  2.4786],\n",
       "          [ 0.5002,  0.0610,  0.4057,  9.4221],\n",
       "          [ 4.6218,  1.4769, -6.3351, -7.0424]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单通道 Conv 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_channel_conv(\n",
    "    input_tensor, kernel, bias=None, stride=1, padding=0, dilation=1\n",
    "):\n",
    "    assert input_tensor.ndim == 2 and kernel.ndim == 2\n",
    "\n",
    "    # Apply padding if needed\n",
    "    if padding > 0:\n",
    "        input_tensor = F.pad(input_tensor, (padding, padding, padding, padding))\n",
    "    # Set bias to zero if not provided\n",
    "    if bias is None:\n",
    "        bias = torch.zeros(1)\n",
    "\n",
    "    input_h, input_w = input_tensor.shape\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "\n",
    "    # Calculate dilated kernel dimensions\n",
    "    # (k - 1) * (d - 1) + k\n",
    "    dilated_win_h = (kernel_h - 1) * dilation + 1\n",
    "    dilated_win_w = (kernel_w - 1) * dilation + 1\n",
    "\n",
    "    # Calculate output dimensions\n",
    "    output_h = (input_h - dilated_win_h) // stride + 1\n",
    "    output_w = (input_w - dilated_win_w) // stride + 1\n",
    "\n",
    "    # Initialize output tensor\n",
    "    output_tensor = torch.zeros(output_h, output_w)\n",
    "\n",
    "    # Perform convolution\n",
    "    for i in range(output_h):\n",
    "        for j in range(output_w):\n",
    "            # Extract the sliding window from the input tensor\n",
    "            input_slice = input_tensor[\n",
    "                i * stride : i * stride + dilated_win_h : dilation,\n",
    "                j * stride : j * stride + dilated_win_w : dilation,\n",
    "            ]\n",
    "            # Perform element-wise multiplication and sum\n",
    "            output_tensor[i, j] = torch.sum(input_slice * kernel) + bias\n",
    "\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多通道 Conv 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多输入通道二维卷积\n",
    "def multi_input_channel_conv(\n",
    "    input_tensor, kernels, bias=None, stride=1, padding=0, dilation=1\n",
    "):\n",
    "    assert input_tensor.ndim == 3 and kernels.ndim == 3\n",
    "\n",
    "    # 对每个输入通道执行单通道卷积并叠加\n",
    "    output_tensor = torch.stack(\n",
    "        [\n",
    "            single_channel_conv(input_channel, kernel, None, stride, padding, dilation)\n",
    "            for input_channel, kernel in zip(input_tensor, kernels)\n",
    "        ]\n",
    "    ).sum(dim=0)\n",
    "\n",
    "    # 如果有偏置则加上\n",
    "    if bias is not None:\n",
    "        output_tensor += bias\n",
    "\n",
    "    return output_tensor\n",
    "\n",
    "\n",
    "# 多输出通道二维卷积\n",
    "def multi_output_channel_conv(\n",
    "    input_tensor, kernels, bias=None, stride=1, padding=0, dilation=1, groups=1\n",
    "):\n",
    "    assert input_tensor.ndim == 3 and kernels.ndim == 4\n",
    "\n",
    "    out_channels = kernels.size(0)\n",
    "    in_channels = input_tensor.size(0)\n",
    "\n",
    "    # 确保输出和输入通道数可以被分组均匀整除\n",
    "    assert out_channels % groups == 0 and in_channels % groups == 0\n",
    "\n",
    "    # 每个分组的输入和输出通道大小\n",
    "    input_group_size = in_channels // groups\n",
    "    output_group_size = out_channels // groups\n",
    "\n",
    "    # 根据输出通道索引获取对应的输入通道范围\n",
    "    def get_input_group(index):\n",
    "        group_start = (index // output_group_size) * input_group_size\n",
    "        group_end = group_start + input_group_size\n",
    "        return slice(group_start, group_end)\n",
    "\n",
    "    # 对每个输出通道执行多输入通道卷积\n",
    "    output_tensor = torch.stack(\n",
    "        [\n",
    "            multi_input_channel_conv(\n",
    "                input_tensor[get_input_group(output_index)],  # 获取输入通道分组\n",
    "                kernel,  # 对应的输出通道卷积核\n",
    "                bias_channel,  # 对应的偏置\n",
    "                stride,\n",
    "                padding,\n",
    "                dilation,\n",
    "            )\n",
    "            for output_index, (kernel, bias_channel) in enumerate(zip(kernels, bias))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return output_tensor\n",
    "\n",
    "\n",
    "# 批量多输出通道二维卷积\n",
    "def batch_multi_channel_conv(\n",
    "    input_batch, kernels, bias=None, stride=1, padding=0, dilation=1, groups=1\n",
    "):\n",
    "    assert input_batch.ndim == 4 and kernels.ndim == 4\n",
    "\n",
    "    # 对每个批次执行多输出通道卷积\n",
    "    output_batch = torch.stack(\n",
    "        [\n",
    "            multi_output_channel_conv(\n",
    "                input_tensor, kernels, bias, stride, padding, dilation, groups\n",
    "            )\n",
    "            for input_tensor in input_batch\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return output_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 5.5165,  1.5675,  4.4946,  3.6341],\n",
      "          [-1.3439,  2.2366,  9.1844,  4.2583],\n",
      "          [ 1.1333,  1.0637, 11.7791,  8.8648],\n",
      "          [ 3.2420,  3.1308,  0.2205,  4.3939]],\n",
      "\n",
      "         [[ 0.7311,  2.7015,  1.6318,  1.2317],\n",
      "          [-0.4422,  3.3261, -6.8021,  1.3115],\n",
      "          [-1.0562, -3.0663, -1.3042, -0.7498],\n",
      "          [ 0.7304, -1.6261,  1.4370, -0.0572]],\n",
      "\n",
      "         [[ 1.9504,  4.7555,  1.8208,  0.0887],\n",
      "          [-1.7191, -3.8353,  4.9241,  3.5999],\n",
      "          [-0.2118,  4.1998,  3.5847,  5.6590],\n",
      "          [ 4.1347,  3.7212,  3.9359, -2.5025]],\n",
      "\n",
      "         [[ 6.0964,  9.5853,  5.6209,  0.0882],\n",
      "          [-3.4786, -2.2103,  4.6434,  2.4786],\n",
      "          [ 0.5002,  0.0610,  0.4057,  9.4221],\n",
      "          [ 4.6218,  1.4769, -6.3351, -7.0424]]]])\n",
      "mean diff:  tensor(-1.0245e-08)\n"
     ]
    }
   ],
   "source": [
    "output_tensor1 = batch_multi_channel_conv(\n",
    "    input_tensor,\n",
    "    kernel,\n",
    "    bias,\n",
    "    stride=stride,\n",
    "    padding=padding,\n",
    "    dilation=dilation,\n",
    "    groups=groups,\n",
    ")\n",
    "print(output_tensor1)\n",
    "print(\"mean diff: \", torch.mean(output_tensor1 - output_tensor_torch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多通道的另一种实现\n",
    "\n",
    "将多通道输入看成是一个每个 `(i,j)` 位置上都是一个 $\\mathbb{R}^{c\\_in}$ 的向量。 将多通道的权重，看成是一个每个位置都是一个 $\\mathbb{R}^{c\\_out\\times c\\_in}$的二维矩阵。那么在滑动窗口内，原来的点积就变成为矩阵乘法，计算得到一个 $\\mathbb{R}^{c\\_{out}}$ 的向量，然后再将窗口内所的有向量相加。\n",
    "\n",
    "该实现方案要求输入 Tensor 的布局是 $N\\times H\\times W\\times C_{in}$, weight 的布局是：$K\\times K\\times C_{in} \\times C_{out} $\n",
    "\n",
    "该实现暂未支持分组卷积。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_channel_conv2d_flat(\n",
    "    input_tensor, kernel, bias=None, stride=1, padding=0, dilation=1\n",
    "):\n",
    "    assert input_tensor.ndim == 4 and kernel.ndim == 4\n",
    "\n",
    "    # (N,C,H,W)->(N,H,W,C)\n",
    "    input_tensor = torch.permute(input_tensor, (0, 2, 3, 1))\n",
    "    # (C_out, C_in, K, K) -> (K, K, C_in, C_out)\n",
    "    kernel = torch.permute(kernel, (2, 3, 1, 0))\n",
    "\n",
    "    if padding > 0:\n",
    "        input_tensor = F.pad(\n",
    "            input_tensor, (0, 0, padding, padding, padding, padding, 0, 0)\n",
    "        )\n",
    "\n",
    "    # Set bias to zero if not provided\n",
    "    if bias is None:\n",
    "        bias = torch.zeros(1)\n",
    "\n",
    "    batch_size, input_h, input_w, in_channel = input_tensor.shape\n",
    "    kernel_h, kernel_w, _, out_channel = kernel.shape\n",
    "\n",
    "    # Calculate dilated kernel dimensions\n",
    "    # (k - 1) * (d - 1) + k\n",
    "    dilated_win_h = (kernel_h - 1) * dilation + 1\n",
    "    dilated_win_w = (kernel_w - 1) * dilation + 1\n",
    "\n",
    "    # Calculate output dimensions\n",
    "    output_h = (input_h - dilated_win_h) // stride + 1\n",
    "    output_w = (input_w - dilated_win_w) // stride + 1\n",
    "\n",
    "    # Initialize output tensor\n",
    "    output_tensor = torch.zeros(batch_size, output_h, output_w, out_channel)\n",
    "\n",
    "    for i in range(output_h):\n",
    "        for j in range(output_w):\n",
    "            input_slice = input_tensor[\n",
    "                :,\n",
    "                i * stride : i * stride + dilated_win_h : dilation,\n",
    "                j * stride : j * stride + dilated_win_w : dilation,\n",
    "            ]\n",
    "            # (bs, k, k, c_in) * (k, k, c_in, c_out) -> (bs, k, k, c_out)\n",
    "            output_tensor[:, i, j] = torch.einsum(\n",
    "                \"bijk,ijkl->bijl\", input_slice, kernel\n",
    "            ).sum(dim=(1, 2))\n",
    "\n",
    "    output_tensor += bias\n",
    "    return torch.permute(output_tensor, (0, 3, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 5.5165,  1.5675,  4.4946,  3.6341],\n",
      "          [-1.3439,  2.2366,  9.1844,  4.2583],\n",
      "          [ 1.1333,  1.0637, 11.7791,  8.8648],\n",
      "          [ 3.2420,  3.1308,  0.2205,  4.3939]],\n",
      "\n",
      "         [[ 0.7311,  2.7015,  1.6318,  1.2317],\n",
      "          [-0.4422,  3.3261, -6.8021,  1.3115],\n",
      "          [-1.0562, -3.0663, -1.3042, -0.7498],\n",
      "          [ 0.7304, -1.6261,  1.4370, -0.0572]],\n",
      "\n",
      "         [[ 1.9504,  4.7555,  1.8208,  0.0887],\n",
      "          [-1.7191, -3.8353,  4.9241,  3.5999],\n",
      "          [-0.2118,  4.1998,  3.5847,  5.6590],\n",
      "          [ 4.1347,  3.7212,  3.9359, -2.5025]],\n",
      "\n",
      "         [[ 6.0964,  9.5853,  5.6209,  0.0882],\n",
      "          [-3.4786, -2.2103,  4.6434,  2.4786],\n",
      "          [ 0.5002,  0.0610,  0.4057,  9.4221],\n",
      "          [ 4.6218,  1.4769, -6.3351, -7.0424]]]])\n",
      "mean diff:  tensor(-2.0489e-08)\n"
     ]
    }
   ],
   "source": [
    "output_tensor2 = multi_channel_conv2d_flat(\n",
    "    input_tensor, kernel, bias, stride=stride, padding=padding, dilation=dilation\n",
    ")\n",
    "print(output_tensor2)\n",
    "print(\"mean diff: \", torch.mean(output_tensor2 - output_tensor_torch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将卷积转换为1x1卷积核的矩阵乘法\n",
    "\n",
    "依次计算卷积核心每个平面位置 $(i,j)$上的 $1\\times 1$ 的卷积核在每个滑动窗口上和输入 Tensor 的乘法响应，输出是一个和输出 Tensor 尺寸相同的 Tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_matrix_multiplication(\n",
    "    input_tensor, kernel, bias=None, stride=1, padding=0, dilation=1, groups=1\n",
    "):\n",
    "    assert input_tensor.ndim == 4 and kernel.ndim == 4\n",
    "    if padding > 0:\n",
    "        input_tensor = F.pad(input_tensor, (padding, padding, padding, padding))\n",
    "    if bias is None:\n",
    "        bias = torch.zeros((kernel.size(0),))\n",
    "    if groups > 1:\n",
    "        raise NotImplemented\n",
    "    # NCHW -> NHWC\n",
    "    input_tensor = torch.permute(input_tensor, (0, 2, 3, 1)).contiguous()\n",
    "    batch_size, input_h, input_w, in_channel = input_tensor.shape\n",
    "    # Cout,Cin,K,K -> K, K, C_in, C_out\n",
    "    kernel = torch.permute(kernel, (2, 3, 1, 0))\n",
    "    kernel_h, kernel_w, in_channel, out_channel = kernel.shape\n",
    "\n",
    "    # Calculate dilated kernel dimensions\n",
    "    # (k - 1) * (d - 1) + k\n",
    "    dilated_win_h = (kernel_h - 1) * dilation + 1\n",
    "    dilated_win_w = (kernel_w - 1) * dilation + 1\n",
    "\n",
    "    # Calculate output dimensions\n",
    "    output_h = (input_h - dilated_win_h) // stride + 1\n",
    "    output_w = (input_w - dilated_win_w) // stride + 1\n",
    "\n",
    "    output_tensor = torch.zeros(batch_size, output_h, output_w, out_channel)\n",
    "    for i in range(kernel_h):\n",
    "        for j in range(kernel_w):\n",
    "            input_slice = input_tensor[\n",
    "                :,\n",
    "                i * dilation : i * dilation + output_h * stride : stride,\n",
    "                j * dilation : j * dilation + output_w * stride : stride,\n",
    "            ]\n",
    "            output_tensor += input_slice @ kernel[i, j]\n",
    "    output_tensor += bias\n",
    "    return output_tensor.permute((0, 3, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 5.5165,  1.5675,  4.4946,  3.6341],\n",
      "          [-1.3439,  2.2366,  9.1844,  4.2583],\n",
      "          [ 1.1333,  1.0637, 11.7791,  8.8648],\n",
      "          [ 3.2420,  3.1308,  0.2205,  4.3939]],\n",
      "\n",
      "         [[ 0.7311,  2.7015,  1.6318,  1.2317],\n",
      "          [-0.4422,  3.3261, -6.8021,  1.3115],\n",
      "          [-1.0562, -3.0663, -1.3042, -0.7498],\n",
      "          [ 0.7304, -1.6261,  1.4370, -0.0572]],\n",
      "\n",
      "         [[ 1.9504,  4.7555,  1.8208,  0.0887],\n",
      "          [-1.7191, -3.8353,  4.9241,  3.5999],\n",
      "          [-0.2118,  4.1998,  3.5847,  5.6590],\n",
      "          [ 4.1347,  3.7212,  3.9359, -2.5025]],\n",
      "\n",
      "         [[ 6.0964,  9.5853,  5.6209,  0.0882],\n",
      "          [-3.4786, -2.2103,  4.6434,  2.4786],\n",
      "          [ 0.5002,  0.0610,  0.4057,  9.4221],\n",
      "          [ 4.6218,  1.4769, -6.3351, -7.0424]]]])\n",
      "mean diff:  tensor(-2.0489e-08)\n"
     ]
    }
   ],
   "source": [
    "output_tensor3 = conv_matrix_multiplication(\n",
    "    input_tensor, kernel, bias, stride=stride, padding=padding, dilation=dilation\n",
    ")\n",
    "print(output_tensor3)\n",
    "print(\"mean diff: \", torch.mean(output_tensor3 - output_tensor_torch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积的 im2col 的实现\n",
    "\n",
    "暂不支持分组卷积的功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_im2col(\n",
    "    input_tensor, kernel, bias=None, stride=1, padding=0, dilation=1, groups=1\n",
    "):\n",
    "    assert input_tensor.ndim == 4 and kernel.ndim == 4\n",
    "    if padding > 0:\n",
    "        input_tensor = F.pad(input_tensor, (padding, padding, padding, padding))\n",
    "    if bias is None:\n",
    "        bias = torch.zeros((kernel.size(0),))\n",
    "    if groups > 1:\n",
    "        raise NotImplemented\n",
    "    # NCHW -> NHWC\n",
    "    input_tensor = torch.permute(input_tensor, (0, 2, 3, 1)).contiguous()\n",
    "    batch_size, input_h, input_w, in_channel = input_tensor.shape\n",
    "    out_channel, _, kernel_h, kernel_w = kernel.shape\n",
    "    output_h = (input_h - (kernel_h - 1) * dilation - 1) // stride + 1\n",
    "    output_w = (input_w - (kernel_w - 1) * dilation - 1) // stride + 1\n",
    "    ns, hs, ws, cs = input_tensor.stride()\n",
    "    inner_dim = kernel_h * kernel_w * in_channel\n",
    "    input_tensor = input_tensor.as_strided(\n",
    "        (batch_size, output_h, output_w, kernel_h, kernel_w, in_channel),\n",
    "        (ns, hs * stride, ws * stride, hs * dilation, ws * dilation, cs),\n",
    "    )\n",
    "    input_tensor = input_tensor.reshape(-1, inner_dim)\n",
    "    out = input_tensor @ kernel.permute(2, 3, 1, 0).reshape(inner_dim, -1) + bias\n",
    "    return out.reshape(batch_size, output_h, output_w, out_channel).permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 5.5165,  1.5675,  4.4946,  3.6341],\n",
      "          [-1.3439,  2.2366,  9.1844,  4.2583],\n",
      "          [ 1.1333,  1.0637, 11.7791,  8.8648],\n",
      "          [ 3.2420,  3.1308,  0.2205,  4.3939]],\n",
      "\n",
      "         [[ 0.7311,  2.7015,  1.6318,  1.2317],\n",
      "          [-0.4422,  3.3261, -6.8021,  1.3115],\n",
      "          [-1.0562, -3.0663, -1.3042, -0.7498],\n",
      "          [ 0.7304, -1.6261,  1.4370, -0.0572]],\n",
      "\n",
      "         [[ 1.9504,  4.7555,  1.8208,  0.0887],\n",
      "          [-1.7191, -3.8353,  4.9241,  3.5999],\n",
      "          [-0.2118,  4.1998,  3.5847,  5.6590],\n",
      "          [ 4.1347,  3.7212,  3.9359, -2.5025]],\n",
      "\n",
      "         [[ 6.0964,  9.5853,  5.6209,  0.0882],\n",
      "          [-3.4786, -2.2103,  4.6434,  2.4786],\n",
      "          [ 0.5002,  0.0610,  0.4057,  9.4221],\n",
      "          [ 4.6218,  1.4769, -6.3351, -7.0424]]]])\n",
      "mean diff:  tensor(-5.2154e-08)\n"
     ]
    }
   ],
   "source": [
    "output_tensor4 = conv_im2col(\n",
    "    input_tensor, kernel, bias, stride=stride, padding=padding, dilation=dilation\n",
    ")\n",
    "print(output_tensor4)\n",
    "print(\"mean diff: \", torch.mean(output_tensor4 - output_tensor_torch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 转置卷积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 4, 4])\n",
      "tensor([[[[-1.3045,  1.7306,  1.2207, -1.9655],\n",
      "          [-1.7244,  2.4155,  0.9919,  1.1436],\n",
      "          [ 0.0267, -1.2458, -2.0820,  0.1620],\n",
      "          [-0.2911, -0.3754,  3.1075, -1.3857]],\n",
      "\n",
      "         [[-1.5654, -2.2614, -0.1039,  1.5502],\n",
      "          [-2.1295,  0.8266,  2.1824,  1.8024],\n",
      "          [-1.3203,  3.8147,  1.0539, -1.0710],\n",
      "          [-1.7585,  1.0746,  0.4378, -1.0524]],\n",
      "\n",
      "         [[ 1.4943, -1.5160,  1.1899, -0.8850],\n",
      "          [-1.2057, -0.1939,  1.3018, -2.5872],\n",
      "          [-1.8554,  2.1404,  0.8853, -1.3569],\n",
      "          [-0.2891, -1.4639,  1.7653,  1.2461]],\n",
      "\n",
      "         [[ 0.8424,  0.4848,  2.1443,  1.4764],\n",
      "          [-0.5719, -3.7087,  4.5201, -1.9685],\n",
      "          [ 2.0317, -5.7544,  4.1698, -2.6215],\n",
      "          [ 1.9188, -2.2231,  1.4131, -0.4709]],\n",
      "\n",
      "         [[-0.6279, -1.1798,  0.9922, -0.3383],\n",
      "          [ 0.4551,  0.4996, -0.9740, -0.5333],\n",
      "          [ 2.1379,  0.2680,  1.1029,  0.3916],\n",
      "          [-0.9653, -0.3141,  0.5062,  0.1220]],\n",
      "\n",
      "         [[-0.2717, -0.9509, -0.3149, -0.2877],\n",
      "          [ 1.4769,  0.6700,  1.4917,  0.3097],\n",
      "          [-0.8717,  0.0063, -1.6884, -0.6643],\n",
      "          [-0.5859,  0.7497,  1.1424,  0.1653]],\n",
      "\n",
      "         [[-0.0458, -0.3488,  0.6486,  0.5743],\n",
      "          [ 0.8318, -0.1901, -0.9275, -0.6601],\n",
      "          [ 0.0551, -2.5536, -0.5825,  0.2949],\n",
      "          [ 0.4917,  1.1854,  0.3082,  0.0487]],\n",
      "\n",
      "         [[ 1.7421,  0.1782, -0.2509, -0.0977],\n",
      "          [-2.7301, -0.0133,  0.5383,  0.1679],\n",
      "          [ 1.3211, -2.1151, -1.5233, -0.0701],\n",
      "          [-0.3463,  0.6033, -0.4551, -0.1061]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_h, input_w = 2, 2\n",
    "in_channel, out_channel = 4, 8\n",
    "batch_size = 1\n",
    "kernel_size = 3\n",
    "padding = 0\n",
    "stride = 1\n",
    "dilation = 1\n",
    "groups = 2\n",
    "\n",
    "input_tensor = torch.randn((batch_size, in_channel, input_h, input_w))\n",
    "# weight 的定义是按 正向的 conv 的参数进行定义的\n",
    "weight = torch.randn((in_channel, out_channel // groups, kernel_size, kernel_size))\n",
    "bias = None\n",
    "\n",
    "output_tensor_torch = F.conv_transpose2d(\n",
    "    input_tensor,\n",
    "    weight,\n",
    "    bias,\n",
    "    stride=stride,\n",
    "    padding=padding,\n",
    "    dilation=dilation,\n",
    "    groups=groups,\n",
    ")\n",
    "print(output_tensor_torch.shape)\n",
    "print(output_tensor_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_channel_transposed_conv(\n",
    "    input_tensor, kernel, bias=None, stride=1, padding=0, dilation=1\n",
    "):\n",
    "    assert input_tensor.ndim == 2 and kernel.ndim == 2\n",
    "\n",
    "    # Set bias to zero if not provided\n",
    "    if bias is None:\n",
    "        bias = torch.zeros(1)\n",
    "\n",
    "    input_h, input_w = input_tensor.shape\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "\n",
    "    # Calculate dilated kernel dimensions\n",
    "    # (k - 1) * (d - 1) + k\n",
    "    dilated_win_h = (kernel_h - 1) * dilation + 1\n",
    "    dilated_win_w = (kernel_w - 1) * dilation + 1\n",
    "\n",
    "    # Calculate output dimensions\n",
    "    output_h = (input_h - 1) * stride + dilated_win_h - 2 * padding\n",
    "    output_w = (input_w - 1) * stride + dilated_win_w - 2 * padding\n",
    "\n",
    "    # Initialize output tensor\n",
    "    output_tensor = torch.zeros(output_h, output_w)\n",
    "\n",
    "    # Perform convolution\n",
    "    for i in range(input_h):\n",
    "        for j in range(input_w):\n",
    "            # Extract the sliding window from the input tensor\n",
    "            output_slice = output_tensor[\n",
    "                i * stride : i * stride + dilated_win_h : dilation,\n",
    "                j * stride : j * stride + dilated_win_w : dilation,\n",
    "            ]\n",
    "            # Perform element-wise multiplication and sum\n",
    "            output_slice += input_tensor[i, j] * kernel\n",
    "    output_tensor += bias\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n",
      "tensor([[-0.0991,  0.4821,  0.3528,  0.3290],\n",
      "        [-0.4932, -0.0368,  0.7613,  2.8093],\n",
      "        [-0.2827, -0.7647, -0.3647,  3.1284],\n",
      "        [-0.3378, -0.9811, -0.7781,  0.8926]])\n"
     ]
    }
   ],
   "source": [
    "outptu_tensor = single_channel_transposed_conv(\n",
    "    input_tensor[0, 0], kernel[0, 0], bias, stride, padding, dilation\n",
    ")\n",
    "print(outptu_tensor.shape)\n",
    "print(outptu_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_transposed2d(\n",
    "    input_tensor: torch.Tensor,\n",
    "    kernel: torch.Tensor,\n",
    "    bias=None,\n",
    "    stride=1,\n",
    "    padding=0,\n",
    "    dilation=1,\n",
    "    groups=1,\n",
    "):\n",
    "    batch_size, in_channel, input_h, input_w = input_tensor.shape\n",
    "    out_channel, _, kernel_h, kernel_w = kernel.shape\n",
    "\n",
    "    upsampled_input_h = (input_h - 1) * stride + 1\n",
    "    upsampled_input_w = (input_w - 1) * stride + 1\n",
    "\n",
    "    upsampled_tensor = torch.zeros(\n",
    "        batch_size, in_channel, upsampled_input_h, upsampled_input_w\n",
    "    )\n",
    "    upsampled_tensor[:, :, ::stride, ::stride] = input_tensor\n",
    "\n",
    "    kernel = torch.flip(kernel, dims=(2, 3))\n",
    "    # 将 kernel 先按 in_channels 拆成多组：[4,4,3,3] 拆分为两个[2,4,3,3]\n",
    "    group_kernels = torch.chunk(kernel, groups)\n",
    "    # 再将 kernel 在 out_channels 上Concat 起来，合并一个[2,8,3,3]\n",
    "    kernel = torch.concat(group_kernels, dim=1).permute(1, 0, 2, 3)\n",
    "\n",
    "    # 计算需要的 padding\n",
    "    output_padding = (kernel_h - 1) * dilation + 1 - padding - 1\n",
    "\n",
    "    return F.conv2d(\n",
    "        upsampled_tensor,\n",
    "        kernel,\n",
    "        bias,\n",
    "        stride=1,\n",
    "        padding=output_padding,\n",
    "        dilation=dilation,\n",
    "        groups=groups,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 4, 4])\n",
      "tensor([[[[-1.3045,  1.7306,  1.2207, -1.9655],\n",
      "          [-1.7244,  2.4155,  0.9919,  1.1436],\n",
      "          [ 0.0267, -1.2458, -2.0820,  0.1620],\n",
      "          [-0.2911, -0.3754,  3.1075, -1.3857]],\n",
      "\n",
      "         [[-1.5654, -2.2614, -0.1039,  1.5502],\n",
      "          [-2.1295,  0.8266,  2.1824,  1.8024],\n",
      "          [-1.3203,  3.8147,  1.0539, -1.0710],\n",
      "          [-1.7585,  1.0746,  0.4378, -1.0524]],\n",
      "\n",
      "         [[ 1.4943, -1.5160,  1.1899, -0.8850],\n",
      "          [-1.2057, -0.1939,  1.3018, -2.5872],\n",
      "          [-1.8554,  2.1404,  0.8853, -1.3569],\n",
      "          [-0.2891, -1.4639,  1.7653,  1.2461]],\n",
      "\n",
      "         [[ 0.8424,  0.4848,  2.1443,  1.4764],\n",
      "          [-0.5719, -3.7087,  4.5201, -1.9685],\n",
      "          [ 2.0317, -5.7544,  4.1698, -2.6215],\n",
      "          [ 1.9188, -2.2231,  1.4131, -0.4709]],\n",
      "\n",
      "         [[-0.6279, -1.1798,  0.9922, -0.3383],\n",
      "          [ 0.4551,  0.4996, -0.9740, -0.5333],\n",
      "          [ 2.1379,  0.2680,  1.1029,  0.3916],\n",
      "          [-0.9653, -0.3141,  0.5062,  0.1220]],\n",
      "\n",
      "         [[-0.2717, -0.9509, -0.3149, -0.2877],\n",
      "          [ 1.4769,  0.6700,  1.4917,  0.3097],\n",
      "          [-0.8717,  0.0063, -1.6884, -0.6643],\n",
      "          [-0.5859,  0.7497,  1.1424,  0.1653]],\n",
      "\n",
      "         [[-0.0458, -0.3488,  0.6486,  0.5743],\n",
      "          [ 0.8318, -0.1901, -0.9275, -0.6601],\n",
      "          [ 0.0551, -2.5536, -0.5825,  0.2949],\n",
      "          [ 0.4917,  1.1854,  0.3082,  0.0487]],\n",
      "\n",
      "         [[ 1.7421,  0.1782, -0.2509, -0.0977],\n",
      "          [-2.7301, -0.0133,  0.5383,  0.1679],\n",
      "          [ 1.3211, -2.1151, -1.5233, -0.0701],\n",
      "          [-0.3463,  0.6033, -0.4551, -0.1061]]]])\n",
      "match:  True\n"
     ]
    }
   ],
   "source": [
    "outptu_tensor = conv_transposed2d(\n",
    "    input_tensor, weight, bias, stride, padding, dilation, groups\n",
    ")\n",
    "print(outptu_tensor.shape)\n",
    "print(outptu_tensor)\n",
    "print(\"match: \", torch.allclose(output_tensor_torch, outptu_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积的梯度计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch的反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size = 1\n",
    "input_w, input_h = 11, 11\n",
    "kernel_size = 3\n",
    "in_channel, out_channel = 4, 8\n",
    "padding = 1\n",
    "stride = 2\n",
    "dilation = 2\n",
    "groups = 1\n",
    "\n",
    "input_tensor = torch.randn(batch_size, in_channel, input_w, input_h, requires_grad=True)\n",
    "kernel = torch.randn(\n",
    "    out_channel, in_channel // groups, kernel_size, kernel_size, requires_grad=True\n",
    ")\n",
    "bias = torch.randn(out_channel, requires_grad=True)\n",
    "\n",
    "output_tensor = F.conv2d(\n",
    "    input_tensor,\n",
    "    kernel,\n",
    "    bias,\n",
    "    stride=stride,\n",
    "    padding=padding,\n",
    "    dilation=dilation,\n",
    "    groups=groups,\n",
    ")\n",
    "output_tensor.retain_grad()\n",
    "loss = output_tensor.sum()\n",
    "loss.backward()\n",
    "output_tensor_grad = output_tensor.grad\n",
    "input_tensor_grad = input_tensor.grad\n",
    "kernel_grad = kernel.grad\n",
    "bias_grad = bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用卷积来实现反向传播\n",
    "\n",
    "\n",
    "未实现 groups 的功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_backward(\n",
    "    output_grad,\n",
    "    input_tensor,\n",
    "    kernel,\n",
    "    bias=None,\n",
    "    stride=1,\n",
    "    padding=0,\n",
    "    dilation=1,\n",
    "    groups=1,\n",
    "):\n",
    "    batch_size, out_channel, output_h, output_w = output_grad.shape\n",
    "    kernel_size = kernel.size(3)\n",
    "\n",
    "    upsampled_grad_h = (output_h - 1) * stride + 1\n",
    "    upsampled_grad_w = (output_w - 1) * stride + 1\n",
    "\n",
    "    upsampled_grad_tensor = torch.zeros(\n",
    "        batch_size, out_channel, upsampled_grad_h, upsampled_grad_w\n",
    "    )\n",
    "    upsampled_grad_tensor[:, :, ::stride, ::stride] = output_grad\n",
    "\n",
    "    flipped_kernel = torch.flip(kernel, dims=(2, 3)).permute(1, 0, 2, 3)\n",
    "\n",
    "    # 计算需要的 padding\n",
    "    output_padding = (kernel_size - 1) * dilation + 1 - padding - 1\n",
    "\n",
    "    input_grad = F.conv2d(\n",
    "        upsampled_grad_tensor,\n",
    "        flipped_kernel,\n",
    "        bias=None,\n",
    "        stride=1,\n",
    "        padding=output_padding,\n",
    "        dilation=dilation,\n",
    "        groups=groups,\n",
    "    )\n",
    "\n",
    "    # C_in,C_out,K,K -> C_out,C_in,K,K\n",
    "    weight_grad = F.conv2d(\n",
    "        input_tensor.permute(1, 0, 2, 3),  # N,C_in,H,W->C_in,N,H,W\n",
    "        upsampled_grad_tensor.permute(1, 0, 2, 3),  # N,C_out,H,W -> C_out,N,H,W\n",
    "        bias=None,\n",
    "        stride=1,\n",
    "        padding=padding,\n",
    "        dilation=1,\n",
    "        groups=groups,\n",
    "    ).permute(1, 0, 2, 3)\n",
    "    weight_grad = weight_grad[:, :, ::dilation, ::dilation]\n",
    "\n",
    "    bias_grad = torch.sum(output_grad, dim=(0, 2, 3))\n",
    "\n",
    "    return input_grad, (weight_grad, bias_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "input_tensor_grad_1, (kernel_grad_1, bias_grad_1) = conv2d_backward(\n",
    "    output_tensor_grad,\n",
    "    input_tensor,\n",
    "    kernel,\n",
    "    None,\n",
    "    stride=stride,\n",
    "    padding=padding,\n",
    "    dilation=dilation,\n",
    "    groups=groups,\n",
    ")\n",
    "print(torch.allclose(input_tensor_grad, input_tensor_grad_1))\n",
    "print(torch.allclose(kernel_grad, kernel_grad_1))\n",
    "print(torch.allclose(bias_grad, bias_grad_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用转置卷积与矩阵乘法\n",
    "\n",
    "未实现 groups 的功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_backward_use_transposed(\n",
    "    output_grad: torch.Tensor,\n",
    "    input_tensor,\n",
    "    kernel,\n",
    "    bias=None,\n",
    "    stride=1,\n",
    "    padding=0,\n",
    "    dilation=1,\n",
    "    groups=1,\n",
    "):\n",
    "    input_grad = F.conv_transpose2d(\n",
    "        output_grad,\n",
    "        kernel,\n",
    "        bias,\n",
    "        stride=stride,\n",
    "        padding=padding,\n",
    "        dilation=dilation,\n",
    "        groups=groups,\n",
    "    )\n",
    "    # F.unflod 输出的是 (N,C*K*K,L)\n",
    "    input_unflod = F.unfold(\n",
    "        input_tensor, kernel.size(3), dilation=dilation, padding=padding, stride=stride\n",
    "    ).permute(0, 2, 1)\n",
    "    input_unflod = input_unflod.reshape(-1, input_unflod.size(-1))\n",
    "    weight_grad = input_unflod.T @ output_grad.permute(0, 2, 3, 1).reshape(\n",
    "        input_unflod.size(0), -1\n",
    "    )\n",
    "    weight_grad = weight_grad.reshape(\n",
    "        input_tensor.size(1), kernel.size(3), kernel.size(3), -1\n",
    "    ).permute(3, 0, 1, 2)\n",
    "    bias_grad = torch.sum(output_grad, dim=(0, 2, 3))\n",
    "    return input_grad, (weight_grad, bias_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "input_tensor_grad_2, (kernel_grad_2, bias_grad_2) = conv2d_backward_use_transposed(\n",
    "    output_tensor_grad,\n",
    "    input_tensor,\n",
    "    kernel,\n",
    "    None,\n",
    "    stride=stride,\n",
    "    padding=padding,\n",
    "    dilation=dilation,\n",
    "    groups=groups,\n",
    ")\n",
    "print(torch.allclose(input_tensor_grad, input_tensor_grad_2))\n",
    "print(torch.allclose(kernel_grad, kernel_grad_2))\n",
    "print(torch.allclose(bias_grad, bias_grad_2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
