{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "prescribed-relation",
   "metadata": {},
   "source": [
    "# Pytorch中的神经网络基本单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imperial-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "rough-amplifier",
   "metadata": {},
   "source": [
    "# nn.Module\n",
    "\n",
    "nn.Module是神经网络结构的表示，它可以表示一个层，也可以表示一个结构块，也可以表示一个完整的模型结构。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "committed-acting",
   "metadata": {},
   "source": [
    "## 自定义一个layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "protected-google",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReluLayer()\n"
     ]
    }
   ],
   "source": [
    "class ReluLayer(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x > 0) * x\n",
    "\n",
    "relu = ReluLayer()\n",
    "print(relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "conceptual-writer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, -0.0000, -0.0000],\n",
       "        [0.0045, 1.7920, -0.0000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2, 3)\n",
    "relu(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "celtic-breach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyFCLayer()\n"
     ]
    }
   ],
   "source": [
    "# 带参数的Layer\n",
    "class MyFCLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(in_dim, out_dim))\n",
    "        self.bias = nn.Parameter(torch.randn(out_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.matmul(self.weights.data) + self.bias.data\n",
    "\n",
    "\n",
    "fclayer = MyFCLayer(25, 10)\n",
    "print(fclayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "light-archives",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -5.1787,   0.2756,  -8.5485,   7.4007,  -3.6588,   8.8778,  -4.9287,\n",
       "          10.6064, -11.0471,  -2.9971]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(1, 25)\n",
    "fclayer(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "vulnerable-gilbert",
   "metadata": {},
   "source": [
    "## 自定义一个Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "weird-potato",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearReluStack(\n",
      "  (stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=100, bias=True)\n",
      "    (1): ReluLayer()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReluLayer()\n",
      "    (4): MyFCLayer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LinearReluStack(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 100),\n",
    "            ReluLayer(),\n",
    "            nn.Linear(100, 100),\n",
    "            ReluLayer(),\n",
    "            MyFCLayer(100, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.stack(x)\n",
    "\n",
    "\n",
    "linear_relu_stack = LinearReluStack()\n",
    "print(linear_relu_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "systematic-lewis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5678, -3.6984,  0.5302, -0.1042,  0.5509,  0.0973, -1.8260, -1.6011,\n",
       "          1.2910, -2.4117]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(1, 28 * 28)\n",
    "linear_relu_stack(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "italian-redhead",
   "metadata": {},
   "source": [
    "## 自定义一个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "attended-grant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (stack): LinearReluStack(\n",
      "    (stack): Sequential(\n",
      "      (0): Linear(in_features=784, out_features=100, bias=True)\n",
      "      (1): ReluLayer()\n",
      "      (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (3): ReluLayer()\n",
      "      (4): MyFCLayer()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.stack = LinearReluStack()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.stack(self.flatten(x))\n",
    "\n",
    "\n",
    "model = NeuralNetwork()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "official-listing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1757,  2.8397,  2.7330, -0.8250, -1.3345, -1.5037, -0.0362, -2.6218,\n",
       "          1.4468, -1.3714]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(1, 28, 28)\n",
    "model(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "sitting-warner",
   "metadata": {},
   "source": [
    "## 模仿nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "designing-strand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySequential(\n",
      "  (0): Linear(in_features=25, out_features=100, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self.add_module(str(idx), module)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # _modules是内部的一个OrderedDict\n",
    "        for module in self._modules.values():\n",
    "            x = module(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "mlp = MySequential(nn.Linear(25, 100), nn.ReLU(), nn.Linear(100, 10))\n",
    "print(mlp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "regulated-scout",
   "metadata": {},
   "source": [
    "# 参数\n",
    "\n",
    "每一层的参数，我们可以通过`layer.bias`和`layer.weight`来访问，得到的是一个`nn.parameter.Parameter`的类型对象。\n",
    "\n",
    "对于Sequential的模型，我们可以通过下标来访问每一层：`seqmodel[i]`\n",
    "\n",
    "我们也可以通过`state_dict`来获取nn.Module中的所有层的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "competent-pontiac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.OrderedDict"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = nn.Sequential(nn.Linear(25, 100), nn.ReLU(), nn.Linear(100, 10))\n",
    "first_layer = mlp[0]\n",
    "first_layer.bias\n",
    "first_layer.weight\n",
    "first_layer.state_dict()\n",
    "type(mlp.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ultimate-pulse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0.weight', torch.Size([100, 25])) ('0.bias', torch.Size([100])) ('2.weight', torch.Size([10, 100])) ('2.bias', torch.Size([10]))\n"
     ]
    }
   ],
   "source": [
    "# 获取所有参数\n",
    "print(*[(name, param.shape) for name, param in mlp.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "confirmed-screen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 访问OrderedDict\n",
    "mlp.state_dict()[\"2.weight\"].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "needed-pastor",
   "metadata": {},
   "source": [
    "对于`nn.parameter.Parameter`类型的对象，我们可以通过`.data`与`.grad`拿到其数据与梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "international-capacity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100]), None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_layer.bias.shape, first_layer.bias.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "vital-great",
   "metadata": {},
   "source": [
    "# 参数初始化\n",
    "\n",
    "对整个网络应用某个初始化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "middle-gamma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25, out_features=100, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def norm_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "mlp.apply(norm_init)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "unlike-operations",
   "metadata": {},
   "source": [
    "单独的某层layer应用初始化 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "stuck-multimedia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=100, out_features=10, bias=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def xiaver_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "\n",
    "mlp[2].apply(xiaver_init)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "configured-printing",
   "metadata": {},
   "source": [
    "# 多个layer共享参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "complete-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared = nn.Linear(8, 8)  # 需要共享参数的layer\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4, 8), nn.ReLU(), shared, nn.ReLU(), shared, nn.ReLU(), nn.Linear(8, 1)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "elementary-block",
   "metadata": {},
   "source": [
    "net[2]和net[4]是共享参数的，梯度累加。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a48b5f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25, out_features=100, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('./data/mlp-model.pt')\n",
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
